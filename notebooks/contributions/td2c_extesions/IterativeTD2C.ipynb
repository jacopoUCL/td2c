{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative TD2C Comparison with Benchmark Methods\n",
    "\n",
    "This notebook presents a comparative analysis between an Iterative implementation of the `TD2C` method and several benchmark approaches. The `TD2C` algorithm is extended in this work through an iterative process that refines its performance by leveraging dynamic feature selection and causal graph construction.\n",
    "\n",
    "The proposed Iterative `TD2C` method involves the following steps:\n",
    "\n",
    "1. **Model Training and Initial Causal Discovery**: The model is initially trained, and the TD2C algorithm is applied to identify the top K causal connections.\n",
    "2. **Graph Construction**: A causal graph is constructed based on the identified connections.\n",
    "3. **Historical Extension**: The graph is extended to include past states, reflecting the progression from past to present.\n",
    "4. **Application of Meek Rules**: The graph is refined using Meek's Rules to ensure proper orientation of edges and to derive causal implications.\n",
    "5. **Missing Pair Identification and Markov Blanket Derivation**: Missing causal pairs are identified, and the Markov Blanket (MB) is derived if possible from the current graph structure.\n",
    "6. **Descriptor Recalculation**: Descriptors are recalculated using the derived MB, providing refined inputs for subsequent iterations.\n",
    "\n",
    "Through this iterative process, the TD2C method is expected to enhance its accuracy in detecting causal structures, making it a robust tool for temporal causal discovery. The effectiveness of this approach will be evaluated against benchmark methods identified from previous analyses, specifically from the `Compare_TD2C_MB_Strategies` notebook, to assess its performance improvements in terms of ROC-AUC, Precision, Recall adn F1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import pickle \n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from d2c.data_generation.builder import TSBuilder\n",
    "from d2c.descriptors_generation import D2C, DataLoader\n",
    "from d2c.benchmark import D2CWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = 40 # number of jobs to run in parallel. For D2C, parallelism is implemented at the observation level: each observation from a single file is processed in parallel\n",
    "SEED = 42 # random seed for reproducibility\n",
    "MB_SIZE = 2 # size to consider when estimating the markov blanket. This is only useful if the MB is actually estimated\n",
    "COUPLES_TO_CONSIDER_PER_DAG = -1 # edges that are considered in total to compute descriptors, for each TS. This can speed up the process. If set to -1, all possible edges are considered\n",
    "maxlags = 5 # maximum lags to consider when considering variable couples\n",
    "noise_std_filter = 0.01  # Example noise standard deviation to filter\n",
    "max_neighborhood_size_filter = 2  # Example filter for neighborhood size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "Data are generated with the `TSBuilder` class and saved in a specific folder. Then, the code checks for missing datasets in the folder and keeps running untill all the possible combinations of parameters have genereted a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET THE TSBUILDER WITH THE DESIRED PARAMETERS\n",
    "def run_process(params):\n",
    "    \"\"\"\n",
    "    Run a single process of the data generation.\n",
    "    \"\"\"\n",
    "    process, n_variables, max_neighborhood_size, noise_std = params\n",
    "    try:\n",
    "        tsbuilder = TSBuilder(observations_per_time_series=250, \n",
    "                              maxlags=5, \n",
    "                              n_variables=n_variables, \n",
    "                              time_series_per_process=40, \n",
    "                              processes_to_use=[process], \n",
    "                              noise_std=noise_std, \n",
    "                              max_neighborhood_size=max_neighborhood_size, \n",
    "                              seed=42, \n",
    "                              max_attempts=200,\n",
    "                              verbose=True)\n",
    "\n",
    "        tsbuilder.build()\n",
    "        tsbuilder.to_pickle(f'/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl')\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} done')\n",
    "    except ValueError as e:\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} failed: {e}')\n",
    "\n",
    "# BUILD THE DATA\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    This script generates the data for different parameters: processes, number of variables, neighborhood sizes and noise levels.\n",
    "    The data is saved in the .data folder.\n",
    "    The if __name__ == '__main__': is used to avoid multiprocessing issues in Jupyter notebooks, i.e. the script is run as a script and not\n",
    "    as a module as it would have been if the script was imported, with the __name__ being the name of the module.\n",
    "    If the script is imported, the __name__ is the name of the module, if it is run as a script, the __name__ is __main__.\n",
    "    So, to run this script in a Jupyter notebook, we write the code inside the if __name__ == '__main__': block, while, if we want to import\n",
    "    the functions from this script, we write \"from script import run_process\".\n",
    "    \"\"\"\n",
    "    parameters = [(process, n_variables, max_neighborhood_size, noise_std)\n",
    "                    for process in [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20] # \n",
    "                    for n_variables in [5] # , 10, 25\n",
    "                    for max_neighborhood_size in [2] # , 4, 8\n",
    "                    for noise_std in [0.01]] # , 0.005, 0.001\n",
    "\n",
    "    with Pool(processes=N_JOBS) as pool:\n",
    "        pool.map(run_process, parameters)\n",
    "\n",
    "\n",
    "# Function to check for missing files\n",
    "def check_missing_files():\n",
    "    missing = []\n",
    "    for process in [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20]:  # \n",
    "        for n_variables in [5]:  # , 10, 25\n",
    "            for max_neighborhood_size in [2]:  # , 4, 8\n",
    "                for noise_std in [0.01]:  # , 0.005, 0.001\n",
    "                    filename = f'/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "                    if not os.path.exists(filename):\n",
    "                        missing.append(filename)\n",
    "    return missing\n",
    "\n",
    "# Function to run the process\n",
    "def run_process(params):\n",
    "    process, n_variables, max_neighborhood_size, noise_std = params\n",
    "    try:  # we change the seed and increase the max_attempts\n",
    "        tsbuilder = TSBuilder(observations_per_time_series=250, \n",
    "                              maxlags=5, \n",
    "                              n_variables=n_variables, \n",
    "                              time_series_per_process=40, \n",
    "                              processes_to_use=[process], \n",
    "                              noise_std=noise_std, \n",
    "                              max_neighborhood_size=max_neighborhood_size, \n",
    "                              seed=24, \n",
    "                              max_attempts=400,\n",
    "                              verbose=True)\n",
    "\n",
    "        tsbuilder.build()\n",
    "        tsbuilder.to_pickle(f'/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl')\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} done')\n",
    "    except ValueError as e:\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} failed: {e}')\n",
    "\n",
    "# CHECK FOR MISSING FILES (IT CHEKS THE FILES WITH A DIFFERENT SEED AND MORE MAX_ATTEMPTS UNTILL MISSING IS EMPTY)\n",
    "if __name__ == '__main__':\n",
    "    while True:\n",
    "        missing = check_missing_files()\n",
    "        if not missing:\n",
    "            break\n",
    "\n",
    "        parameters = []\n",
    "        for missing_file in missing:\n",
    "            process = int(missing_file.split('/')[-1].split('_')[0][1:])\n",
    "            n_variables = int(missing_file.split('/')[-1].split('_')[1][1:])\n",
    "            max_neighborhood_size = int(missing_file.split('/')[-1].split('_')[2][2:])\n",
    "            noise_std = float(missing_file.split('/')[-1].split('_')[3][1:-4])\n",
    "            parameters.append((process, n_variables, max_neighborhood_size, noise_std))\n",
    "\n",
    "        with Pool(processes=N_JOBS) as pool:\n",
    "            pool.map(run_process, parameters)\n",
    "\n",
    "len(os.listdir('/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative TD2C implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First estimate\n",
    "2. top k (5) causally connected couples\n",
    "3. function that integrates nodes in these top k couples where necessary (when the MB for the destination node is computed)\n",
    "4. 2nd estimate (in theory better)\n",
    "5. keep going for each iteration\n",
    "6. plot evalutation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set k\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/initial/'\n",
    "\n",
    "# List of files to process\n",
    "to_process = []\n",
    "\n",
    "# Filtering the files to process\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    if noise_std != noise_std_filter or max_neighborhood_size != max_neighborhood_size_filter:\n",
    "        continue\n",
    "\n",
    "    to_process.append(file)\n",
    "\n",
    "# Create output folder if it does not exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Process each file and create new DAGs based on causal paths\n",
    "for file in tqdm(to_process):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    dataloader = DataLoader(n_variables=n_variables, maxlags=maxlags)\n",
    "    dataloader.from_pickle(input_folder + file)\n",
    "\n",
    "    d2c = D2C(\n",
    "        observations=dataloader.get_observations(),\n",
    "        dags=dataloader.get_dags(),\n",
    "        couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "        MB_size=MB_SIZE,\n",
    "        n_variables=n_variables,\n",
    "        maxlags=maxlags,\n",
    "        seed=SEED,\n",
    "        n_jobs=N_JOBS,\n",
    "        full=True,\n",
    "        quantiles=True,\n",
    "        normalize=True,\n",
    "        cmi='original',\n",
    "        mb_estimator='ts',\n",
    "        top_vars=3\n",
    "    )\n",
    "\n",
    "    d2c.initialize()  # Initializes the D2C object\n",
    "    descriptors_df = d2c.get_descriptors_df()  # Computes the descriptors\n",
    "\n",
    "    # Save the descriptors along with new DAGs if needed\n",
    "    descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "    descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "    descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "    descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "    descriptors_df.to_pickle(output_folder + f'P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "\n",
    "to_dos = []\n",
    "\n",
    "# This loop gets a list of all the files to be processed\n",
    "for testing_file in sorted(os.listdir(data_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "          \n",
    "        if noise_std != 0.01: # if the noise is different we skip the file\n",
    "            continue\n",
    "\n",
    "        if max_neighborhood_size != 2: # if the max_neighborhood_size is different we skip the file\n",
    "            continue\n",
    "\n",
    "        to_dos.append(testing_file) # we add the file to the list (to_dos) to be processed\n",
    "\n",
    "# sort to_dos by number of variables\n",
    "to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "# to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "# to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "\n",
    "# we create a dictionary with the lists of files to be processed\n",
    "todos = {'5': to_dos_5_variables} # , '10': to_dos_10_variables, '25': to_dos_25_variables\n",
    "\n",
    "# we create a dictionary to store the results\n",
    "dfs = []\n",
    "descriptors_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/initial/'\n",
    "\n",
    "# Re-save pickle files with protocol 4\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        file_path = os.path.join(descriptors_root, testing_file)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Re-save with protocol 4\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f, protocol=4)\n",
    "\n",
    "# This loop gets the descriptors for the files to be processed\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(os.path.join(descriptors_root, testing_file))\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            dfs.append(df)\n",
    "\n",
    "# we concatenate the descriptors\n",
    "descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop does the following:\n",
    "# 1. Creates some dictionaries to store the results\n",
    "# 2. Loads the training data\n",
    "# 3. Trains the model\n",
    "# 4. Evaluates the model\n",
    "# 5. Stores the results in the dictionaries\n",
    "# 6. Saves the dictionaries in a pickle file\n",
    "\n",
    "for n_vars, todo in todos.items():\n",
    "    td2c_rocs_process = {}\n",
    "    td2c_precision_process = {}\n",
    "    td2c_recall_process = {}\n",
    "    td2c_f1_process = {}\n",
    "    causal_df = {}\n",
    "    for testing_file in tqdm(todo):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "        # split training and testing data\n",
    "        training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "        model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "        # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "        model = model.fit(X_train, y_train)\n",
    "\n",
    "        rocs = {}\n",
    "        precisions = {}\n",
    "        recalls = {}\n",
    "        f1s = {}\n",
    "        causal_dfs = {}\n",
    "        for graph_id in range(40):\n",
    "            #load testing descriptors\n",
    "            test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "            test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "            X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_test = test_df['is_causal']\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            roc = roc_auc_score(y_test, y_pred_proba)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            rocs[graph_id] = roc\n",
    "            precisions[graph_id] = precision\n",
    "            recalls[graph_id] = recall\n",
    "            f1s[graph_id] = f1\n",
    "            \n",
    "            # add to causal_df test_df, y_pred_proba and y_pred\n",
    "            causal_dfs[graph_id] = test_df\n",
    "            causal_dfs[graph_id]['y_pred_proba'] = y_pred_proba\n",
    "            causal_dfs[graph_id]['y_pred'] = y_pred\n",
    "\n",
    "        causal_df[gen_process_number] = causal_dfs\n",
    "        td2c_rocs_process[gen_process_number] = rocs\n",
    "        td2c_precision_process[gen_process_number] = precisions\n",
    "        td2c_recall_process[gen_process_number] = recalls\n",
    "        td2c_f1_process[gen_process_number] = f1s\n",
    "\n",
    "# pickle everything\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'journal_results_td2c_R_N5.pkl'), 'wb') as f:\n",
    "    everything = (td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process, causal_df)\n",
    "    pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import causal_df \n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "with open(os.path.join(input_folder, 'journal_results_td2c_R_N5.pkl'), 'rb') as f:\n",
    "    TD2C_0_rocs_process, TD2C_0_precision_process, TD2C_0_recall_process, TD2C_0_f1_process, causal_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshape causal_df and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only rows for top k y_pred_proba\n",
    "for process_id, process_data in causal_df.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        causal_df[process_id][graph_id] = graph_data.nlargest(k, 'y_pred_proba')\n",
    "\n",
    "# for each causal_df keep only process_id, graph_id, edge_source, edge_dest and y_pred_proba\n",
    "for process_id, process_data in causal_df.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        causal_df[process_id][graph_id] = graph_data[['process_id', 'graph_id', 'edge_source', 'edge_dest', 'y_pred_proba']]\n",
    "        causal_df[process_id][graph_id].reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save the causal_df as a pkl file alone\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'wb') as f:\n",
    "    pickle.dump(causal_df, f)\n",
    "\n",
    "causal_df[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load reshaped causal_df, unify it and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'rb') as f:\n",
    "    causal_df = pickle.load(f)\n",
    "\n",
    "# create a dataframe with all the causal_df\n",
    "dfs = []\n",
    "for process_id, process_data in causal_df.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        dfs.append(graph_data)\n",
    "\n",
    "causal_df_unif = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "# sort in ascending order by process_id, graph_id, edge_source and edge_dest\n",
    "causal_df_unif.sort_values(by=['process_id', 'graph_id', 'edge_source', 'edge_dest'], inplace=True)\n",
    "\n",
    "# possible to insert that a couple must be present for all processes or for a certain percentage of processes (2/3)\n",
    "\n",
    "# unique of causal_df_unif for couples of edge_source and edge_dest\n",
    "causal_df_unif = causal_df_unif.drop_duplicates(subset=['edge_source', 'edge_dest'])\n",
    "\n",
    "# drop rows with y_pred_proba < 0.7\n",
    "causal_df_unif = causal_df_unif[causal_df_unif['y_pred_proba'] >= 0.7]\n",
    "\n",
    "# keep only the top 5 rows with highest y_pred_proba\n",
    "causal_df_unif = causal_df_unif.nlargest(5, 'y_pred_proba')\n",
    "\n",
    "# index reset\n",
    "causal_df_unif.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save the causal_df as a pkl file alone\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'wb') as f:\n",
    "    pickle.dump(causal_df_unif, f)\n",
    "\n",
    "causal_df_unif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of top variables to keep\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load unified dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df_unified\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'rb') as f:\n",
    "    causal_df_unif = pickle.load(f)\n",
    "\n",
    "causal_df_unif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Descriptors Generation using causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/second_estimate/'\n",
    "# List of files to process\n",
    "to_process = []\n",
    "\n",
    "# Filtering the files to process\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    if noise_std != noise_std_filter or max_neighborhood_size != max_neighborhood_size_filter:\n",
    "        continue\n",
    "\n",
    "    to_process.append(file)\n",
    "\n",
    "# Create output folder if it does not exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Process each file and create new DAGs based on causal paths\n",
    "for file in tqdm(to_process):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    dataloader = DataLoader(n_variables=n_variables, maxlags=maxlags)\n",
    "    dataloader.from_pickle(input_folder + file)\n",
    "\n",
    "    d2c = D2C(\n",
    "        observations=dataloader.get_observations(),\n",
    "        dags=dataloader.get_dags(),\n",
    "        couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "        MB_size=MB_SIZE,\n",
    "        n_variables=n_variables,\n",
    "        maxlags=maxlags,\n",
    "        seed=SEED,\n",
    "        n_jobs=N_JOBS,\n",
    "        full=True,\n",
    "        quantiles=True,\n",
    "        normalize=True,\n",
    "        cmi='original',\n",
    "        mb_estimator='iterative',\n",
    "        top_vars = 3,\n",
    "        causal_df = causal_df_unif\n",
    "    )\n",
    "\n",
    "    d2c.initialize()  # Initializes the D2C object\n",
    "    descriptors_df = d2c.get_descriptors_df()  # Computes the descriptors\n",
    "\n",
    "    # Save the descriptors along with new DAGs if needed\n",
    "    descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "    descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "    descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "    descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "    descriptors_df.to_pickle(output_folder + f'P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "\n",
    "to_dos = []\n",
    "\n",
    "# This loop gets a list of all the files to be processed\n",
    "for testing_file in sorted(os.listdir(data_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "          \n",
    "        if noise_std != 0.01: # if the noise is different we skip the file\n",
    "            continue\n",
    "\n",
    "        if max_neighborhood_size != 2: # if the max_neighborhood_size is different we skip the file\n",
    "            continue\n",
    "\n",
    "        to_dos.append(testing_file) # we add the file to the list (to_dos) to be processed\n",
    "\n",
    "# sort to_dos by number of variables\n",
    "to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "# to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "# to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "\n",
    "# we create a dictionary with the lists of files to be processed\n",
    "todos = {'5': to_dos_5_variables} # , '10': to_dos_10_variables, '25': to_dos_25_variables\n",
    "\n",
    "# we create a dictionary to store the results\n",
    "dfs = []\n",
    "descriptors_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/second_estimate/'\n",
    "\n",
    "# Re-save pickle files with protocol 4\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        file_path = os.path.join(descriptors_root, testing_file)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Re-save with protocol 4\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f, protocol=4)\n",
    "\n",
    "# This loop gets the descriptors for the files to be processed\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(os.path.join(descriptors_root, testing_file))\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            dfs.append(df)\n",
    "\n",
    "# we concatenate the descriptors\n",
    "descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop does the following:\n",
    "# 1. Creates some dictionaries to store the results\n",
    "# 2. Loads the training data\n",
    "# 3. Trains the model\n",
    "# 4. Evaluates the model\n",
    "# 5. Stores the results in the dictionaries\n",
    "# 6. Saves the dictionaries in a pickle file\n",
    "\n",
    "for n_vars, todo in todos.items():\n",
    "    td2c_rocs_process = {}\n",
    "    td2c_precision_process = {}\n",
    "    td2c_recall_process = {}\n",
    "    td2c_f1_process = {}\n",
    "    causal_df = {}\n",
    "    for testing_file in tqdm(todo):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "        # split training and testing data\n",
    "        training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "        model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "        # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "        model = model.fit(X_train, y_train)\n",
    "\n",
    "        rocs = {}\n",
    "        precisions = {}\n",
    "        recalls = {}\n",
    "        f1s = {}\n",
    "        causal_dfs = {}\n",
    "        for graph_id in range(40):\n",
    "            #load testing descriptors\n",
    "            test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "            test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "            X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_test = test_df['is_causal']\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            roc = roc_auc_score(y_test, y_pred_proba)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            rocs[graph_id] = roc\n",
    "            precisions[graph_id] = precision\n",
    "            recalls[graph_id] = recall\n",
    "            f1s[graph_id] = f1\n",
    "            \n",
    "            # add to causal_df test_df, y_pred_proba and y_pred\n",
    "            causal_dfs[graph_id] = test_df\n",
    "            causal_dfs[graph_id]['y_pred_proba'] = y_pred_proba\n",
    "            causal_dfs[graph_id]['y_pred'] = y_pred\n",
    "\n",
    "        causal_df[gen_process_number] = causal_dfs\n",
    "        td2c_rocs_process[gen_process_number] = rocs\n",
    "        td2c_precision_process[gen_process_number] = precisions\n",
    "        td2c_recall_process[gen_process_number] = recalls\n",
    "        td2c_f1_process[gen_process_number] = f1s\n",
    "\n",
    "# pickle everything\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'wb') as f:\n",
    "    everything = (td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process, causal_df)\n",
    "    pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import results\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'rb') as f:\n",
    "    TD2C_1_rocs_process, TD2C_1_precision_process, TD2C_1_recall_process, TD2C_1_f1_process, causal_df_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshape causal_df and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'rb') as f:\n",
    "    TD2C_1_rocs_process, TD2C_1_precision_process, TD2C_1_recall_process, TD2C_1_f1_process, causal_df_2 = pickle.load(f)\n",
    "\n",
    "# keep only rows for top k y_pred_proba\n",
    "for process_id, process_data in causal_df_2.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        causal_df_2[process_id][graph_id] = graph_data.nlargest(k, 'y_pred_proba')\n",
    "\n",
    "# for each causal_df keep only process_id, graph_id, edge_source, edge_dest and y_pred_proba\n",
    "for process_id, process_data in causal_df_2.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        causal_df_2[process_id][graph_id] = graph_data[['process_id', 'graph_id', 'edge_source', 'edge_dest', 'y_pred_proba']]\n",
    "        causal_df_2[process_id][graph_id].reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save the causal_df as a pkl file alone\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'wb') as f:\n",
    "    pickle.dump(causal_df_2, f)\n",
    "\n",
    "causal_df_2[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load reshaped causal_df, unify it and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'rb') as f:\n",
    "    causal_df_2 = pickle.load(f)\n",
    "\n",
    "# create a dataframe with all the causal_df\n",
    "dfs = []\n",
    "for process_id, process_data in causal_df_2.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        dfs.append(graph_data)\n",
    "\n",
    "causal_df_unif_2 = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "# sort in ascending order by process_id, graph_id, edge_source and edge_dest\n",
    "causal_df_unif_2.sort_values(by=['process_id', 'graph_id', 'edge_source', 'edge_dest'], inplace=True)\n",
    "\n",
    "# unique of causal_df_unif for couples of edge_source and edge_dest\n",
    "causal_df_unif_2 = causal_df_unif_2.drop_duplicates(subset=['edge_source', 'edge_dest'])\n",
    "\n",
    "# drop rows with y_pred_proba < 0.7\n",
    "causal_df_unif_2 = causal_df_unif_2[causal_df_unif_2['y_pred_proba'] >= 0.7]\n",
    "\n",
    "# keep only the top 5 rows with highest y_pred_proba\n",
    "causal_df_unif_2 = causal_df_unif_2.nlargest(5, 'y_pred_proba')\n",
    "\n",
    "# index reset\n",
    "causal_df_unif_2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save the causal_df as a pkl file alone\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'wb') as f:\n",
    "    pickle.dump(causal_df_unif_2, f)\n",
    "\n",
    "causal_df_unif_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of top variables to keep\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load unified dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df_unified\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'rb') as f:\n",
    "    causal_df_unif_2 = pickle.load(f)\n",
    "\n",
    "causal_df_unif_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Descriptors Generation using causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/third_estimate/'\n",
    "# List of files to process\n",
    "to_process = []\n",
    "\n",
    "# Filtering the files to process\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    if noise_std != noise_std_filter or max_neighborhood_size != max_neighborhood_size_filter:\n",
    "        continue\n",
    "\n",
    "    to_process.append(file)\n",
    "\n",
    "# Create output folder if it does not exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Process each file and create new DAGs based on causal paths\n",
    "for file in tqdm(to_process):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    dataloader = DataLoader(n_variables=n_variables, maxlags=maxlags)\n",
    "    dataloader.from_pickle(input_folder + file)\n",
    "\n",
    "    d2c = D2C(\n",
    "        observations=dataloader.get_observations(),\n",
    "        dags=dataloader.get_dags(),\n",
    "        couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "        MB_size=MB_SIZE,\n",
    "        n_variables=n_variables,\n",
    "        maxlags=maxlags,\n",
    "        seed=SEED,\n",
    "        n_jobs=N_JOBS,\n",
    "        full=True,\n",
    "        quantiles=True,\n",
    "        normalize=True,\n",
    "        cmi='original',\n",
    "        mb_estimator='iterative',\n",
    "        top_vars=3,\n",
    "        causal_df=causal_df_unif_2\n",
    "    )\n",
    "\n",
    "    d2c.initialize()  # Initializes the D2C object\n",
    "    descriptors_df = d2c.get_descriptors_df()  # Computes the descriptors\n",
    "\n",
    "    # Save the descriptors along with new DAGs if needed\n",
    "    descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "    descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "    descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "    descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "    descriptors_df.to_pickle(output_folder + f'P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "\n",
    "to_dos = []\n",
    "\n",
    "# This loop gets a list of all the files to be processed\n",
    "for testing_file in sorted(os.listdir(data_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "          \n",
    "        if noise_std != 0.01: # if the noise is different we skip the file\n",
    "            continue\n",
    "\n",
    "        if max_neighborhood_size != 2: # if the max_neighborhood_size is different we skip the file\n",
    "            continue\n",
    "\n",
    "        to_dos.append(testing_file) # we add the file to the list (to_dos) to be processed\n",
    "\n",
    "# sort to_dos by number of variables\n",
    "to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "# to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "# to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "\n",
    "# we create a dictionary with the lists of files to be processed\n",
    "todos = {'5': to_dos_5_variables} # , '10': to_dos_10_variables, '25': to_dos_25_variables\n",
    "\n",
    "# we create a dictionary to store the results\n",
    "dfs = []\n",
    "descriptors_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/third_estimate/'\n",
    "\n",
    "# Re-save pickle files with protocol 4\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        file_path = os.path.join(descriptors_root, testing_file)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Re-save with protocol 4\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f, protocol=4)\n",
    "\n",
    "# This loop gets the descriptors for the files to be processed\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(os.path.join(descriptors_root, testing_file))\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            dfs.append(df)\n",
    "\n",
    "# we concatenate the descriptors\n",
    "descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop does the following:\n",
    "# 1. Creates some dictionaries to store the results\n",
    "# 2. Loads the training data\n",
    "# 3. Trains the model\n",
    "# 4. Evaluates the model\n",
    "# 5. Stores the results in the dictionaries\n",
    "# 6. Saves the dictionaries in a pickle file\n",
    "\n",
    "for n_vars, todo in todos.items():\n",
    "    td2c_rocs_process = {}\n",
    "    td2c_precision_process = {}\n",
    "    td2c_recall_process = {}\n",
    "    td2c_f1_process = {}\n",
    "    causal_df = {}\n",
    "    for testing_file in tqdm(todo):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "        # split training and testing data\n",
    "        training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "        model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "        # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "        model = model.fit(X_train, y_train)\n",
    "\n",
    "        rocs = {}\n",
    "        precisions = {}\n",
    "        recalls = {}\n",
    "        f1s = {}\n",
    "        causal_dfs = {}\n",
    "        for graph_id in range(40):\n",
    "            #load testing descriptors\n",
    "            test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "            test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "            X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_test = test_df['is_causal']\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            roc = roc_auc_score(y_test, y_pred_proba)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            rocs[graph_id] = roc\n",
    "            precisions[graph_id] = precision\n",
    "            recalls[graph_id] = recall\n",
    "            f1s[graph_id] = f1\n",
    "            \n",
    "            # add to causal_df test_df, y_pred_proba and y_pred\n",
    "            causal_dfs[graph_id] = test_df\n",
    "            causal_dfs[graph_id]['y_pred_proba'] = y_pred_proba\n",
    "            causal_dfs[graph_id]['y_pred'] = y_pred\n",
    "\n",
    "        causal_df[gen_process_number] = causal_dfs\n",
    "        td2c_rocs_process[gen_process_number] = rocs\n",
    "        td2c_precision_process[gen_process_number] = precisions\n",
    "        td2c_recall_process[gen_process_number] = recalls\n",
    "        td2c_f1_process[gen_process_number] = f1s\n",
    "\n",
    "# pickle everything\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/third_estimation/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'wb') as f:\n",
    "    everything = (td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process, causal_df)\n",
    "    pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import results\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/third_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'rb') as f:\n",
    "    TD2C_2_rocs_process, TD2C_2_precision_process, TD2C_2_recall_process, TD2C_2_f1_process, causal_df_3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC scores PLOTS\n",
    "df1 = pd.DataFrame(TD2C_0_rocs_process)\n",
    "df2 = pd.DataFrame(TD2C_1_rocs_process)\n",
    "df3 = pd.DataFrame(TD2C_2_rocs_process)\n",
    "\n",
    "# Combine data for boxplot\n",
    "combined_data = []\n",
    "\n",
    "for col in df1.columns:\n",
    "    combined_data.append(df1[col])\n",
    "    combined_data.append(df2[col])\n",
    "    combined_data.append(df3[col])\n",
    "\n",
    "# Create labels for x-axis\n",
    "labels = []\n",
    "for col in df1.columns:\n",
    "    labels.append(f'{col} TD2C_Fist_Estimation')\n",
    "    labels.append(f'{col} TD2C_Second_Estimation')\n",
    "    labels.append(f'{col} TD2C_Third_Estimation')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "box = plt.boxplot(combined_data, patch_artist=True)\n",
    "\n",
    "# Color customization\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "for patch, i in zip(box['boxes'], range(len(box['boxes']))):\n",
    "    patch.set_facecolor(colors[i % 3])\n",
    "\n",
    "plt.xticks(range(1, len(labels) + 1), labels, rotation=-90)\n",
    "plt.title(f'Boxplot of ROC-AUC scores for TD2C and Iterative TD2C (with {k} top var) with Regression to estimate MI (5 variables processes)')\n",
    "plt.xlabel('Processes')\n",
    "plt.ylabel('ROC-AUC score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for df1 and df2 with the ROC-AUC scores for each process over all graphs\n",
    "df1.mean().mean(), df2.mean().mean() , df3.mean().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITERATIVE TD2C\n",
    "\n",
    "def iterative_td2c(method = 'ts', k = 1, it = 3, data_folder = 'home/data/', descr_folder = 'home/descr/', results_folder = 'home/results/'):\n",
    "    \"\"\"\n",
    "    # This function requires data already generated and stored in the data folder\n",
    "    # methods are: \n",
    "        # 'ts' = for classic TD2C\n",
    "        # 'original' = for original D2C\n",
    "        # 'ts_rank' = for TD2C with ranking\n",
    "        # 'ts_rank_2' = for TD2C with ranking 2\n",
    "        # 'ts_rank_3' = for TD2C with ranking 3\n",
    "        # 'ts_rank_4' = for TD2C with ranking 4\n",
    "        # 'ts_rank_5' = for TD2C with ranking 5\n",
    "        # 'ts_rank_6' = for TD2C with ranking 6\n",
    "        # 'ts_rank_7' = for TD2C with ranking 7\n",
    "        # 'ts_past' = for TD2C with past and future nodes\n",
    "        # 'ts_rank_no_count' = for TD2C with ranking with no contemporaneous nodes\n",
    "    # k is the number of top variables to keep at each iteration\n",
    "    # it is the number of iterations to perform\n",
    "    # data_folder is the folder where the data is stored\n",
    "    # descr_folder is the folder where the descriptors are stored\n",
    "    # results_folder is the folder where the results are stored\n",
    "\n",
    "    # COMPUTATION TIME: 20 min for it = 3, k = 1, method = 'ts'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Estimation Loop #################################################################################\n",
    "    for i in range(1,it+1):\n",
    "\n",
    "        input_folder = data_folder\n",
    "        \n",
    "        # first estimation\n",
    "        if i == 1:\n",
    "\n",
    "            output_folder = descr_folder + 'initial/'\n",
    "            \n",
    "            # Descriptors Generation #############################################################################\n",
    "            # List of files to process\n",
    "            to_process = []\n",
    "\n",
    "            # Filtering the files to process\n",
    "            for file in sorted(os.listdir(input_folder)):\n",
    "                gen_process_number = int(file.split('_')[0][1:])\n",
    "                n_variables = int(file.split('_')[1][1:])\n",
    "                max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "                noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "                if noise_std != noise_std_filter or max_neighborhood_size != max_neighborhood_size_filter:\n",
    "                    continue\n",
    "\n",
    "                to_process.append(file)\n",
    "\n",
    "            # Create output folder if it does not exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            # Process each file and create new DAGs based on causal paths\n",
    "            for file in tqdm(to_process):\n",
    "                gen_process_number = int(file.split('_')[0][1:])\n",
    "                n_variables = int(file.split('_')[1][1:])\n",
    "                max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "                noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "                dataloader = DataLoader(n_variables=n_variables, maxlags=maxlags)\n",
    "                dataloader.from_pickle(input_folder + file)\n",
    "\n",
    "                d2c = D2C(\n",
    "                    observations=dataloader.get_observations(),\n",
    "                    dags=dataloader.get_dags(),\n",
    "                    couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "                    MB_size=MB_SIZE,\n",
    "                    n_variables=n_variables,\n",
    "                    maxlags=maxlags,\n",
    "                    seed=SEED,\n",
    "                    n_jobs=N_JOBS,\n",
    "                    full=True,\n",
    "                    quantiles=True,\n",
    "                    normalize=True,\n",
    "                    cmi='original',\n",
    "                    mb_estimator=method,\n",
    "                    top_vars=3\n",
    "                )\n",
    "\n",
    "                d2c.initialize()  # Initializes the D2C object\n",
    "                descriptors_df = d2c.get_descriptors_df()  # Computes the descriptors\n",
    "\n",
    "                # Save the descriptors along with new DAGs if needed\n",
    "                descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "                descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "                descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "                descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "                descriptors_df.to_pickle(output_folder + f'Estimate{i}_P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')\n",
    "\n",
    "            # Set Classifier #################################################################################\n",
    "            data_root = data_folder\n",
    "\n",
    "            to_dos = []\n",
    "\n",
    "            # This loop gets a list of all the files to be processed\n",
    "            for testing_file in sorted(os.listdir(data_root)):\n",
    "                if testing_file.endswith('.pkl'):\n",
    "                    gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "                    n_variables = int(testing_file.split('_')[1][1:])\n",
    "                    max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "                    noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "                    \n",
    "                    if noise_std != 0.01: # if the noise is different we skip the file\n",
    "                        continue\n",
    "\n",
    "                    if max_neighborhood_size != 2: # if the max_neighborhood_size is different we skip the file\n",
    "                        continue\n",
    "\n",
    "                    to_dos.append(testing_file) # we add the file to the list (to_dos) to be processed\n",
    "\n",
    "            # sort to_dos by number of variables\n",
    "            to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "            # to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "            # to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "\n",
    "            # we create a dictionary with the lists of files to be processed\n",
    "            todos = {'5': to_dos_5_variables} # , '10': to_dos_10_variables, '25': to_dos_25_variables\n",
    "\n",
    "            # we create a dictionary to store the results\n",
    "            dfs = []\n",
    "            descriptors_root = output_folder\n",
    "\n",
    "            # Re-save pickle files with protocol 4\n",
    "            for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "                if testing_file.endswith('.pkl'):\n",
    "                    file_path = os.path.join(descriptors_root, testing_file)\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        data = pickle.load(f)\n",
    "                    \n",
    "                    # Re-save with protocol 4\n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        pickle.dump(data, f, protocol=4)\n",
    "\n",
    "            # This loop gets the descriptors for the files to be processed\n",
    "            for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "                if testing_file.endswith('.pkl'):\n",
    "                    df = pd.read_pickle(os.path.join(descriptors_root, testing_file))\n",
    "                    if isinstance(df, pd.DataFrame):\n",
    "                        dfs.append(df)\n",
    "\n",
    "            # we concatenate the descriptors\n",
    "            descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "            # Classifier & Evaluation Metrics #################################################################\n",
    "            for n_vars, todo in todos.items():\n",
    "                td2c_rocs_process = {}\n",
    "                td2c_precision_process = {}\n",
    "                td2c_recall_process = {}\n",
    "                td2c_f1_process = {}\n",
    "                causal_df = {}\n",
    "                for testing_file in tqdm(todo):\n",
    "                    gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "                    n_variables = int(testing_file.split('_')[1][1:])\n",
    "                    max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "                    noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "                    # split training and testing data\n",
    "                    training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "                    X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "                    y_train = training_data['is_causal']\n",
    "\n",
    "                    testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "                    model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "                    # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "                    model = model.fit(X_train, y_train)\n",
    "\n",
    "                    rocs = {}\n",
    "                    precisions = {}\n",
    "                    recalls = {}\n",
    "                    f1s = {}\n",
    "                    causal_dfs = {}\n",
    "                    for graph_id in range(40):\n",
    "                        #load testing descriptors\n",
    "                        test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "                        test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "                        X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "                        y_test = test_df['is_causal']\n",
    "\n",
    "                        y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "                        y_pred = model.predict(X_test)\n",
    "\n",
    "                        roc = roc_auc_score(y_test, y_pred_proba)\n",
    "                        precision = precision_score(y_test, y_pred)\n",
    "                        recall = recall_score(y_test, y_pred)\n",
    "                        f1 = f1_score(y_test, y_pred)\n",
    "                        \n",
    "                        rocs[graph_id] = roc\n",
    "                        precisions[graph_id] = precision\n",
    "                        recalls[graph_id] = recall\n",
    "                        f1s[graph_id] = f1\n",
    "                        \n",
    "                        # add to causal_df test_df, y_pred_proba and y_pred\n",
    "                        causal_dfs[graph_id] = test_df\n",
    "                        causal_dfs[graph_id]['y_pred_proba'] = y_pred_proba\n",
    "                        causal_dfs[graph_id]['y_pred'] = y_pred\n",
    "\n",
    "                    causal_df[gen_process_number] = causal_dfs\n",
    "                    td2c_rocs_process[gen_process_number] = rocs\n",
    "                    td2c_precision_process[gen_process_number] = precisions\n",
    "                    td2c_recall_process[gen_process_number] = recalls\n",
    "                    td2c_f1_process[gen_process_number] = f1s\n",
    "\n",
    "            # pickle everything\n",
    "            output_folder = results_folder + 'journals/initial/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            with open(os.path.join(output_folder, f'journal_results_td2c_R_N5.pkl'), 'wb') as f:\n",
    "                everything = (td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process, causal_df)\n",
    "                pickle.dump(everything, f)\n",
    "\n",
    "            # Load results #####################################################################################\n",
    "            input_folder = results_folder + 'journals/initial/'\n",
    "\n",
    "            with open(os.path.join(input_folder, 'journal_results_td2c_R_N5.pkl'), 'rb') as f:\n",
    "                TD2C_1_rocs_process, TD2C_1_precision_process, TD2C_1_recall_process, TD2C_1_f1_process, causal_df = pickle.load(f)\n",
    "\n",
    "            # Reshape causal_df #################################################################################\n",
    "            # keep only rows for top k y_pred_proba\n",
    "            for process_id, process_data in causal_df.items():\n",
    "                for graph_id, graph_data in process_data.items():\n",
    "                    causal_df[process_id][graph_id] = graph_data.nlargest(k, 'y_pred_proba')\n",
    "\n",
    "            # for each causal_df keep only process_id, graph_id, edge_source, edge_dest and y_pred_proba\n",
    "            for process_id, process_data in causal_df.items():\n",
    "                for graph_id, graph_data in process_data.items():\n",
    "                    causal_df[process_id][graph_id] = graph_data[['process_id', 'graph_id', 'edge_source', 'edge_dest', 'y_pred_proba']]\n",
    "                    causal_df[process_id][graph_id].reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # save the causal_df as a pkl file alone\n",
    "            output_folder = results_folder + 'journals/initial/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'wb') as f:\n",
    "                pickle.dump(causal_df, f)\n",
    "\n",
    "            print(causal_df[1][0])\n",
    "\n",
    "            # Unify causal_df #################################################################################\n",
    "            input_folder = results_folder + 'journals/initial/'\n",
    "\n",
    "            with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'rb') as f:\n",
    "                causal_df = pickle.load(f)\n",
    "\n",
    "            # create a dataframe with all the causal_df\n",
    "            dfs = []\n",
    "            for process_id, process_data in causal_df.items():\n",
    "                for graph_id, graph_data in process_data.items():\n",
    "                    dfs.append(graph_data)\n",
    "\n",
    "            causal_df_unif = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "            # sort in ascending order by process_id, graph_id, edge_source and edge_dest\n",
    "            causal_df_unif.sort_values(by=['process_id', 'graph_id', 'edge_source', 'edge_dest'], inplace=True)\n",
    "\n",
    "            # possible to insert that a couple must be present for all processes or for a certain percentage of processes (2/3)\n",
    "\n",
    "            # unique of causal_df_unif for couples of edge_source and edge_dest\n",
    "            causal_df_unif = causal_df_unif.drop_duplicates(subset=['edge_source', 'edge_dest'])\n",
    "\n",
    "            # drop rows with y_pred_proba < 0.7\n",
    "            causal_df_unif = causal_df_unif[causal_df_unif['y_pred_proba'] >= 0.7]\n",
    "\n",
    "            # if n row > 5, keep only the top 5 rows with highest y_pred_proba\n",
    "            if causal_df_unif.shape[0] > 5:\n",
    "                causal_df_unif = causal_df_unif.nlargest(5, 'y_pred_proba')\n",
    "\n",
    "            # index reset\n",
    "            causal_df_unif.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # save the causal_df as a pkl file alone\n",
    "            output_folder = results_folder + 'journals/initial/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'wb') as f:\n",
    "                pickle.dump(causal_df_unif, f)\n",
    "\n",
    "            print(causal_df_unif)\n",
    "            \n",
    "        # iterations from 2 to it\n",
    "        else:\n",
    "            \n",
    "            output_folder = descr_folder + f'estimate_{i}/'\n",
    "            \n",
    "            # Descriptors Generation #############################################################################\n",
    "            # List of files to process\n",
    "            to_process = []\n",
    "\n",
    "            # Filtering the files to process\n",
    "            for file in sorted(os.listdir(input_folder)):\n",
    "                gen_process_number = int(file.split('_')[0][1:])\n",
    "                n_variables = int(file.split('_')[1][1:])\n",
    "                max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "                noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "                if noise_std != noise_std_filter or max_neighborhood_size != max_neighborhood_size_filter:\n",
    "                    continue\n",
    "\n",
    "                to_process.append(file)\n",
    "\n",
    "            # Create output folder if it does not exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            # Process each file and create new DAGs based on causal paths\n",
    "            for file in tqdm(to_process):\n",
    "                gen_process_number = int(file.split('_')[0][1:])\n",
    "                n_variables = int(file.split('_')[1][1:])\n",
    "                max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "                noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "                dataloader = DataLoader(n_variables=n_variables, maxlags=maxlags)\n",
    "                dataloader.from_pickle(input_folder + file)\n",
    "\n",
    "                d2c = D2C(\n",
    "                    observations=dataloader.get_observations(),\n",
    "                    dags=dataloader.get_dags(),\n",
    "                    couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "                    MB_size=MB_SIZE,\n",
    "                    n_variables=n_variables,\n",
    "                    maxlags=maxlags,\n",
    "                    seed=SEED,\n",
    "                    n_jobs=N_JOBS,\n",
    "                    full=True,\n",
    "                    quantiles=True,\n",
    "                    normalize=True,\n",
    "                    cmi='original',\n",
    "                    mb_estimator='ts',\n",
    "                    top_vars=3,\n",
    "                    causal_df=causal_df_unif\n",
    "                )\n",
    "\n",
    "                d2c.initialize()  # Initializes the D2C object\n",
    "                descriptors_df = d2c.get_descriptors_df()  # Computes the descriptors\n",
    "\n",
    "                # Save the descriptors along with new DAGs if needed\n",
    "                descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "                descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "                descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "                descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "                descriptors_df.to_pickle(output_folder + f'Estimate_{i}_P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')\n",
    "\n",
    "            # Set Classifier #################################################################################\n",
    "            data_root = data_folder\n",
    "\n",
    "            to_dos = []\n",
    "\n",
    "            # This loop gets a list of all the files to be processed\n",
    "            for testing_file in sorted(os.listdir(data_root)):\n",
    "                if testing_file.endswith('.pkl'):\n",
    "                    gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "                    n_variables = int(testing_file.split('_')[1][1:])\n",
    "                    max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "                    noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "                    \n",
    "                    if noise_std != 0.01: # if the noise is different we skip the file\n",
    "                        continue\n",
    "\n",
    "                    if max_neighborhood_size != 2: # if the max_neighborhood_size is different we skip the file\n",
    "                        continue\n",
    "\n",
    "                    to_dos.append(testing_file) # we add the file to the list (to_dos) to be processed\n",
    "\n",
    "            # sort to_dos by number of variables\n",
    "            to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "            # to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "            # to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "\n",
    "            # we create a dictionary with the lists of files to be processed\n",
    "            todos = {'5': to_dos_5_variables} # , '10': to_dos_10_variables, '25': to_dos_25_variables\n",
    "\n",
    "            # we create a dictionary to store the results\n",
    "            dfs = []\n",
    "            descriptors_root = descr_folder + f'estimate_{i}/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(descriptors_root):\n",
    "                os.makedirs(descriptors_root)\n",
    "\n",
    "            # Re-save pickle files with protocol 4\n",
    "            for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "                if testing_file.endswith('.pkl'):\n",
    "                    file_path = os.path.join(descriptors_root, testing_file)\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        data = pickle.load(f)\n",
    "                    \n",
    "                    # Re-save with protocol 4\n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        pickle.dump(data, f, protocol=4)\n",
    "\n",
    "            # This loop gets the descriptors for the files to be processed\n",
    "            for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "                if testing_file.endswith('.pkl'):\n",
    "                    df = pd.read_pickle(os.path.join(descriptors_root, testing_file))\n",
    "                    if isinstance(df, pd.DataFrame):\n",
    "                        dfs.append(df)\n",
    "\n",
    "            # we concatenate the descriptors\n",
    "            descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "            # Classifier & Evaluation Metrics #################################################################\n",
    "            for n_vars, todo in todos.items():\n",
    "\n",
    "                m1 = f'Estimate_{i}_rocs_process'\n",
    "                m2 = f'Estimate_{i}_precision_process'\n",
    "                m3 = f'Estimate_{i}_recall_process'\n",
    "                m4 = f'Estimate_{i}_f1_process'\n",
    "\n",
    "                globals()[m1] = {}\n",
    "                globals()[m2] = {}\n",
    "                globals()[m3] = {}\n",
    "                globals()[m4] = {}\n",
    "                causal_df_1 = {}\n",
    "\n",
    "                for testing_file in tqdm(todo):\n",
    "                    gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "                    n_variables = int(testing_file.split('_')[1][1:])\n",
    "                    max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "                    noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "                    # split training and testing data\n",
    "                    training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "                    X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "                    y_train = training_data['is_causal']\n",
    "\n",
    "                    testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "                    model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "                    # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "                    model = model.fit(X_train, y_train)\n",
    "\n",
    "                    rocs = {}\n",
    "                    precisions = {}\n",
    "                    recalls = {}\n",
    "                    f1s = {}\n",
    "                    causal_dfs = {}\n",
    "                    for graph_id in range(40):\n",
    "                        #load testing descriptors\n",
    "                        test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "                        test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "                        X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "                        y_test = test_df['is_causal']\n",
    "\n",
    "                        y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "                        y_pred = model.predict(X_test)\n",
    "\n",
    "                        roc = roc_auc_score(y_test, y_pred_proba)\n",
    "                        precision = precision_score(y_test, y_pred)\n",
    "                        recall = recall_score(y_test, y_pred)\n",
    "                        f1 = f1_score(y_test, y_pred)\n",
    "                        \n",
    "                        rocs[graph_id] = roc\n",
    "                        precisions[graph_id] = precision\n",
    "                        recalls[graph_id] = recall\n",
    "                        f1s[graph_id] = f1\n",
    "                        \n",
    "                        # add to causal_df test_df, y_pred_proba and y_pred\n",
    "                        causal_dfs[graph_id] = test_df\n",
    "                        causal_dfs[graph_id]['y_pred_proba'] = y_pred_proba\n",
    "                        causal_dfs[graph_id]['y_pred'] = y_pred\n",
    "\n",
    "                    causal_df_1[gen_process_number] = causal_dfs\n",
    "                    globals()[m1][gen_process_number] = rocs\n",
    "                    globals()[m2][gen_process_number] = precisions\n",
    "                    globals()[m3][gen_process_number] = recalls\n",
    "                    globals()[m4][gen_process_number] = f1s\n",
    "\n",
    "            # pickle everything\n",
    "            output_folder = results_folder + f'journals/estimate_{i}/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            with open(os.path.join(output_folder, f'journal_results_td2c_R_N5.pkl'), 'wb') as f:\n",
    "                everything = (globals()[m1], globals()[m2], globals()[m3], globals()[m4], causal_df_1)\n",
    "                pickle.dump(everything, f)\n",
    "\n",
    "            # Load results #####################################################################################\n",
    "            input_folder = results_folder + f'journals/estimate_{i}/'\n",
    "\n",
    "            with open(os.path.join(input_folder, f'journal_results_td2c_R_N5.pkl'), 'rb') as f:\n",
    "                TD2C_1_rocs_process, TD2C_1_precision_process, TD2C_1_recall_process, TD2C_1_f1_process, causal_df_1 = pickle.load(f)\n",
    "\n",
    "            # Reshape causal_df #################################################################################\n",
    "            # keep only rows for top k y_pred_proba\n",
    "            for process_id, process_data in causal_df_1.items():\n",
    "                for graph_id, graph_data in process_data.items():\n",
    "                    causal_df_1[process_id][graph_id] = graph_data.nlargest(k, 'y_pred_proba')\n",
    "\n",
    "            # for each causal_df keep only process_id, graph_id, edge_source, edge_dest and y_pred_proba\n",
    "            for process_id, process_data in causal_df_1.items():\n",
    "                for graph_id, graph_data in process_data.items():\n",
    "                    causal_df_1[process_id][graph_id] = graph_data[['process_id', 'graph_id', 'edge_source', 'edge_dest', 'y_pred_proba']]\n",
    "                    causal_df_1[process_id][graph_id].reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # save the causal_df as a pkl file alone\n",
    "            output_folder = results_folder + f'journals/estimate_{i}/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'wb') as f:\n",
    "                pickle.dump(causal_df_1, f)\n",
    "\n",
    "            print(f'causal_df_{i} is: {causal_df_1[1][0]}')\n",
    "\n",
    "            # Unify causal_df #################################################################################\n",
    "            input_folder = results_folder + f'journals/estimate_{i}/'\n",
    "\n",
    "            with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'rb') as f:\n",
    "                causal_df_1 = pickle.load(f)\n",
    "\n",
    "            # create a dataframe with all the causal_df\n",
    "            dfs = []\n",
    "            for process_id, process_data in causal_df_1.items():\n",
    "                for graph_id, graph_data in process_data.items():\n",
    "                    dfs.append(graph_data)\n",
    "\n",
    "            causal_df_unif_1 = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "            # sort in ascending order by process_id, graph_id, edge_source and edge_dest\n",
    "            causal_df_unif_1.sort_values(by=['process_id', 'graph_id', 'edge_source', 'edge_dest'], inplace=True)\n",
    "\n",
    "            # possible to insert that a couple must be present for all processes or for a certain percentage of processes (2/3)\n",
    "\n",
    "            # unique of causal_df_unif for couples of edge_source and edge_dest\n",
    "            causal_df_unif_1 = causal_df_unif_1.drop_duplicates(subset=['edge_source', 'edge_dest'])\n",
    "\n",
    "            # drop rows with y_pred_proba < 0.7\n",
    "            causal_df_unif_1 = causal_df_unif_1[causal_df_unif_1['y_pred_proba'] >= 0.7]\n",
    "\n",
    "            # if n row > 5, keep only the top 5 rows with highest y_pred_proba\n",
    "            if causal_df_unif_1.shape[0] > 5:\n",
    "                causal_df_unif_1 = causal_df_unif_1.nlargest(5, 'y_pred_proba')\n",
    "\n",
    "            # index reset\n",
    "            causal_df_unif_1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # save the causal_df as a pkl file alone\n",
    "            output_folder = results_folder + f'journals/estimate_{i}/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'wb') as f:\n",
    "                pickle.dump(causal_df_unif_1, f)\n",
    "\n",
    "            print(f'causal_df_unif_{i} is {causal_df_unif_1}')\n",
    "    \n",
    "    # Plotting #############################################################################################\n",
    "\n",
    "    # ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_td2c(method = 'ts', k = 1, it = 3, \n",
    "               data_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/', \n",
    "               descr_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/try_complete_function/', \n",
    "               results_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/try_complete_function/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "it = 3\n",
    "method = 'TD2C'\n",
    "\n",
    "# load results\n",
    "input_folder_0 = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/try_complete_function/journals/initial/'\n",
    "input_folder_1 = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/try_complete_function/journals/estimate_2/'\n",
    "input_folder_2 = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/try_complete_function/journals/estimate_3/'\n",
    "\n",
    "with open(os.path.join(input_folder_0, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'rb') as f:\n",
    "    TD2C_0_rocs_process, TD2C_0_precision_process, TD2C_0_recall_process, TD2C_0_f1_process, causal_df_0 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(input_folder_1, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'rb') as f:\n",
    "    Estimate_1_rocs_process, Estimate_1_precision_process, Estimate_1_recall_process, Estimate_1_f1_process, causal_df_1 = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(input_folder_2, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'rb') as f:\n",
    "    Estimate_2_rocs_process, Estimate_2_precision_process, Estimate_2_recall_process, Estimate_2_f1_process, causal_df_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "it = 3\n",
    "method = 'TD2C'\n",
    "\n",
    "# ROC-AUC scores PLOTS\n",
    "df1 = pd.DataFrame(TD2C_0_rocs_process)\n",
    "df2 = pd.DataFrame(Estimate_1_rocs_process)\n",
    "df3 = pd.DataFrame(Estimate_2_rocs_process)\n",
    "# df4 = pd.DataFrame(Estimate_3_rocs_process)\n",
    "# df5 = pd.DataFrame(Estimate_4_rocs_process)\n",
    "# df6 = pd.DataFrame(Estimate_5_rocs_process)\n",
    "# df7 = pd.DataFrame(Estimate_6_rocs_process)\n",
    "# df8 = pd.DataFrame(Estimate_7_rocs_process)\n",
    "# df9 = pd.DataFrame(Estimate_8_rocs_process)\n",
    "# df10 = pd.DataFrame(Estimate_9_rocs_process)\n",
    "\n",
    "# Combine data for boxplot\n",
    "combined_data = []\n",
    "\n",
    "for col in df1.columns:\n",
    "    combined_data.append(df1[col])\n",
    "    combined_data.append(df2[col])\n",
    "    combined_data.append(df3[col])\n",
    "    # combined_data.append(df4[col])\n",
    "    # combined_data.append(df5[col])\n",
    "    # combined_data.append(df6[col])\n",
    "    # combined_data.append(df7[col])\n",
    "    # combined_data.append(df8[col])\n",
    "    # combined_data.append(df9[col])\n",
    "    # combined_data.append(df10[col])\n",
    "\n",
    "\n",
    "# Create labels for x-axis\n",
    "labels = []\n",
    "for col in df1.columns:\n",
    "    labels.append(f'{col} TD2C_Fist_Estimation')\n",
    "    labels.append(f'{col} TD2C_Second_Estimation')\n",
    "    labels.append(f'{col} TD2C_Third_Estimation')\n",
    "    # labels.append(f'{col} TD2C_Fourth_Estimation')\n",
    "    # labels.append(f'{col} TD2C_Fifth_Estimation')\n",
    "    # labels.append(f'{col} TD2C_Sixth_Estimation')\n",
    "    # labels.append(f'{col} TD2C_Seventh_Estimation')\n",
    "    # labels.append(f'{col} TD2C_Eighth_Estimation')\n",
    "    # labels.append(f'{col} TD2C_Ninth_Estimation')\n",
    "    # labels.append(f'{col} TD2C_Tenth_Estimation')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "box = plt.boxplot(combined_data, patch_artist=True)\n",
    "\n",
    "# Color customization\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "for patch, i in zip(box['boxes'], range(len(box['boxes']))):\n",
    "    patch.set_facecolor(colors[i % 3])\n",
    "\n",
    "plt.xticks(range(1, len(labels) + 1), labels, rotation=-90)\n",
    "plt.title(f'Boxplot of ROC-AUC scores for Iterative {method} ({it} iterations and {k} top var) with Regression MI (5 vars processes)')\n",
    "plt.xlabel('Processes')\n",
    "plt.ylabel('ROC-AUC score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "it = 3\n",
    "method = 'TD2C'\n",
    "\n",
    "roc1 = 0.84 #df1.mean().mean()\n",
    "roc2 = 0.85 #df2.mean().mean()\n",
    "roc3 = 0.83 #df3.mean().mean()\n",
    "# roc4 = df4.mean().mean()\n",
    "# roc5 = df5.mean().mean()\n",
    "# roc6 = df6.mean().mean()\n",
    "# roc7 = df7.mean().mean()\n",
    "# roc8 = df8.mean().mean()\n",
    "# roc9 = df9.mean().mean()\n",
    "# roc10 = df10.mean().mean()\n",
    "\n",
    "# make a vector with roc scores\n",
    "roc_scores = [roc1, roc2, roc3] # , roc4, roc5, roc6, roc7, roc8, roc9, roc10\n",
    "\n",
    "# line plot of the roc scores\n",
    "plt.plot(roc_scores)\n",
    "plt.title(f'ROC-AUC scores for Iterative {method} ({it} iterations and {k} top var) with Regression MI (5 vars processes)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('ROC-AUC score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
