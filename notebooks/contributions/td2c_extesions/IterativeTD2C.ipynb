{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative TD2C Comparison with Benchmark Methods\n",
    "\n",
    "This notebook presents a comparative analysis between an Iterative implementation of the `TD2C` method and several benchmark approaches. The `TD2C` algorithm is extended in this work through an iterative process that refines its performance by leveraging dynamic feature selection and causal graph construction.\n",
    "\n",
    "The proposed Iterative `TD2C` method involves the following steps:\n",
    "\n",
    "1. **Model Training and Initial Causal Discovery**: The model is initially trained, and the TD2C algorithm is applied to identify the top K causal connections.\n",
    "2. **Graph Construction**: A causal graph is constructed based on the identified connections.\n",
    "3. **Historical Extension**: The graph is extended to include past states, reflecting the progression from past to present.\n",
    "4. **Application of Meek Rules**: The graph is refined using Meek's Rules to ensure proper orientation of edges and to derive causal implications.\n",
    "5. **Missing Pair Identification and Markov Blanket Derivation**: Missing causal pairs are identified, and the Markov Blanket (MB) is derived if possible from the current graph structure.\n",
    "6. **Descriptor Recalculation**: Descriptors are recalculated using the derived MB, providing refined inputs for subsequent iterations.\n",
    "\n",
    "Through this iterative process, the TD2C method is expected to enhance its accuracy in detecting causal structures, making it a robust tool for temporal causal discovery. The effectiveness of this approach will be evaluated against benchmark methods identified from previous analyses, specifically from the `Compare_TD2C_MB_Strategies` notebook, to assess its performance improvements in terms of ROC-AUC, Precision, Recall adn F1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import pickle \n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from d2c.data_generation.builder import TSBuilder\n",
    "from d2c.descriptors_generation import D2C, DataLoader\n",
    "from d2c.benchmark import D2CWrapper\n",
    "from d2c.benchmark import IterativeTD2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = 40 # number of jobs to run in parallel. For D2C, parallelism is implemented at the observation level: each observation from a single file is processed in parallel\n",
    "SEED = 42 # random seed for reproducibility\n",
    "MB_SIZE = 2 # size to consider when estimating the markov blanket. This is only useful if the MB is actually estimated\n",
    "COUPLES_TO_CONSIDER_PER_DAG = -1 # edges that are considered in total to compute descriptors, for each TS. This can speed up the process. If set to -1, all possible edges are considered\n",
    "maxlags = 5 # maximum lags to consider when considering variable couples\n",
    "noise_std_filter = 0.01  # Example noise standard deviation to filter\n",
    "max_neighborhood_size_filter = 2  # Example filter for neighborhood size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "Data are generated with the `TSBuilder` class and saved in a specific folder. Then, the code checks for missing datasets in the folder and keeps running untill all the possible combinations of parameters have genereted a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET THE TSBUILDER WITH THE DESIRED PARAMETERS\n",
    "def run_process(params):\n",
    "    \"\"\"\n",
    "    Run a single process of the data generation.\n",
    "    \"\"\"\n",
    "    process, n_variables, max_neighborhood_size, noise_std = params\n",
    "    try:\n",
    "        tsbuilder = TSBuilder(observations_per_time_series=250, \n",
    "                              maxlags=5, \n",
    "                              n_variables=n_variables, \n",
    "                              time_series_per_process=40, \n",
    "                              processes_to_use=[process], \n",
    "                              noise_std=noise_std, \n",
    "                              max_neighborhood_size=max_neighborhood_size, \n",
    "                              seed=42, \n",
    "                              max_attempts=200,\n",
    "                              verbose=True)\n",
    "\n",
    "        tsbuilder.build()\n",
    "        tsbuilder.to_pickle(f'/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl')\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} done')\n",
    "    except ValueError as e:\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} failed: {e}')\n",
    "\n",
    "# BUILD THE DATA\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    This script generates the data for different parameters: processes, number of variables, neighborhood sizes and noise levels.\n",
    "    The data is saved in the .data folder.\n",
    "    The if __name__ == '__main__': is used to avoid multiprocessing issues in Jupyter notebooks, i.e. the script is run as a script and not\n",
    "    as a module as it would have been if the script was imported, with the __name__ being the name of the module.\n",
    "    If the script is imported, the __name__ is the name of the module, if it is run as a script, the __name__ is __main__.\n",
    "    So, to run this script in a Jupyter notebook, we write the code inside the if __name__ == '__main__': block, while, if we want to import\n",
    "    the functions from this script, we write \"from script import run_process\".\n",
    "    \"\"\"\n",
    "    parameters = [(process, n_variables, max_neighborhood_size, noise_std)\n",
    "                    for process in [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20] # \n",
    "                    for n_variables in [5] # , 10, 25\n",
    "                    for max_neighborhood_size in [2] # , 4, 8\n",
    "                    for noise_std in [0.01]] # , 0.005, 0.001\n",
    "\n",
    "    with Pool(processes=N_JOBS) as pool:\n",
    "        pool.map(run_process, parameters)\n",
    "\n",
    "\n",
    "# Function to check for missing files\n",
    "def check_missing_files():\n",
    "    missing = []\n",
    "    for process in [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20]:  # \n",
    "        for n_variables in [5]:  # , 10, 25\n",
    "            for max_neighborhood_size in [2]:  # , 4, 8\n",
    "                for noise_std in [0.01]:  # , 0.005, 0.001\n",
    "                    filename = f'/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "                    if not os.path.exists(filename):\n",
    "                        missing.append(filename)\n",
    "    return missing\n",
    "\n",
    "# Function to run the process\n",
    "def run_process(params):\n",
    "    process, n_variables, max_neighborhood_size, noise_std = params\n",
    "    try:  # we change the seed and increase the max_attempts\n",
    "        tsbuilder = TSBuilder(observations_per_time_series=250, \n",
    "                              maxlags=5, \n",
    "                              n_variables=n_variables, \n",
    "                              time_series_per_process=40, \n",
    "                              processes_to_use=[process], \n",
    "                              noise_std=noise_std, \n",
    "                              max_neighborhood_size=max_neighborhood_size, \n",
    "                              seed=24, \n",
    "                              max_attempts=400,\n",
    "                              verbose=True)\n",
    "\n",
    "        tsbuilder.build()\n",
    "        tsbuilder.to_pickle(f'/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl')\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} done')\n",
    "    except ValueError as e:\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} failed: {e}')\n",
    "\n",
    "# CHECK FOR MISSING FILES (IT CHEKS THE FILES WITH A DIFFERENT SEED AND MORE MAX_ATTEMPTS UNTILL MISSING IS EMPTY)\n",
    "if __name__ == '__main__':\n",
    "    while True:\n",
    "        missing = check_missing_files()\n",
    "        if not missing:\n",
    "            break\n",
    "\n",
    "        parameters = []\n",
    "        for missing_file in missing:\n",
    "            process = int(missing_file.split('/')[-1].split('_')[0][1:])\n",
    "            n_variables = int(missing_file.split('/')[-1].split('_')[1][1:])\n",
    "            max_neighborhood_size = int(missing_file.split('/')[-1].split('_')[2][2:])\n",
    "            noise_std = float(missing_file.split('/')[-1].split('_')[3][1:-4])\n",
    "            parameters.append((process, n_variables, max_neighborhood_size, noise_std))\n",
    "\n",
    "        with Pool(processes=N_JOBS) as pool:\n",
    "            pool.map(run_process, parameters)\n",
    "\n",
    "len(os.listdir('/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative TD2C implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First estimate\n",
    "2. top k (5) causally connected couples\n",
    "3. function that integrates nodes in these top k couples where necessary (when the MB for the destination node is computed)\n",
    "4. 2nd estimate (in theory better)\n",
    "5. keep going for each iteration\n",
    "6. plot evalutation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working phase (Not for users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set k\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/initial/'\n",
    "\n",
    "# List of files to process\n",
    "to_process = []\n",
    "\n",
    "# Filtering the files to process\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    if noise_std != noise_std_filter or max_neighborhood_size != max_neighborhood_size_filter:\n",
    "        continue\n",
    "\n",
    "    to_process.append(file)\n",
    "\n",
    "# Create output folder if it does not exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Process each file and create new DAGs based on causal paths\n",
    "for file in tqdm(to_process):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    dataloader = DataLoader(n_variables=n_variables, maxlags=maxlags)\n",
    "    dataloader.from_pickle(input_folder + file)\n",
    "\n",
    "    d2c = D2C(\n",
    "        observations=dataloader.get_observations(),\n",
    "        dags=dataloader.get_dags(),\n",
    "        couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "        MB_size=MB_SIZE,\n",
    "        n_variables=n_variables,\n",
    "        maxlags=maxlags,\n",
    "        seed=SEED,\n",
    "        n_jobs=N_JOBS,\n",
    "        full=True,\n",
    "        quantiles=True,\n",
    "        normalize=True,\n",
    "        cmi='original',\n",
    "        mb_estimator='ts',\n",
    "        top_vars=3\n",
    "    )\n",
    "\n",
    "    d2c.initialize()  # Initializes the D2C object\n",
    "    descriptors_df = d2c.get_descriptors_df()  # Computes the descriptors\n",
    "\n",
    "    # Save the descriptors along with new DAGs if needed\n",
    "    descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "    descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "    descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "    descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "    descriptors_df.to_pickle(output_folder + f'P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "\n",
    "to_dos = []\n",
    "\n",
    "# This loop gets a list of all the files to be processed\n",
    "for testing_file in sorted(os.listdir(data_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "          \n",
    "        if noise_std != 0.01: # if the noise is different we skip the file\n",
    "            continue\n",
    "\n",
    "        if max_neighborhood_size != 2: # if the max_neighborhood_size is different we skip the file\n",
    "            continue\n",
    "\n",
    "        to_dos.append(testing_file) # we add the file to the list (to_dos) to be processed\n",
    "\n",
    "# sort to_dos by number of variables\n",
    "to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "# to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "# to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "\n",
    "# we create a dictionary with the lists of files to be processed\n",
    "todos = {'5': to_dos_5_variables} # , '10': to_dos_10_variables, '25': to_dos_25_variables\n",
    "\n",
    "# we create a dictionary to store the results\n",
    "dfs = []\n",
    "descriptors_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/initial/'\n",
    "\n",
    "# Re-save pickle files with protocol 4\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        file_path = os.path.join(descriptors_root, testing_file)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Re-save with protocol 4\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f, protocol=4)\n",
    "\n",
    "# This loop gets the descriptors for the files to be processed\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(os.path.join(descriptors_root, testing_file))\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            dfs.append(df)\n",
    "\n",
    "# we concatenate the descriptors\n",
    "descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop does the following:\n",
    "# 1. Creates some dictionaries to store the results\n",
    "# 2. Loads the training data\n",
    "# 3. Trains the model\n",
    "# 4. Evaluates the model\n",
    "# 5. Stores the results in the dictionaries\n",
    "# 6. Saves the dictionaries in a pickle file\n",
    "\n",
    "for n_vars, todo in todos.items():\n",
    "    td2c_rocs_process = {}\n",
    "    td2c_precision_process = {}\n",
    "    td2c_recall_process = {}\n",
    "    td2c_f1_process = {}\n",
    "    causal_df = {}\n",
    "    for testing_file in tqdm(todo):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "        # split training and testing data\n",
    "        training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "        model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "        # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "        model = model.fit(X_train, y_train)\n",
    "\n",
    "        rocs = {}\n",
    "        precisions = {}\n",
    "        recalls = {}\n",
    "        f1s = {}\n",
    "        causal_dfs = {}\n",
    "        for graph_id in range(40):\n",
    "            #load testing descriptors\n",
    "            test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "            test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "            X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_test = test_df['is_causal']\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            roc = roc_auc_score(y_test, y_pred_proba)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            rocs[graph_id] = roc\n",
    "            precisions[graph_id] = precision\n",
    "            recalls[graph_id] = recall\n",
    "            f1s[graph_id] = f1\n",
    "            \n",
    "            # add to causal_df test_df, y_pred_proba and y_pred\n",
    "            causal_dfs[graph_id] = test_df\n",
    "            causal_dfs[graph_id]['y_pred_proba'] = y_pred_proba\n",
    "            causal_dfs[graph_id]['y_pred'] = y_pred\n",
    "\n",
    "        causal_df[gen_process_number] = causal_dfs\n",
    "        td2c_rocs_process[gen_process_number] = rocs\n",
    "        td2c_precision_process[gen_process_number] = precisions\n",
    "        td2c_recall_process[gen_process_number] = recalls\n",
    "        td2c_f1_process[gen_process_number] = f1s\n",
    "\n",
    "# pickle everything\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'journal_results_td2c_R_N5.pkl'), 'wb') as f:\n",
    "    everything = (td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process, causal_df)\n",
    "    pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import causal_df \n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "with open(os.path.join(input_folder, 'journal_results_td2c_R_N5.pkl'), 'rb') as f:\n",
    "    TD2C_0_rocs_process, TD2C_0_precision_process, TD2C_0_recall_process, TD2C_0_f1_process, causal_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshape causal_df and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only rows for top k y_pred_proba\n",
    "for process_id, process_data in causal_df.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        causal_df[process_id][graph_id] = graph_data.nlargest(k, 'y_pred_proba')\n",
    "\n",
    "# for each causal_df keep only process_id, graph_id, edge_source, edge_dest and y_pred_proba\n",
    "for process_id, process_data in causal_df.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        causal_df[process_id][graph_id] = graph_data[['process_id', 'graph_id', 'edge_source', 'edge_dest', 'y_pred_proba']]\n",
    "        causal_df[process_id][graph_id].reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save the causal_df as a pkl file alone\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'wb') as f:\n",
    "    pickle.dump(causal_df, f)\n",
    "\n",
    "causal_df[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load reshaped causal_df, unify it and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'rb') as f:\n",
    "    causal_df = pickle.load(f)\n",
    "\n",
    "# create a dataframe with all the causal_df\n",
    "dfs = []\n",
    "for process_id, process_data in causal_df.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        dfs.append(graph_data)\n",
    "\n",
    "causal_df_unif = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "# sort in ascending order by process_id, graph_id, edge_source and edge_dest\n",
    "causal_df_unif.sort_values(by=['process_id', 'graph_id', 'edge_source', 'edge_dest'], inplace=True)\n",
    "\n",
    "# possible to insert that a couple must be present for all processes or for a certain percentage of processes (2/3)\n",
    "\n",
    "# unique of causal_df_unif for couples of edge_source and edge_dest\n",
    "causal_df_unif = causal_df_unif.drop_duplicates(subset=['edge_source', 'edge_dest'])\n",
    "\n",
    "# drop rows with y_pred_proba < 0.7\n",
    "causal_df_unif = causal_df_unif[causal_df_unif['y_pred_proba'] >= 0.7]\n",
    "\n",
    "# keep only the top 5 rows with highest y_pred_proba\n",
    "causal_df_unif = causal_df_unif.nlargest(5, 'y_pred_proba')\n",
    "\n",
    "# index reset\n",
    "causal_df_unif.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save the causal_df as a pkl file alone\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'wb') as f:\n",
    "    pickle.dump(causal_df_unif, f)\n",
    "\n",
    "causal_df_unif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of top variables to keep\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load unified dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df_unified\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'rb') as f:\n",
    "    causal_df_unif = pickle.load(f)\n",
    "\n",
    "causal_df_unif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Descriptors Generation using causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/second_estimate/'\n",
    "# List of files to process\n",
    "to_process = []\n",
    "\n",
    "# Filtering the files to process\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    if noise_std != noise_std_filter or max_neighborhood_size != max_neighborhood_size_filter:\n",
    "        continue\n",
    "\n",
    "    to_process.append(file)\n",
    "\n",
    "# Create output folder if it does not exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Process each file and create new DAGs based on causal paths\n",
    "for file in tqdm(to_process):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    dataloader = DataLoader(n_variables=n_variables, maxlags=maxlags)\n",
    "    dataloader.from_pickle(input_folder + file)\n",
    "\n",
    "    d2c = D2C(\n",
    "        observations=dataloader.get_observations(),\n",
    "        dags=dataloader.get_dags(),\n",
    "        couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "        MB_size=MB_SIZE,\n",
    "        n_variables=n_variables,\n",
    "        maxlags=maxlags,\n",
    "        seed=SEED,\n",
    "        n_jobs=N_JOBS,\n",
    "        full=True,\n",
    "        quantiles=True,\n",
    "        normalize=True,\n",
    "        cmi='original',\n",
    "        mb_estimator='iterative',\n",
    "        top_vars = 3,\n",
    "        causal_df = causal_df\n",
    "    )\n",
    "\n",
    "    d2c.initialize()  # Initializes the D2C object\n",
    "    descriptors_df = d2c.get_descriptors_df()  # Computes the descriptors\n",
    "\n",
    "    # Save the descriptors along with new DAGs if needed\n",
    "    descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "    descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "    descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "    descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "    descriptors_df.to_pickle(output_folder + f'P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "\n",
    "to_dos = []\n",
    "\n",
    "# This loop gets a list of all the files to be processed\n",
    "for testing_file in sorted(os.listdir(data_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "          \n",
    "        if noise_std != 0.01: # if the noise is different we skip the file\n",
    "            continue\n",
    "\n",
    "        if max_neighborhood_size != 2: # if the max_neighborhood_size is different we skip the file\n",
    "            continue\n",
    "\n",
    "        to_dos.append(testing_file) # we add the file to the list (to_dos) to be processed\n",
    "\n",
    "# sort to_dos by number of variables\n",
    "to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "# to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "# to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "\n",
    "# we create a dictionary with the lists of files to be processed\n",
    "todos = {'5': to_dos_5_variables} # , '10': to_dos_10_variables, '25': to_dos_25_variables\n",
    "\n",
    "# we create a dictionary to store the results\n",
    "dfs = []\n",
    "descriptors_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/second_estimate/'\n",
    "\n",
    "# Re-save pickle files with protocol 4\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        file_path = os.path.join(descriptors_root, testing_file)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Re-save with protocol 4\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f, protocol=4)\n",
    "\n",
    "# This loop gets the descriptors for the files to be processed\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(os.path.join(descriptors_root, testing_file))\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            dfs.append(df)\n",
    "\n",
    "# we concatenate the descriptors\n",
    "descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop does the following:\n",
    "# 1. Creates some dictionaries to store the results\n",
    "# 2. Loads the training data\n",
    "# 3. Trains the model\n",
    "# 4. Evaluates the model\n",
    "# 5. Stores the results in the dictionaries\n",
    "# 6. Saves the dictionaries in a pickle file\n",
    "\n",
    "for n_vars, todo in todos.items():\n",
    "    td2c_rocs_process = {}\n",
    "    td2c_precision_process = {}\n",
    "    td2c_recall_process = {}\n",
    "    td2c_f1_process = {}\n",
    "    causal_df = {}\n",
    "    for testing_file in tqdm(todo):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "        # split training and testing data\n",
    "        training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "        model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "        # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "        model = model.fit(X_train, y_train)\n",
    "\n",
    "        rocs = {}\n",
    "        precisions = {}\n",
    "        recalls = {}\n",
    "        f1s = {}\n",
    "        causal_dfs = {}\n",
    "        for graph_id in range(40):\n",
    "            #load testing descriptors\n",
    "            test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "            test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "            X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_test = test_df['is_causal']\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            roc = roc_auc_score(y_test, y_pred_proba)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            rocs[graph_id] = roc\n",
    "            precisions[graph_id] = precision\n",
    "            recalls[graph_id] = recall\n",
    "            f1s[graph_id] = f1\n",
    "            \n",
    "            # add to causal_df test_df, y_pred_proba and y_pred\n",
    "            causal_dfs[graph_id] = test_df\n",
    "            causal_dfs[graph_id]['y_pred_proba'] = y_pred_proba\n",
    "            causal_dfs[graph_id]['y_pred'] = y_pred\n",
    "\n",
    "        causal_df[gen_process_number] = causal_dfs\n",
    "        td2c_rocs_process[gen_process_number] = rocs\n",
    "        td2c_precision_process[gen_process_number] = precisions\n",
    "        td2c_recall_process[gen_process_number] = recalls\n",
    "        td2c_f1_process[gen_process_number] = f1s\n",
    "\n",
    "# pickle everything\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'wb') as f:\n",
    "    everything = (td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process, causal_df)\n",
    "    pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import results\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'rb') as f:\n",
    "    TD2C_1_rocs_process, TD2C_1_precision_process, TD2C_1_recall_process, TD2C_1_f1_process, causal_df_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshape causal_df and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'rb') as f:\n",
    "    TD2C_1_rocs_process, TD2C_1_precision_process, TD2C_1_recall_process, TD2C_1_f1_process, causal_df_2 = pickle.load(f)\n",
    "\n",
    "# keep only rows for top k y_pred_proba\n",
    "for process_id, process_data in causal_df_2.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        causal_df_2[process_id][graph_id] = graph_data.nlargest(k, 'y_pred_proba')\n",
    "\n",
    "# for each causal_df keep only process_id, graph_id, edge_source, edge_dest and y_pred_proba\n",
    "for process_id, process_data in causal_df_2.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        causal_df_2[process_id][graph_id] = graph_data[['process_id', 'graph_id', 'edge_source', 'edge_dest', 'y_pred_proba']]\n",
    "        causal_df_2[process_id][graph_id].reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save the causal_df as a pkl file alone\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'wb') as f:\n",
    "    pickle.dump(causal_df_2, f)\n",
    "\n",
    "causal_df_2[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load reshaped causal_df, unify it and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'rb') as f:\n",
    "    causal_df_2 = pickle.load(f)\n",
    "\n",
    "# create a dataframe with all the causal_df\n",
    "dfs = []\n",
    "for process_id, process_data in causal_df_2.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        dfs.append(graph_data)\n",
    "\n",
    "causal_df_unif_2 = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "# sort in ascending order by process_id, graph_id, edge_source and edge_dest\n",
    "causal_df_unif_2.sort_values(by=['process_id', 'graph_id', 'edge_source', 'edge_dest'], inplace=True)\n",
    "\n",
    "# unique of causal_df_unif for couples of edge_source and edge_dest\n",
    "causal_df_unif_2 = causal_df_unif_2.drop_duplicates(subset=['edge_source', 'edge_dest'])\n",
    "\n",
    "# drop rows with y_pred_proba < 0.7\n",
    "causal_df_unif_2 = causal_df_unif_2[causal_df_unif_2['y_pred_proba'] >= 0.7]\n",
    "\n",
    "# keep only the top 5 rows with highest y_pred_proba\n",
    "causal_df_unif_2 = causal_df_unif_2.nlargest(5, 'y_pred_proba')\n",
    "\n",
    "# index reset\n",
    "causal_df_unif_2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save the causal_df as a pkl file alone\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'wb') as f:\n",
    "    pickle.dump(causal_df_unif_2, f)\n",
    "\n",
    "causal_df_unif_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of top variables to keep\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load unified dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df_unified\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'rb') as f:\n",
    "    causal_df_unif_2 = pickle.load(f)\n",
    "\n",
    "causal_df_unif_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Descriptors Generation using causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/third_estimate/'\n",
    "# List of files to process\n",
    "to_process = []\n",
    "\n",
    "# Filtering the files to process\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    if noise_std != noise_std_filter or max_neighborhood_size != max_neighborhood_size_filter:\n",
    "        continue\n",
    "\n",
    "    to_process.append(file)\n",
    "\n",
    "# Create output folder if it does not exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Process each file and create new DAGs based on causal paths\n",
    "for file in tqdm(to_process):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    dataloader = DataLoader(n_variables=n_variables, maxlags=maxlags)\n",
    "    dataloader.from_pickle(input_folder + file)\n",
    "\n",
    "    d2c = D2C(\n",
    "        observations=dataloader.get_observations(),\n",
    "        dags=dataloader.get_dags(),\n",
    "        couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "        MB_size=MB_SIZE,\n",
    "        n_variables=n_variables,\n",
    "        maxlags=maxlags,\n",
    "        seed=SEED,\n",
    "        n_jobs=N_JOBS,\n",
    "        full=True,\n",
    "        quantiles=True,\n",
    "        normalize=True,\n",
    "        cmi='original',\n",
    "        mb_estimator='iterative',\n",
    "        top_vars=3,\n",
    "        causal_df=causal_df_unif_2\n",
    "    )\n",
    "\n",
    "    d2c.initialize()  # Initializes the D2C object\n",
    "    descriptors_df = d2c.get_descriptors_df()  # Computes the descriptors\n",
    "\n",
    "    # Save the descriptors along with new DAGs if needed\n",
    "    descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "    descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "    descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "    descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "    descriptors_df.to_pickle(output_folder + f'P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "\n",
    "to_dos = []\n",
    "\n",
    "# This loop gets a list of all the files to be processed\n",
    "for testing_file in sorted(os.listdir(data_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "          \n",
    "        if noise_std != 0.01: # if the noise is different we skip the file\n",
    "            continue\n",
    "\n",
    "        if max_neighborhood_size != 2: # if the max_neighborhood_size is different we skip the file\n",
    "            continue\n",
    "\n",
    "        to_dos.append(testing_file) # we add the file to the list (to_dos) to be processed\n",
    "\n",
    "# sort to_dos by number of variables\n",
    "to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "# to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "# to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "\n",
    "# we create a dictionary with the lists of files to be processed\n",
    "todos = {'5': to_dos_5_variables} # , '10': to_dos_10_variables, '25': to_dos_25_variables\n",
    "\n",
    "# we create a dictionary to store the results\n",
    "dfs = []\n",
    "descriptors_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/third_estimate/'\n",
    "\n",
    "# Re-save pickle files with protocol 4\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        file_path = os.path.join(descriptors_root, testing_file)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Re-save with protocol 4\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f, protocol=4)\n",
    "\n",
    "# This loop gets the descriptors for the files to be processed\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(os.path.join(descriptors_root, testing_file))\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            dfs.append(df)\n",
    "\n",
    "# we concatenate the descriptors\n",
    "descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop does the following:\n",
    "# 1. Creates some dictionaries to store the results\n",
    "# 2. Loads the training data\n",
    "# 3. Trains the model\n",
    "# 4. Evaluates the model\n",
    "# 5. Stores the results in the dictionaries\n",
    "# 6. Saves the dictionaries in a pickle file\n",
    "\n",
    "for n_vars, todo in todos.items():\n",
    "    td2c_rocs_process = {}\n",
    "    td2c_precision_process = {}\n",
    "    td2c_recall_process = {}\n",
    "    td2c_f1_process = {}\n",
    "    causal_df = {}\n",
    "    for testing_file in tqdm(todo):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "        # split training and testing data\n",
    "        training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "        model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "        # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "        model = model.fit(X_train, y_train)\n",
    "\n",
    "        rocs = {}\n",
    "        precisions = {}\n",
    "        recalls = {}\n",
    "        f1s = {}\n",
    "        causal_dfs = {}\n",
    "        for graph_id in range(40):\n",
    "            #load testing descriptors\n",
    "            test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "            test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "            X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_test = test_df['is_causal']\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            roc = roc_auc_score(y_test, y_pred_proba)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            rocs[graph_id] = roc\n",
    "            precisions[graph_id] = precision\n",
    "            recalls[graph_id] = recall\n",
    "            f1s[graph_id] = f1\n",
    "            \n",
    "            # add to causal_df test_df, y_pred_proba and y_pred\n",
    "            causal_dfs[graph_id] = test_df\n",
    "            causal_dfs[graph_id]['y_pred_proba'] = y_pred_proba\n",
    "            causal_dfs[graph_id]['y_pred'] = y_pred\n",
    "\n",
    "        causal_df[gen_process_number] = causal_dfs\n",
    "        td2c_rocs_process[gen_process_number] = rocs\n",
    "        td2c_precision_process[gen_process_number] = precisions\n",
    "        td2c_recall_process[gen_process_number] = recalls\n",
    "        td2c_f1_process[gen_process_number] = f1s\n",
    "\n",
    "# pickle everything\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/third_estimation/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'wb') as f:\n",
    "    everything = (td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process, causal_df)\n",
    "    pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import results\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/third_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'rb') as f:\n",
    "    TD2C_2_rocs_process, TD2C_2_precision_process, TD2C_2_recall_process, TD2C_2_f1_process, causal_df_3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC scores PLOTS\n",
    "df1 = pd.DataFrame(TD2C_0_rocs_process)\n",
    "df2 = pd.DataFrame(TD2C_1_rocs_process)\n",
    "df3 = pd.DataFrame(TD2C_2_rocs_process)\n",
    "\n",
    "# Combine data for boxplot\n",
    "combined_data = []\n",
    "\n",
    "for col in df1.columns:\n",
    "    combined_data.append(df1[col])\n",
    "    combined_data.append(df2[col])\n",
    "    combined_data.append(df3[col])\n",
    "\n",
    "# Create labels for x-axis\n",
    "labels = []\n",
    "for col in df1.columns:\n",
    "    labels.append(f'{col} TD2C_Fist_Estimation')\n",
    "    labels.append(f'{col} TD2C_Second_Estimation')\n",
    "    labels.append(f'{col} TD2C_Third_Estimation')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "box = plt.boxplot(combined_data, patch_artist=True)\n",
    "\n",
    "# Color customization\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "for patch, i in zip(box['boxes'], range(len(box['boxes']))):\n",
    "    patch.set_facecolor(colors[i % 3])\n",
    "\n",
    "plt.xticks(range(1, len(labels) + 1), labels, rotation=-90)\n",
    "plt.title(f'Boxplot of ROC-AUC scores for TD2C and Iterative TD2C (with {k} top var) with Regression to estimate MI (5 variables processes)')\n",
    "plt.xlabel('Processes')\n",
    "plt.ylabel('ROC-AUC score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for df1 and df2 with the ROC-AUC scores for each process over all graphs\n",
    "df1.mean().mean(), df2.mean().mean() , df3.mean().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITERATIVE TD2C\n",
    "\n",
    "def iterative_td2c(method = 'ts', k = 1, it = 3, top_vars = 3, treshold = True, treshold_value = 0.9, size_causal_df = 5,\n",
    "                   data_folder = 'home/data/', descr_folder = 'home/descr/', results_folder = 'home/results/'):\n",
    "    \"\"\"\n",
    "    # This function requires data already generated and stored in the data folder\n",
    "\n",
    "    # Methods:\n",
    "        # 'ts' = for classic TD2C\n",
    "        # 'original' = for original D2C\n",
    "        # 'ts_rank' = for TD2C with ranking\n",
    "        # 'ts_rank_2' = for TD2C with ranking 2\n",
    "        # 'ts_rank_3' = for TD2C with ranking 3\n",
    "        # 'ts_rank_4' = for TD2C with ranking 4\n",
    "        # 'ts_rank_5' = for TD2C with ranking 5\n",
    "        # 'ts_rank_6' = for TD2C with ranking 6\n",
    "        # 'ts_rank_7' = for TD2C with ranking 7\n",
    "        # 'ts_past' = for TD2C with past and future nodes\n",
    "        # 'ts_rank_no_count' = for TD2C with ranking with no contemporaneous nodes\n",
    "\n",
    "    Parameters:\n",
    "    # k is the number of top variables to keep at each iteration for each DAG (keep = 1 if treshold = False)\n",
    "    # it is the limit for the number of iterations to perform\n",
    "    # top_vars is the number of top variables to keep in case of TD2C Ranking\n",
    "    # treshold is a boolean to determine if we want to keep in the causal df the variables with a pred.proba higher than treshold_value\n",
    "    # treshold_value is the value to keep the variables in the causal df\n",
    "    # size_causal_df is the number of variables to keep in the causal_df in case of treshold = False\n",
    "    # data_folder is the folder where the data is stored\n",
    "    # descr_folder is the folder where the descriptors are stored\n",
    "    # results_folder is the folder where the results are stored\n",
    "\n",
    "    Stopping Criteria:\n",
    "     1. if average ROC-AUC score does not improve or is the same as the previous iteration for 3 consecutive iterations\n",
    "     2. if the first iteration has an average ROC-AUC score lower than 0.5\n",
    "     3. if the average ROC-AUC score is more than 0.2 points lower than the first iteration\n",
    "     4. if causal df is the same as the previous one for 3 consecutive iterations\n",
    "\n",
    "    Output:\n",
    "     1. Plot of average ROC-AUC scores (saved in results folder as pdf file)\n",
    "     2. Average ROC-AUC scores for each iteration (saved in results folder as csv file)\n",
    "    \"\"\"\n",
    "    \n",
    "    iter_df = pd.DataFrame\n",
    "    stop_1 = 0\n",
    "    stop_2 = 0\n",
    "    roc_scores = []\n",
    "\n",
    "    print()\n",
    "    print(f'Iterative TD2C - Method: {method} - Max iterations: {it} - Variables to keep per DAG: {k} - Top Variables: {top_vars} - Treshold: {treshold} - Size of Causal DF: {size_causal_df}')\n",
    "    print()\n",
    "    if COUPLES_TO_CONSIDER_PER_DAG == -1 and size_causal_df == 5 and treshold == False:\n",
    "        print('Using all couples for each DAG')\n",
    "        print(f'This iteration will take approximately {8.5*it} minutes')\n",
    "        print()\n",
    "    elif COUPLES_TO_CONSIDER_PER_DAG == -1 and treshold == True:\n",
    "        print(f'Using all couples for each DAG and a pred.proba higher than {treshold_value}')\n",
    "        print(f'This iteration will take approximately {10.5*it} minutes')\n",
    "        print()\n",
    "    elif COUPLES_TO_CONSIDER_PER_DAG != -1 and size_causal_df == 5:\n",
    "        print(f'Using the top {COUPLES_TO_CONSIDER_PER_DAG} couples for each DAG')\n",
    "        print(f'This iteration will take approximately {4*it} minutes')\n",
    "        print()\n",
    "    elif COUPLES_TO_CONSIDER_PER_DAG != -1 and size_causal_df == 1:\n",
    "        print(f'Using the top {COUPLES_TO_CONSIDER_PER_DAG} couples for each DAG')\n",
    "        print(f'This iteration will take approximately {3.5*it} minutes')\n",
    "        print()\n",
    "\n",
    "    print(\"Do you want to continue with the rest of the function? (y/n): \")\n",
    "\n",
    "    response = input(\"Do you want to continue with the rest of the function? (y/n): \").strip().lower()\n",
    "\n",
    "    if response in ['yes', 'y', 'Yes', 'Y']:\n",
    "        print()\n",
    "        print(\"Ok! Let's start the iteration.\")\n",
    "        print()\n",
    "\n",
    "        # Estimation For Cycle\n",
    "        for i in range(1,it+1):\n",
    "\n",
    "            print()\n",
    "            print(f'----------------------------  Estimation {i}  ----------------------------')\n",
    "            print()\n",
    "\n",
    "            warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "            input_folder = data_folder\n",
    "            \n",
    "            output_folder = descr_folder + f'estimate_{i}/'\n",
    "            \n",
    "            # Descriptors Generation #############################################################################\n",
    "            # List of files to process\n",
    "            to_process = []\n",
    "\n",
    "            # Filtering the files to process\n",
    "            for file in sorted(os.listdir(input_folder)):\n",
    "                gen_process_number = int(file.split('_')[0][1:])\n",
    "                n_variables = int(file.split('_')[1][1:])\n",
    "                max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "                noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "                if noise_std != noise_std_filter or max_neighborhood_size != max_neighborhood_size_filter:\n",
    "                    continue\n",
    "\n",
    "                to_process.append(file)\n",
    "\n",
    "            # Create output folder if it does not exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            print('Making Descriptors...')\n",
    "\n",
    "            # Process each file and create new DAGs based on causal paths\n",
    "            for file in tqdm(to_process):\n",
    "                gen_process_number = int(file.split('_')[0][1:])\n",
    "                n_variables = int(file.split('_')[1][1:])\n",
    "                max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "                noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "                dataloader = DataLoader(n_variables=n_variables, maxlags=maxlags)\n",
    "                dataloader.from_pickle(input_folder + file)\n",
    "\n",
    "                if i  == 1:\n",
    "                    d2c = D2C(\n",
    "                        observations=dataloader.get_observations(),\n",
    "                        dags=dataloader.get_dags(),\n",
    "                        couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "                        MB_size=MB_SIZE,\n",
    "                        n_variables=n_variables,\n",
    "                        maxlags=maxlags,\n",
    "                        seed=SEED,\n",
    "                        n_jobs=N_JOBS,\n",
    "                        full=True,\n",
    "                        quantiles=True,\n",
    "                        normalize=True,\n",
    "                        cmi='original',\n",
    "                        mb_estimator=method,\n",
    "                        top_vars=top_vars\n",
    "                    )\n",
    "                \n",
    "                else:\n",
    "                    d2c = D2C(\n",
    "                        observations=dataloader.get_observations(),\n",
    "                        dags=dataloader.get_dags(),\n",
    "                        couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "                        MB_size=MB_SIZE,\n",
    "                        n_variables=n_variables,\n",
    "                        maxlags=maxlags,\n",
    "                        seed=SEED,\n",
    "                        n_jobs=N_JOBS,\n",
    "                        full=True,\n",
    "                        quantiles=True,\n",
    "                        normalize=True,\n",
    "                        cmi='original',\n",
    "                        mb_estimator= 'iterative',\n",
    "                        top_vars=top_vars,\n",
    "                        causal_df=iter_df\n",
    "                    )\n",
    "\n",
    "                d2c.initialize()  # Initializes the D2C object\n",
    "                descriptors_df = d2c.get_descriptors_df()  # Computes the descriptors\n",
    "\n",
    "                # Save the descriptors along with new DAGs if needed\n",
    "                descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "                descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "                descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "                descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "                descriptors_df.to_pickle(output_folder + f'Estimate_{i}_P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')\n",
    "\n",
    "            # Set Classifier #################################################################################\n",
    "            data_root = data_folder\n",
    "\n",
    "            to_dos = []\n",
    "\n",
    "            # This loop gets a list of all the files to be processed\n",
    "            for testing_file in sorted(os.listdir(data_root)):\n",
    "                if testing_file.endswith('.pkl'):\n",
    "                    gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "                    n_variables = int(testing_file.split('_')[1][1:])\n",
    "                    max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "                    noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "                    \n",
    "                    if noise_std != 0.01: # if the noise is different we skip the file\n",
    "                        continue\n",
    "\n",
    "                    if max_neighborhood_size != 2: # if the max_neighborhood_size is different we skip the file\n",
    "                        continue\n",
    "\n",
    "                    to_dos.append(testing_file) # we add the file to the list (to_dos) to be processed\n",
    "\n",
    "            # sort to_dos by number of variables\n",
    "            to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "            # to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "            # to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "\n",
    "            # we create a dictionary with the lists of files to be processed\n",
    "            todos = {'5': to_dos_5_variables} # , '10': to_dos_10_variables, '25': to_dos_25_variables\n",
    "\n",
    "            # we create a dictionary to store the results\n",
    "            dfs = []\n",
    "            descriptors_root = descr_folder + f'estimate_{i}/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(descriptors_root):\n",
    "                os.makedirs(descriptors_root)\n",
    "\n",
    "            # Re-save pickle files with protocol 4\n",
    "            for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "                if testing_file.endswith('.pkl'):\n",
    "                    file_path = os.path.join(descriptors_root, testing_file)\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        data = pickle.load(f)\n",
    "                    \n",
    "                    # Re-save with protocol 4\n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        pickle.dump(data, f, protocol=4)\n",
    "\n",
    "            # This loop gets the descriptors for the files to be processed\n",
    "            for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "                if testing_file.endswith('.pkl'):\n",
    "                    df = pd.read_pickle(os.path.join(descriptors_root, testing_file))\n",
    "                    if isinstance(df, pd.DataFrame):\n",
    "                        dfs.append(df)\n",
    "\n",
    "            # we concatenate the descriptors\n",
    "            descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "            # Classifier & Evaluation Metrics #################################################################\n",
    "\n",
    "            print('Classification & Evaluation Metrics')\n",
    "\n",
    "            for n_vars, todo in todos.items():\n",
    "\n",
    "                m1 = f'Estimate_{i}_rocs_process'\n",
    "                # m2 = f'Estimate_{i}_precision_process'\n",
    "                # m3 = f'Estimate_{i}_recall_process'\n",
    "                # m4 = f'Estimate_{i}_f1_process'\n",
    "\n",
    "                globals()[m1] = {}\n",
    "                # globals()[m2] = {}\n",
    "                # globals()[m3] = {}\n",
    "                # globals()[m4] = {}\n",
    "                causal_df_1 = {}\n",
    "\n",
    "                for testing_file in tqdm(todo):\n",
    "                    gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "                    n_variables = int(testing_file.split('_')[1][1:])\n",
    "                    max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "                    noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "                    # split training and testing data\n",
    "                    training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "                    X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "                    y_train = training_data['is_causal']\n",
    "\n",
    "                    testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "                    model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "                    # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "                    model = model.fit(X_train, y_train)\n",
    "\n",
    "                    rocs = {}\n",
    "                    # precisions = {}\n",
    "                    # recalls = {}\n",
    "                    # f1s = {}\n",
    "                    causal_dfs = {}\n",
    "                    for graph_id in range(40):\n",
    "                        #load testing descriptors\n",
    "                        test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "                        test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "                        X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "                        y_test = test_df['is_causal']\n",
    "\n",
    "                        y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "                        y_pred = model.predict(X_test)\n",
    "\n",
    "                        roc = roc_auc_score(y_test, y_pred_proba)\n",
    "                        # precision = precision_score(y_test, y_pred)\n",
    "                        # recall = recall_score(y_test, y_pred)\n",
    "                        # f1 = f1_score(y_test, y_pred)\n",
    "                        \n",
    "                        rocs[graph_id] = roc\n",
    "                        # precisions[graph_id] = precision\n",
    "                        # recalls[graph_id] = recall\n",
    "                        # f1s[graph_id] = f1\n",
    "                        \n",
    "                        # add to causal_df test_df, y_pred_proba and y_pred\n",
    "                        causal_dfs[graph_id] = test_df\n",
    "                        causal_dfs[graph_id]['y_pred_proba'] = y_pred_proba\n",
    "                        causal_dfs[graph_id]['y_pred'] = y_pred\n",
    "\n",
    "                    causal_df_1[gen_process_number] = causal_dfs\n",
    "                    globals()[m1][gen_process_number] = rocs\n",
    "                    # globals()[m2][gen_process_number] = precisions\n",
    "                    # globals()[m3][gen_process_number] = recalls\n",
    "                    # globals()[m4][gen_process_number] = f1s\n",
    "\n",
    "            # pickle everything\n",
    "            output_folder = results_folder + f'journals/estimate_{i}/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            with open(os.path.join(output_folder, f'journal_results_td2c_R_N5.pkl'), 'wb') as f:\n",
    "                everything = (globals()[m1], causal_df_1) #, globals()[m2], globals()[m3], globals()[m4]\n",
    "                pickle.dump(everything, f)\n",
    "\n",
    "            # Load results #####################################################################################\n",
    "            input_folder = results_folder + f'journals/estimate_{i}/'\n",
    "\n",
    "            with open(os.path.join(input_folder, f'journal_results_td2c_R_N5.pkl'), 'rb') as f:\n",
    "                TD2C_1_rocs_process, causal_df_1 = pickle.load(f) # , TD2C_1_precision_process, TD2C_1_recall_process, TD2C_1_f1_process\n",
    "\n",
    "\n",
    "            # STOPPING CRITERIA 2: Using ROC-AUC score\n",
    "            roc = pd.DataFrame(TD2C_1_rocs_process).mean().mean()\n",
    "\n",
    "            if i == 1:\n",
    "                if roc > 0.5:\n",
    "                    roc_first = roc\n",
    "                else:\n",
    "                    print('ROC-AUC is too low, let\\'s stop here.')\n",
    "                    break\n",
    "            elif i > 1:\n",
    "                if roc <= roc_0:\n",
    "                    stop_2 = stop_2 + 1\n",
    "                    if stop_2 == 3:\n",
    "                        print()\n",
    "                        print('Estimation are not improving, let\\'s stop here.')\n",
    "                        print()\n",
    "                        break\n",
    "                else:\n",
    "                    stop_2 = 0\n",
    "                \n",
    "                if roc <= roc_first-0.2:\n",
    "                    print()\n",
    "                    print('Estimation are not improving, let\\'s stop here.')\n",
    "                    print()\n",
    "                    break\n",
    "            \n",
    "            print()\n",
    "            print(f'ROC-AUC score: {roc}')\n",
    "            print()\n",
    "            roc_scores.append(roc)\n",
    "            roc_0 = roc\n",
    "\n",
    "            # Reshape causal_df #################################################################################\n",
    "            # keep only rows for top k y_pred_proba\n",
    "            for process_id, process_data in causal_df_1.items():\n",
    "                for graph_id, graph_data in process_data.items():\n",
    "                    causal_df_1[process_id][graph_id] = graph_data.nlargest(k, 'y_pred_proba')\n",
    "\n",
    "            # for each causal_df keep only process_id, graph_id, edge_source, edge_dest and y_pred_proba\n",
    "            for process_id, process_data in causal_df_1.items():\n",
    "                for graph_id, graph_data in process_data.items():\n",
    "                    causal_df_1[process_id][graph_id] = graph_data[['process_id', 'graph_id', 'edge_source', 'edge_dest', 'y_pred_proba']]\n",
    "                    causal_df_1[process_id][graph_id].reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # save the causal_df as a pkl file alone\n",
    "            output_folder = results_folder + f'metrics/estimate_{i}/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'wb') as f:\n",
    "                pickle.dump(causal_df_1, f)\n",
    "\n",
    "            # Unify causal_df #################################################################################\n",
    "            input_folder = results_folder + f'metrics/estimate_{i}/'\n",
    "\n",
    "            with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'rb') as f:\n",
    "                causal_df_1 = pickle.load(f)\n",
    "\n",
    "            # create a dataframe with all the causal_df\n",
    "            dfs = []\n",
    "            for process_id, process_data in causal_df_1.items():\n",
    "                for graph_id, graph_data in process_data.items():\n",
    "                    dfs.append(graph_data)\n",
    "\n",
    "            causal_df_unif_1 = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "            # sort in ascending order by process_id, graph_id, edge_source and edge_dest\n",
    "            causal_df_unif_1.sort_values(by=['process_id', 'graph_id', 'edge_source', 'edge_dest'], inplace=True)\n",
    "\n",
    "            # # keep couples edge-dest,edge_source present for more than one dag (TO TRY)\n",
    "            # causal_df_unif_1 = causal_df_unif_1[causal_df_unif_1.duplicated(subset=['edge_source', 'edge_dest'], keep=False)]\n",
    "\n",
    "            # unique of causal_df_unif for couples of edge_source and edge_dest\n",
    "            causal_df_unif_1 = causal_df_unif_1.drop_duplicates(subset=['edge_source', 'edge_dest'])\n",
    "\n",
    "            # KEEP VARIABLES IN CAUSAL_DF FOR A TRESHOLD OR TOP N \n",
    "            if treshold == True:\n",
    "                # drop rows with y_pred_proba < 0.7 (not necessary given the next step)\n",
    "                causal_df_unif_1 = causal_df_unif_1[causal_df_unif_1['y_pred_proba'] >= treshold_value]\n",
    "                if causal_df_unif_1.shape[0] > 1:\n",
    "                    causal_df_unif_1 = causal_df_unif_1.nlargest(10, 'y_pred_proba')\n",
    "\n",
    "            else:\n",
    "                # if n row > 5, keep only the top 5 rows with highest y_pred_proba\n",
    "                if causal_df_unif_1.shape[0] > 1:\n",
    "                    causal_df_unif_1 = causal_df_unif_1.nlargest(size_causal_df, 'y_pred_proba')\n",
    "\n",
    "            # index reset\n",
    "            causal_df_unif_1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # STOPPING CRITERIA 1: if causal df is the same as the previous one for 2 consecutive iterations\n",
    "            if causal_df_unif_1.equals(iter_df):\n",
    "                stop_1 = stop_1 + 1\n",
    "                if stop_1 == 2:\n",
    "                    print()\n",
    "                    print(f'No new edges to add in the next iteration')\n",
    "                    print()\n",
    "                    break\n",
    "            else:\n",
    "                stop_1 = 0\n",
    "\n",
    "            # save the causal_df as a pkl file alone\n",
    "            output_folder = results_folder + f'metrics/estimate_{i}/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'wb') as f:\n",
    "                pickle.dump(causal_df_unif_1, f)\n",
    "\n",
    "            iter_df = causal_df_unif_1\n",
    "\n",
    "            print()\n",
    "            print(f'Most relevant Edges that will be added in the next iteration:')\n",
    "            print(causal_df_unif_1)\n",
    "            print()\n",
    "\n",
    "        # PLOT RESULTS #################################################################################\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(range(1, it+1), roc_scores, marker='o')\n",
    "        if treshold == False:\n",
    "            size = size_causal_df\n",
    "            plt.title(f'ROC-AUC scores for Iterative {method} ({it} iterations and {size} top vars) with Regression MI (5 vars processes) ({COUPLES_TO_CONSIDER_PER_DAG} couples per dag)')\n",
    "        else:\n",
    "            size = k\n",
    "            plt.title(f'ROC-AUC scores for Iterative {method} ({it} iterations and {size} top vars) with Regression MI (5 vars processes) ({COUPLES_TO_CONSIDER_PER_DAG} couples per dag)')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('ROC-AUC score')\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # save the plot in folder\n",
    "        output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/try_complete_function/plots/'\n",
    "        plt.savefig(output_folder + f'ROC_AUC_scores_TD2C_{method}_{it}_iterations_{size}_top_vars_{COUPLES_TO_CONSIDER_PER_DAG}_couples_per_dag.pdf')\n",
    "\n",
    "        roc_scores_df = pd.DataFrame(roc_scores, columns=['roc_score'])\n",
    "        roc_scores_df['iteration'] = range(1, it+1)\n",
    "\n",
    "        # save the df in a csv file\n",
    "        output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/try_complete_function/metrics/'\n",
    "        roc_scores_df.to_csv(output_folder + f'roc_scores_TD2C_{method}_{it}_iterations_{size}_top_vars_{COUPLES_TO_CONSIDER_PER_DAG}_couples_per_dag.csv', index=False)\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        print()\n",
    "        print(\"Wise choice! Change the parameters and try again.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_td2c(method = 'ts', k = 5, it = 6, top_vars=3, treshold = True, treshold_value = 0.8, size_causal_df = 5,\n",
    "               data_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/', \n",
    "               descr_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/try_complete_function/', \n",
    "               results_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/try_complete_function/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative TD2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterative TD2C - Method: ts - Max iterations: 12 - Variables to keep per DAG: 1 - Top Variables: 3 - Treshold: False - Size of Causal DF: 3\n",
      "\n",
      "Using the top 5 couples for each DAG\n",
      "This iteration will take approximately 42.0 minutes\n",
      "\n",
      "Do you want to continue with the rest of the function? (y/n): \n",
      "\n",
      "Ok! Let's start the iteration.\n",
      "\n",
      "\n",
      "----------------------------  Estimation 0  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [00:20<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification & Evaluation Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [01:26<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC-AUC score: 0.8611111111111112\n",
      "\n",
      "\n",
      "Most relevant Edges that will be added in the next iteration:\n",
      "     process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "694           9        14           14          2          1.00\n",
      "378           1        18            8          3          0.99\n",
      "365           1         5            5          0          0.98\n",
      "\n",
      "\n",
      "----------------------------  Estimation 1  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [00:27<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification & Evaluation Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [01:23<00:00,  4.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC-AUC score: 0.8277777777777778\n",
      "\n",
      "\n",
      "Most relevant Edges that will be added in the next iteration:\n",
      "   process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "0           9        14           14          2          1.00\n",
      "1          15        17            8          2          1.00\n",
      "2           1        18            8          3          0.98\n",
      "3           1         5            5          0          0.97\n",
      "\n",
      "\n",
      "----------------------------  Estimation 2  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [00:27<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification & Evaluation Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [01:24<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC-AUC score: 0.8520833333333333\n",
      "\n",
      "\n",
      "Most relevant Edges that will be added in the next iteration:\n",
      "   process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "0           9        14           14          2          1.00\n",
      "1           1         5            5          0          0.99\n",
      "2           1        18            8          3          0.95\n",
      "3          10         4            6          1          0.89\n",
      "\n",
      "\n",
      "----------------------------  Estimation 3  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [00:26<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification & Evaluation Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [01:24<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC-AUC score: 0.8326388888888888\n",
      "\n",
      "\n",
      "Most relevant Edges that will be added in the next iteration:\n",
      "   process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "0           9        14           14          2          1.00\n",
      "1          15        17            8          2          1.00\n",
      "2           1        18            8          3          0.94\n",
      "3           1         5            5          0          0.93\n",
      "4          15        39            7          3          0.92\n",
      "\n",
      "\n",
      "----------------------------  Estimation 4  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [00:27<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification & Evaluation Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [01:24<00:00,  4.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC-AUC score: 0.8763888888888889\n",
      "\n",
      "\n",
      "Most relevant Edges that will be added in the next iteration:\n",
      "   process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "0           1         5            5          0          0.97\n",
      "1           1        19           19          1          0.88\n",
      "2           2        34           21          1          0.86\n",
      "3           1        25            6          0          0.84\n",
      "4           8        11            8          2          0.84\n",
      "\n",
      "\n",
      "----------------------------  Estimation 5  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [00:30<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification & Evaluation Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [01:26<00:00,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC-AUC score: 0.8618055555555556\n",
      "\n",
      "\n",
      "Most relevant Edges that will be added in the next iteration:\n",
      "   process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "0           9        14           14          2          1.00\n",
      "1          10         4            6          1          0.90\n",
      "2          14        27            5          4          0.82\n",
      "3          20        19           22          3          0.77\n",
      "4           6        30           18          4          0.76\n",
      "5           1         5            5          0          0.74\n",
      "\n",
      "\n",
      "----------------------------  Estimation 6  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [00:30<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification & Evaluation Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [01:25<00:00,  4.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC-AUC score: 0.8045138888888889\n",
      "\n",
      "\n",
      "Most relevant Edges that will be added in the next iteration:\n",
      "   process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "0           9        14           14          2          1.00\n",
      "1           9        18            8          3          1.00\n",
      "2          15        17            8          2          1.00\n",
      "3          10         4            6          1          0.94\n",
      "4           1         5            5          0          0.89\n",
      "\n",
      "\n",
      "----------------------------  Estimation 7  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [00:26<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification & Evaluation Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [01:24<00:00,  4.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC-AUC score: 0.8614583333333333\n",
      "\n",
      "\n",
      "Most relevant Edges that will be added in the next iteration:\n",
      "   process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "0           9        14           14          2          1.00\n",
      "1           1         5            5          0          0.98\n",
      "2           1        18            8          3          0.94\n",
      "3          10         4            6          1          0.89\n",
      "4           2        34           21          1          0.88\n",
      "\n",
      "\n",
      "----------------------------  Estimation 8  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [00:27<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification & Evaluation Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [01:26<00:00,  4.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC-AUC score: 0.8545138888888889\n",
      "\n",
      "\n",
      "Most relevant Edges that will be added in the next iteration:\n",
      "   process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "0           9        14           14          2          1.00\n",
      "1          15        17            8          2          1.00\n",
      "2          10         4            6          1          0.88\n",
      "3           8        10           17          1          0.84\n",
      "\n",
      "\n",
      "----------------------------  Estimation 9  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [00:27<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification & Evaluation Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [01:24<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC-AUC score: 0.9072916666666667\n",
      "\n",
      "\n",
      "Most relevant Edges that will be added in the next iteration:\n",
      "   process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "0           1        14           14          2          0.91\n",
      "1           1         0           15          0          0.87\n",
      "2           1         9           22          1          0.87\n",
      "3           1        18            8          3          0.87\n",
      "\n",
      "\n",
      "----------------------------  Estimation 10  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [00:26<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification & Evaluation Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [01:24<00:00,  4.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC-AUC score: 0.8520833333333333\n",
      "\n",
      "\n",
      "Most relevant Edges that will be added in the next iteration:\n",
      "   process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "0           9        14           14          2          1.00\n",
      "1           1        11           12          0          0.91\n",
      "2          10         4            6          1          0.91\n",
      "\n",
      "\n",
      "----------------------------  Estimation 11  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [00:29<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification & Evaluation Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [01:24<00:00,  4.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC-AUC score: 0.8315972222222222\n",
      "\n",
      "\n",
      "Most relevant Edges that will be added in the next iteration:\n",
      "   process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "0           9        14           14          2          1.00\n",
      "1          15        17            8          2          1.00\n",
      "2           1        18            8          3          0.98\n",
      "3           1        19           19          1          0.83\n",
      "\n",
      "\n",
      "----------------------------  Estimation 12  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [00:27<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification & Evaluation Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 18/18 [01:24<00:00,  4.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC-AUC score: 0.8944444444444445\n",
      "\n",
      "\n",
      "Most relevant Edges that will be added in the next iteration:\n",
      "   process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "0          10         4            6          1          0.95\n",
      "1           1        14           14          2          0.87\n",
      "2           1         5            5          0          0.84\n",
      "3           8        11            8          2          0.83\n",
      "\n",
      "\n",
      "Resultant ROC-AUC scores Plot:\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (14,) and (13,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m response \u001b[38;5;241m=\u001b[39m Iter\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     25\u001b[0m roc_scores \u001b[38;5;241m=\u001b[39m Iter\u001b[38;5;241m.\u001b[39miterative_td2c(response)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mIter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroc_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m Iter\u001b[38;5;241m.\u001b[39mdf_scores(roc_scores)\n",
      "File \u001b[0;32m~/td2c/src/d2c/benchmark/iterative_td2c.py:682\u001b[0m, in \u001b[0;36mplot_results\u001b[0;34m(self, roc_scores)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, roc_scores):\n\u001b[0;32m--> 682\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtreshold \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mit \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_causal_df \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCOUPLES_TO_CONSIDER_PER_DAG \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m roc_scores \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    683\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease run iterative_td2c() function first\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    684\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/td2c/myenv3.8/lib/python3.8/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/td2c/myenv3.8/lib/python3.8/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/td2c/myenv3.8/lib/python3.8/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/td2c/myenv3.8/lib/python3.8/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (14,) and (13,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Iter = IterativeTD2C(\n",
    "    method = 'ts', \n",
    "    k = 1, \n",
    "    it = 12, \n",
    "    top_vars=3, \n",
    "    treshold = False,\n",
    "    adaptive = 'Balancing',\n",
    "    treshold_value = 0.9, \n",
    "    size_causal_df = 3, \n",
    "    data_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/', \n",
    "    descr_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/try_complete_function/', \n",
    "    results_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/try_complete_function/',\n",
    "    COUPLES_TO_CONSIDER_PER_DAG = 5,\n",
    "    MB_SIZE = 2,\n",
    "    SEED = 42,\n",
    "    maxlags = 5,\n",
    "    max_neighborhood_size_filter = 2,\n",
    "\n",
    "    N_JOBS = 40,\n",
    "    noise_std_filter = 0.01\n",
    ")\n",
    "\n",
    "response = Iter.start()\n",
    "\n",
    "roc_scores = Iter.iterative_td2c(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultant ROC-AUC scores Plot:\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (14,) and (13,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# roc_scores_df as a list\u001b[39;00m\n\u001b[1;32m     31\u001b[0m roc_scores \u001b[38;5;241m=\u001b[39m roc_scores_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 34\u001b[0m \u001b[43mIter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroc_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m Iter\u001b[38;5;241m.\u001b[39mdf_scores(roc_scores)\n",
      "File \u001b[0;32m~/td2c/src/d2c/benchmark/iterative_td2c.py:691\u001b[0m, in \u001b[0;36mIterativeTD2C.plot_results\u001b[0;34m(self, roc_scores)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m    690\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m--> 691\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroc_scores\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroc_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinestyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mROC-AUC score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;66;03m# Calculate cumulative average\u001b[39;00m\n\u001b[1;32m    694\u001b[0m cumulative_avg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcumsum(roc_scores) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(roc_scores) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/td2c/myenv3.8/lib/python3.8/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/td2c/myenv3.8/lib/python3.8/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/td2c/myenv3.8/lib/python3.8/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/td2c/myenv3.8/lib/python3.8/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (14,) and (13,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/try_complete_function/metrics/'\n",
    "\n",
    "\n",
    "Iter = IterativeTD2C(\n",
    "    method = 'ts', \n",
    "    k = 1, \n",
    "    it = 12, \n",
    "    top_vars=3, \n",
    "    treshold = False,\n",
    "    adaptive = 'Balancing',\n",
    "    treshold_value = 0.9, \n",
    "    size_causal_df = 3, \n",
    "    data_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/', \n",
    "    descr_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/try_complete_function/', \n",
    "    results_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/try_complete_function/',\n",
    "    COUPLES_TO_CONSIDER_PER_DAG = 5,\n",
    "    MB_SIZE = 2,\n",
    "    SEED = 42,\n",
    "    maxlags = 5,\n",
    "    max_neighborhood_size_filter = 2,\n",
    "\n",
    "    N_JOBS = 40,\n",
    "    noise_std_filter = 0.01\n",
    ")\n",
    "\n",
    "\n",
    "# load roc_scores_df from csv file\n",
    "roc_scores_df = pd.read_csv(output_folder + 'r.csv')\n",
    "\n",
    "# roc_scores_df as a list\n",
    "roc_scores = roc_scores_df['roc_score'].tolist()\n",
    "\n",
    "Iter.plot_results(roc_scores)\n",
    "\n",
    "Iter.df_scores(roc_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
