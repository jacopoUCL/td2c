{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative TD2C Comparison with Benchmark Methods\n",
    "\n",
    "This notebook presents an Iterative implementation of the `TD2C` method. The `TD2C` algorithm is extended in this work through an iterative process that refines its performance by leveraging dynamic feature selection and causal graph construction.\n",
    "\n",
    "The proposed Iterative `TD2C` method involves the following steps:\n",
    "\n",
    "1. **Model Training and Initial Causal Discovery**: The model is initially trained, and the TD2C algorithm is applied to identify the top K causal connections.\n",
    "2. **Graph Construction**: A causal graph is constructed based on the identified connections.\n",
    "3. **Historical Extension**: The graph is extended to include past states, reflecting the progression from past to present.\n",
    "4. **Application of Meek Rules**: The graph is refined using Meek's Rules to ensure proper orientation of edges and to derive causal implications.\n",
    "5. **Missing Pair Identification and Markov Blanket Derivation**: Missing causal pairs are identified, and the Markov Blanket (MB) is derived if possible from the current graph structure.\n",
    "6. **Descriptor Recalculation**: Descriptors are recalculated using the derived MB, providing refined inputs for subsequent iterations.\n",
    "\n",
    "Through this iterative process, the TD2C method is expected to enhance its accuracy in detecting causal structures, making it a robust tool for temporal causal discovery. The effectiveness of this approach will be evaluated against benchmark methods identified from previous analyses, specifically from the `Compare_TD2C_MB_Strategies` notebook, to assess its performance improvements in terms of ROC-AUC, Precision, Recall adn F1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import pickle \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from itertools import permutations\n",
    "\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from d2c.data_generation.builder import TSBuilder\n",
    "from d2c.descriptors_generation import D2C, DataLoader\n",
    "from d2c.benchmark import IterativeTD2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = 1 # number of jobs to run in parallel. For D2C, parallelism is implemented at the observation level: each observation from a single file is processed in parallel\n",
    "SEED = 42 # random seed for reproducibility\n",
    "MB_SIZE = 2 # size to consider when estimating the markov blanket. This is only useful if the MB is actually estimated\n",
    "COUPLES_TO_CONSIDER_PER_DAG = -1 # edges that are considered in total to compute descriptors, for each TS. This can speed up the process. If set to -1, all possible edges are considered\n",
    "maxlags = 5 # maximum lags to consider when considering variable couples\n",
    "noise_std_filter = 0.01  # Example noise standard deviation to filter\n",
    "max_neighborhood_size_filter = 2  # Example filter for neighborhood size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "Data are generated with the `TSBuilder` class and saved in a specific folder. Then, the code checks for missing datasets in the folder and keeps running untill all the possible combinations of parameters have genereted a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m\n\u001b[1;32m     36\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m [(process, n_variables, max_neighborhood_size, noise_std)\n\u001b[1;32m     37\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m process \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m20\u001b[39m] \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m     38\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m n_variables \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m5\u001b[39m] \u001b[38;5;66;03m# , 10, 25\u001b[39;00m\n\u001b[1;32m     39\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m max_neighborhood_size \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m] \u001b[38;5;66;03m# , 4, 8\u001b[39;00m\n\u001b[1;32m     40\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m noise_std \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.01\u001b[39m]] \u001b[38;5;66;03m# , 0.005, 0.001\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mN_JOBS) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 43\u001b[0m         \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_process\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Function to check for missing files\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_missing_files\u001b[39m():\n",
      "File \u001b[0;32m~/python3.8/lib/python3.8/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python3.8/lib/python3.8/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/python3.8/lib/python3.8/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python3.8/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/python3.8/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SET THE TSBUILDER WITH THE DESIRED PARAMETERS\n",
    "def run_process(params):\n",
    "    \"\"\"\n",
    "    Run a single process of the data generation.\n",
    "    \"\"\"\n",
    "    process, n_variables, max_neighborhood_size, noise_std = params\n",
    "    try:\n",
    "        tsbuilder = TSBuilder(observations_per_time_series=250, \n",
    "                              maxlags=5, \n",
    "                              n_variables=n_variables, \n",
    "                              time_series_per_process=40, \n",
    "                              processes_to_use=[process], \n",
    "                              noise_std=noise_std, \n",
    "                              max_neighborhood_size=max_neighborhood_size, \n",
    "                              seed=42, \n",
    "                              max_attempts=200,\n",
    "                              verbose=True)\n",
    "\n",
    "        tsbuilder.build()\n",
    "        tsbuilder.to_pickle(f'/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl')\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} done')\n",
    "    except ValueError as e:\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} failed: {e}')\n",
    "\n",
    "# BUILD THE DATA\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    This script generates the data for different parameters: processes, number of variables, neighborhood sizes and noise levels.\n",
    "    The data is saved in the .data folder.\n",
    "    The if __name__ == '__main__': is used to avoid multiprocessing issues in Jupyter notebooks, i.e. the script is run as a script and not\n",
    "    as a module as it would have been if the script was imported, with the __name__ being the name of the module.\n",
    "    If the script is imported, the __name__ is the name of the module, if it is run as a script, the __name__ is __main__.\n",
    "    So, to run this script in a Jupyter notebook, we write the code inside the if __name__ == '__main__': block, while, if we want to import\n",
    "    the functions from this script, we write \"from script import run_process\".\n",
    "    \"\"\"\n",
    "    parameters = [(process, n_variables, max_neighborhood_size, noise_std)\n",
    "                    for process in [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20] # \n",
    "                    for n_variables in [5] # , 10, 25\n",
    "                    for max_neighborhood_size in [2] # , 4, 8\n",
    "                    for noise_std in [0.01]] # , 0.005, 0.001\n",
    "\n",
    "    with Pool(processes=N_JOBS) as pool:\n",
    "        pool.map(run_process, parameters)\n",
    "\n",
    "\n",
    "# Function to check for missing files\n",
    "def check_missing_files():\n",
    "    missing = []\n",
    "    for process in [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20]:  # \n",
    "        for n_variables in [5]:  # , 10, 25\n",
    "            for max_neighborhood_size in [2]:  # , 4, 8\n",
    "                for noise_std in [0.01]:  # , 0.005, 0.001\n",
    "                    filename = f'/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "                    if not os.path.exists(filename):\n",
    "                        missing.append(filename)\n",
    "    return missing\n",
    "\n",
    "# Function to run the process\n",
    "def run_process(params):\n",
    "    process, n_variables, max_neighborhood_size, noise_std = params\n",
    "    try:  # we change the seed and increase the max_attempts\n",
    "        tsbuilder = TSBuilder(observations_per_time_series=250, \n",
    "                              maxlags=5, \n",
    "                              n_variables=n_variables, \n",
    "                              time_series_per_process=40, \n",
    "                              processes_to_use=[process], \n",
    "                              noise_std=noise_std, \n",
    "                              max_neighborhood_size=max_neighborhood_size, \n",
    "                              seed=24, \n",
    "                              max_attempts=400,\n",
    "                              verbose=True)\n",
    "\n",
    "        tsbuilder.build()\n",
    "        tsbuilder.to_pickle(f'/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl')\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} done')\n",
    "    except ValueError as e:\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} failed: {e}')\n",
    "\n",
    "# CHECK FOR MISSING FILES (IT CHEKS THE FILES WITH A DIFFERENT SEED AND MORE MAX_ATTEMPTS UNTILL MISSING IS EMPTY)\n",
    "if __name__ == '__main__':\n",
    "    while True:\n",
    "        missing = check_missing_files()\n",
    "        if not missing:\n",
    "            break\n",
    "\n",
    "        parameters = []\n",
    "        for missing_file in missing:\n",
    "            process = int(missing_file.split('/')[-1].split('_')[0][1:])\n",
    "            n_variables = int(missing_file.split('/')[-1].split('_')[1][1:])\n",
    "            max_neighborhood_size = int(missing_file.split('/')[-1].split('_')[2][2:])\n",
    "            noise_std = float(missing_file.split('/')[-1].split('_')[3][1:-4])\n",
    "            parameters.append((process, n_variables, max_neighborhood_size, noise_std))\n",
    "\n",
    "        with Pool(processes=N_JOBS) as pool:\n",
    "            pool.map(run_process, parameters)\n",
    "\n",
    "len(os.listdir('/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative TD2C implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First estimate\n",
    "2. top k (5) causally connected couples\n",
    "3. function that integrates nodes in these top k couples where necessary (when the MB for the destination node is computed)\n",
    "4. 2nd estimate (in theory better)\n",
    "5. keep going for each iteration\n",
    "6. plot evalutation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working phase (Not for users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set k\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/initial/'\n",
    "\n",
    "# List of files to process\n",
    "to_process = []\n",
    "\n",
    "# Filtering the files to process\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    if noise_std != noise_std_filter or max_neighborhood_size != max_neighborhood_size_filter:\n",
    "        continue\n",
    "\n",
    "    to_process.append(file)\n",
    "\n",
    "# Create output folder if it does not exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Process each file and create new DAGs based on causal paths\n",
    "for file in tqdm(to_process):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    dataloader = DataLoader(n_variables=n_variables, maxlags=maxlags)\n",
    "    dataloader.from_pickle(input_folder + file)\n",
    "\n",
    "    d2c = D2C(\n",
    "        observations=dataloader.get_observations(),\n",
    "        dags=dataloader.get_dags(),\n",
    "        couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "        MB_size=MB_SIZE,\n",
    "        n_variables=n_variables,\n",
    "        maxlags=maxlags,\n",
    "        seed=SEED,\n",
    "        n_jobs=N_JOBS,\n",
    "        full=True,\n",
    "        quantiles=True,\n",
    "        normalize=True,\n",
    "        cmi='original',\n",
    "        mb_estimator='ts',\n",
    "        top_vars=3\n",
    "    )\n",
    "\n",
    "    d2c.initialize()  # Initializes the D2C object\n",
    "    descriptors_df = d2c.get_descriptors_df()  # Computes the descriptors\n",
    "\n",
    "    # Save the descriptors along with new DAGs if needed\n",
    "    descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "    descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "    descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "    descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "    descriptors_df.to_pickle(output_folder + f'P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "\n",
    "to_dos = []\n",
    "\n",
    "# This loop gets a list of all the files to be processed\n",
    "for testing_file in sorted(os.listdir(data_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "          \n",
    "        if noise_std != 0.01: # if the noise is different we skip the file\n",
    "            continue\n",
    "\n",
    "        if max_neighborhood_size != 2: # if the max_neighborhood_size is different we skip the file\n",
    "            continue\n",
    "\n",
    "        to_dos.append(testing_file) # we add the file to the list (to_dos) to be processed\n",
    "\n",
    "# sort to_dos by number of variables\n",
    "to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "# to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "# to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "\n",
    "# we create a dictionary with the lists of files to be processed\n",
    "todos = {'5': to_dos_5_variables} # , '10': to_dos_10_variables, '25': to_dos_25_variables\n",
    "\n",
    "# we create a dictionary to store the results\n",
    "dfs = []\n",
    "descriptors_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/initial/'\n",
    "\n",
    "# Re-save pickle files with protocol 4\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        file_path = os.path.join(descriptors_root, testing_file)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Re-save with protocol 4\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f, protocol=4)\n",
    "\n",
    "# This loop gets the descriptors for the files to be processed\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(os.path.join(descriptors_root, testing_file))\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            dfs.append(df)\n",
    "\n",
    "# we concatenate the descriptors\n",
    "descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop does the following:\n",
    "# 1. Creates some dictionaries to store the results\n",
    "# 2. Loads the training data\n",
    "# 3. Trains the model\n",
    "# 4. Evaluates the model\n",
    "# 5. Stores the results in the dictionaries\n",
    "# 6. Saves the dictionaries in a pickle file\n",
    "\n",
    "for n_vars, todo in todos.items():\n",
    "    td2c_rocs_process = {}\n",
    "    td2c_precision_process = {}\n",
    "    td2c_recall_process = {}\n",
    "    td2c_f1_process = {}\n",
    "    causal_df = {}\n",
    "    for testing_file in tqdm(todo):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "        # split training and testing data\n",
    "        training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "        model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "        # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "        model = model.fit(X_train, y_train)\n",
    "\n",
    "        rocs = {}\n",
    "        precisions = {}\n",
    "        recalls = {}\n",
    "        f1s = {}\n",
    "        causal_dfs = {}\n",
    "        for graph_id in range(40):\n",
    "            #load testing descriptors\n",
    "            test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "            test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "            X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_test = test_df['is_causal']\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            roc = roc_auc_score(y_test, y_pred_proba)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            rocs[graph_id] = roc\n",
    "            precisions[graph_id] = precision\n",
    "            recalls[graph_id] = recall\n",
    "            f1s[graph_id] = f1\n",
    "            \n",
    "            # add to causal_df test_df, y_pred_proba and y_pred\n",
    "            causal_dfs[graph_id] = test_df\n",
    "            causal_dfs[graph_id]['y_pred_proba'] = y_pred_proba\n",
    "            causal_dfs[graph_id]['y_pred'] = y_pred\n",
    "\n",
    "        causal_df[gen_process_number] = causal_dfs\n",
    "        td2c_rocs_process[gen_process_number] = rocs\n",
    "        td2c_precision_process[gen_process_number] = precisions\n",
    "        td2c_recall_process[gen_process_number] = recalls\n",
    "        td2c_f1_process[gen_process_number] = f1s\n",
    "\n",
    "# pickle everything\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'journal_results_td2c_R_N5.pkl'), 'wb') as f:\n",
    "    everything = (td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process, causal_df)\n",
    "    pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import causal_df \n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "with open(os.path.join(input_folder, 'journal_results_td2c_R_N5.pkl'), 'rb') as f:\n",
    "    TD2C_0_rocs_process, TD2C_0_precision_process, TD2C_0_recall_process, TD2C_0_f1_process, causal_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshape causal_df and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only rows for top k y_pred_proba\n",
    "for process_id, process_data in causal_df.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        causal_df[process_id][graph_id] = graph_data.nlargest(k, 'y_pred_proba')\n",
    "\n",
    "# for each causal_df keep only process_id, graph_id, edge_source, edge_dest and y_pred_proba\n",
    "for process_id, process_data in causal_df.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        causal_df[process_id][graph_id] = graph_data[['process_id', 'graph_id', 'edge_source', 'edge_dest', 'y_pred_proba']]\n",
    "        causal_df[process_id][graph_id].reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save the causal_df as a pkl file alone\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'wb') as f:\n",
    "    pickle.dump(causal_df, f)\n",
    "\n",
    "causal_df[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load reshaped causal_df, unify it and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'rb') as f:\n",
    "    causal_df = pickle.load(f)\n",
    "\n",
    "# create a dataframe with all the causal_df\n",
    "dfs = []\n",
    "for process_id, process_data in causal_df.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        dfs.append(graph_data)\n",
    "\n",
    "causal_df_unif = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "# sort in ascending order by process_id, graph_id, edge_source and edge_dest\n",
    "causal_df_unif.sort_values(by=['process_id', 'graph_id', 'edge_source', 'edge_dest'], inplace=True)\n",
    "\n",
    "# possible to insert that a couple must be present for all processes or for a certain percentage of processes (2/3)\n",
    "\n",
    "# unique of causal_df_unif for couples of edge_source and edge_dest\n",
    "causal_df_unif = causal_df_unif.drop_duplicates(subset=['edge_source', 'edge_dest'])\n",
    "\n",
    "# drop rows with y_pred_proba < 0.7\n",
    "causal_df_unif = causal_df_unif[causal_df_unif['y_pred_proba'] >= 0.7]\n",
    "\n",
    "# keep only the top 5 rows with highest y_pred_proba\n",
    "causal_df_unif = causal_df_unif.nlargest(5, 'y_pred_proba')\n",
    "\n",
    "# index reset\n",
    "causal_df_unif.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save the causal_df as a pkl file alone\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'wb') as f:\n",
    "    pickle.dump(causal_df_unif, f)\n",
    "\n",
    "causal_df_unif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of top variables to keep\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load unified dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df_unified\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/initial/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'rb') as f:\n",
    "    causal_df_unif = pickle.load(f)\n",
    "\n",
    "causal_df_unif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Descriptors Generation using causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/second_estimate/'\n",
    "# List of files to process\n",
    "to_process = []\n",
    "\n",
    "# Filtering the files to process\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    if noise_std != noise_std_filter or max_neighborhood_size != max_neighborhood_size_filter:\n",
    "        continue\n",
    "\n",
    "    to_process.append(file)\n",
    "\n",
    "# Create output folder if it does not exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Process each file and create new DAGs based on causal paths\n",
    "for file in tqdm(to_process):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    dataloader = DataLoader(n_variables=n_variables, maxlags=maxlags)\n",
    "    dataloader.from_pickle(input_folder + file)\n",
    "\n",
    "    d2c = D2C(\n",
    "        observations=dataloader.get_observations(),\n",
    "        dags=dataloader.get_dags(),\n",
    "        couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "        MB_size=MB_SIZE,\n",
    "        n_variables=n_variables,\n",
    "        maxlags=maxlags,\n",
    "        seed=SEED,\n",
    "        n_jobs=N_JOBS,\n",
    "        full=True,\n",
    "        quantiles=True,\n",
    "        normalize=True,\n",
    "        cmi='original',\n",
    "        mb_estimator='iterative',\n",
    "        top_vars = 3,\n",
    "        causal_df = causal_df\n",
    "    )\n",
    "\n",
    "    d2c.initialize()  # Initializes the D2C object\n",
    "    descriptors_df = d2c.get_descriptors_df()  # Computes the descriptors\n",
    "\n",
    "    # Save the descriptors along with new DAGs if needed\n",
    "    descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "    descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "    descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "    descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "    descriptors_df.to_pickle(output_folder + f'P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "\n",
    "to_dos = []\n",
    "\n",
    "# This loop gets a list of all the files to be processed\n",
    "for testing_file in sorted(os.listdir(data_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "          \n",
    "        if noise_std != 0.01: # if the noise is different we skip the file\n",
    "            continue\n",
    "\n",
    "        if max_neighborhood_size != 2: # if the max_neighborhood_size is different we skip the file\n",
    "            continue\n",
    "\n",
    "        to_dos.append(testing_file) # we add the file to the list (to_dos) to be processed\n",
    "\n",
    "# sort to_dos by number of variables\n",
    "to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "# to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "# to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "\n",
    "# we create a dictionary with the lists of files to be processed\n",
    "todos = {'5': to_dos_5_variables} # , '10': to_dos_10_variables, '25': to_dos_25_variables\n",
    "\n",
    "# we create a dictionary to store the results\n",
    "dfs = []\n",
    "descriptors_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/second_estimate/'\n",
    "\n",
    "# Re-save pickle files with protocol 4\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        file_path = os.path.join(descriptors_root, testing_file)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Re-save with protocol 4\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f, protocol=4)\n",
    "\n",
    "# This loop gets the descriptors for the files to be processed\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(os.path.join(descriptors_root, testing_file))\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            dfs.append(df)\n",
    "\n",
    "# we concatenate the descriptors\n",
    "descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop does the following:\n",
    "# 1. Creates some dictionaries to store the results\n",
    "# 2. Loads the training data\n",
    "# 3. Trains the model\n",
    "# 4. Evaluates the model\n",
    "# 5. Stores the results in the dictionaries\n",
    "# 6. Saves the dictionaries in a pickle file\n",
    "\n",
    "for n_vars, todo in todos.items():\n",
    "    td2c_rocs_process = {}\n",
    "    td2c_precision_process = {}\n",
    "    td2c_recall_process = {}\n",
    "    td2c_f1_process = {}\n",
    "    causal_df = {}\n",
    "    for testing_file in tqdm(todo):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "        # split training and testing data\n",
    "        training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "        model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "        # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "        model = model.fit(X_train, y_train)\n",
    "\n",
    "        rocs = {}\n",
    "        precisions = {}\n",
    "        recalls = {}\n",
    "        f1s = {}\n",
    "        causal_dfs = {}\n",
    "        for graph_id in range(40):\n",
    "            #load testing descriptors\n",
    "            test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "            test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "            X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_test = test_df['is_causal']\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            roc = roc_auc_score(y_test, y_pred_proba)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            rocs[graph_id] = roc\n",
    "            precisions[graph_id] = precision\n",
    "            recalls[graph_id] = recall\n",
    "            f1s[graph_id] = f1\n",
    "            \n",
    "            # add to causal_df test_df, y_pred_proba and y_pred\n",
    "            causal_dfs[graph_id] = test_df\n",
    "            causal_dfs[graph_id]['y_pred_proba'] = y_pred_proba\n",
    "            causal_dfs[graph_id]['y_pred'] = y_pred\n",
    "\n",
    "        causal_df[gen_process_number] = causal_dfs\n",
    "        td2c_rocs_process[gen_process_number] = rocs\n",
    "        td2c_precision_process[gen_process_number] = precisions\n",
    "        td2c_recall_process[gen_process_number] = recalls\n",
    "        td2c_f1_process[gen_process_number] = f1s\n",
    "\n",
    "# pickle everything\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'wb') as f:\n",
    "    everything = (td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process, causal_df)\n",
    "    pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import results\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'rb') as f:\n",
    "    TD2C_1_rocs_process, TD2C_1_precision_process, TD2C_1_recall_process, TD2C_1_f1_process, causal_df_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshape causal_df and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'rb') as f:\n",
    "    TD2C_1_rocs_process, TD2C_1_precision_process, TD2C_1_recall_process, TD2C_1_f1_process, causal_df_2 = pickle.load(f)\n",
    "\n",
    "# keep only rows for top k y_pred_proba\n",
    "for process_id, process_data in causal_df_2.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        causal_df_2[process_id][graph_id] = graph_data.nlargest(k, 'y_pred_proba')\n",
    "\n",
    "# for each causal_df keep only process_id, graph_id, edge_source, edge_dest and y_pred_proba\n",
    "for process_id, process_data in causal_df_2.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        causal_df_2[process_id][graph_id] = graph_data[['process_id', 'graph_id', 'edge_source', 'edge_dest', 'y_pred_proba']]\n",
    "        causal_df_2[process_id][graph_id].reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save the causal_df as a pkl file alone\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'wb') as f:\n",
    "    pickle.dump(causal_df_2, f)\n",
    "\n",
    "causal_df_2[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load reshaped causal_df, unify it and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'rb') as f:\n",
    "    causal_df_2 = pickle.load(f)\n",
    "\n",
    "# create a dataframe with all the causal_df\n",
    "dfs = []\n",
    "for process_id, process_data in causal_df_2.items():\n",
    "    for graph_id, graph_data in process_data.items():\n",
    "        dfs.append(graph_data)\n",
    "\n",
    "causal_df_unif_2 = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "# sort in ascending order by process_id, graph_id, edge_source and edge_dest\n",
    "causal_df_unif_2.sort_values(by=['process_id', 'graph_id', 'edge_source', 'edge_dest'], inplace=True)\n",
    "\n",
    "# unique of causal_df_unif for couples of edge_source and edge_dest\n",
    "causal_df_unif_2 = causal_df_unif_2.drop_duplicates(subset=['edge_source', 'edge_dest'])\n",
    "\n",
    "# drop rows with y_pred_proba < 0.7\n",
    "causal_df_unif_2 = causal_df_unif_2[causal_df_unif_2['y_pred_proba'] >= 0.7]\n",
    "\n",
    "# keep only the top 5 rows with highest y_pred_proba\n",
    "causal_df_unif_2 = causal_df_unif_2.nlargest(5, 'y_pred_proba')\n",
    "\n",
    "# index reset\n",
    "causal_df_unif_2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# save the causal_df as a pkl file alone\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'wb') as f:\n",
    "    pickle.dump(causal_df_unif_2, f)\n",
    "\n",
    "causal_df_unif_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of top variables to keep\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load unified dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load causal_df_unified\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/second_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'rb') as f:\n",
    "    causal_df_unif_2 = pickle.load(f)\n",
    "\n",
    "causal_df_unif_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Descriptors Generation using causal_df\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/third_estimate/'\n",
    "# List of files to process\n",
    "to_process = []\n",
    "\n",
    "# Filtering the files to process\n",
    "for file in sorted(os.listdir(input_folder)):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    if noise_std != noise_std_filter or max_neighborhood_size != max_neighborhood_size_filter:\n",
    "        continue\n",
    "\n",
    "    to_process.append(file)\n",
    "\n",
    "# Create output folder if it does not exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Process each file and create new DAGs based on causal paths\n",
    "for file in tqdm(to_process):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    dataloader = DataLoader(n_variables=n_variables, maxlags=maxlags)\n",
    "    dataloader.from_pickle(input_folder + file)\n",
    "\n",
    "    d2c = D2C(\n",
    "        observations=dataloader.get_observations(),\n",
    "        dags=dataloader.get_dags(),\n",
    "        couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "        MB_size=MB_SIZE,\n",
    "        n_variables=n_variables,\n",
    "        maxlags=maxlags,\n",
    "        seed=SEED,\n",
    "        n_jobs=N_JOBS,\n",
    "        full=True,\n",
    "        quantiles=True,\n",
    "        normalize=True,\n",
    "        cmi='original',\n",
    "        mb_estimator='iterative',\n",
    "        top_vars=3,\n",
    "        causal_df=causal_df_unif_2\n",
    "    )\n",
    "\n",
    "    d2c.initialize()  # Initializes the D2C object\n",
    "    descriptors_df = d2c.get_descriptors_df()  # Computes the descriptors\n",
    "\n",
    "    # Save the descriptors along with new DAGs if needed\n",
    "    descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "    descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "    descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "    descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "    descriptors_df.to_pickle(output_folder + f'P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/'\n",
    "\n",
    "to_dos = []\n",
    "\n",
    "# This loop gets a list of all the files to be processed\n",
    "for testing_file in sorted(os.listdir(data_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "          \n",
    "        if noise_std != 0.01: # if the noise is different we skip the file\n",
    "            continue\n",
    "\n",
    "        if max_neighborhood_size != 2: # if the max_neighborhood_size is different we skip the file\n",
    "            continue\n",
    "\n",
    "        to_dos.append(testing_file) # we add the file to the list (to_dos) to be processed\n",
    "\n",
    "# sort to_dos by number of variables\n",
    "to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "# to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "# to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "\n",
    "# we create a dictionary with the lists of files to be processed\n",
    "todos = {'5': to_dos_5_variables} # , '10': to_dos_10_variables, '25': to_dos_25_variables\n",
    "\n",
    "# we create a dictionary to store the results\n",
    "dfs = []\n",
    "descriptors_root = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/iterative/third_estimate/'\n",
    "\n",
    "# Re-save pickle files with protocol 4\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        file_path = os.path.join(descriptors_root, testing_file)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Re-save with protocol 4\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f, protocol=4)\n",
    "\n",
    "# This loop gets the descriptors for the files to be processed\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(os.path.join(descriptors_root, testing_file))\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            dfs.append(df)\n",
    "\n",
    "# we concatenate the descriptors\n",
    "descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop does the following:\n",
    "# 1. Creates some dictionaries to store the results\n",
    "# 2. Loads the training data\n",
    "# 3. Trains the model\n",
    "# 4. Evaluates the model\n",
    "# 5. Stores the results in the dictionaries\n",
    "# 6. Saves the dictionaries in a pickle file\n",
    "\n",
    "for n_vars, todo in todos.items():\n",
    "    td2c_rocs_process = {}\n",
    "    td2c_precision_process = {}\n",
    "    td2c_recall_process = {}\n",
    "    td2c_f1_process = {}\n",
    "    causal_df = {}\n",
    "    for testing_file in tqdm(todo):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "        # split training and testing data\n",
    "        training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "        model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "        # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "        model = model.fit(X_train, y_train)\n",
    "\n",
    "        rocs = {}\n",
    "        precisions = {}\n",
    "        recalls = {}\n",
    "        f1s = {}\n",
    "        causal_dfs = {}\n",
    "        for graph_id in range(40):\n",
    "            #load testing descriptors\n",
    "            test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "            test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "            X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_test = test_df['is_causal']\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            roc = roc_auc_score(y_test, y_pred_proba)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            rocs[graph_id] = roc\n",
    "            precisions[graph_id] = precision\n",
    "            recalls[graph_id] = recall\n",
    "            f1s[graph_id] = f1\n",
    "            \n",
    "            # add to causal_df test_df, y_pred_proba and y_pred\n",
    "            causal_dfs[graph_id] = test_df\n",
    "            causal_dfs[graph_id]['y_pred_proba'] = y_pred_proba\n",
    "            causal_dfs[graph_id]['y_pred'] = y_pred\n",
    "\n",
    "        causal_df[gen_process_number] = causal_dfs\n",
    "        td2c_rocs_process[gen_process_number] = rocs\n",
    "        td2c_precision_process[gen_process_number] = precisions\n",
    "        td2c_recall_process[gen_process_number] = recalls\n",
    "        td2c_f1_process[gen_process_number] = f1s\n",
    "\n",
    "# pickle everything\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/third_estimation/'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(os.path.join(output_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'wb') as f:\n",
    "    everything = (td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process, causal_df)\n",
    "    pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import results\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/journals/iterative/third_estimation/'\n",
    "\n",
    "with open(os.path.join(input_folder, f'journal_results_td2c_R_N5_top_{k}_var.pkl'), 'rb') as f:\n",
    "    TD2C_2_rocs_process, TD2C_2_precision_process, TD2C_2_recall_process, TD2C_2_f1_process, causal_df_3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC scores PLOTS\n",
    "df1 = pd.DataFrame(TD2C_0_rocs_process)\n",
    "df2 = pd.DataFrame(TD2C_1_rocs_process)\n",
    "df3 = pd.DataFrame(TD2C_2_rocs_process)\n",
    "\n",
    "# Combine data for boxplot\n",
    "combined_data = []\n",
    "\n",
    "for col in df1.columns:\n",
    "    combined_data.append(df1[col])\n",
    "    combined_data.append(df2[col])\n",
    "    combined_data.append(df3[col])\n",
    "\n",
    "# Create labels for x-axis\n",
    "labels = []\n",
    "for col in df1.columns:\n",
    "    labels.append(f'{col} TD2C_Fist_Estimation')\n",
    "    labels.append(f'{col} TD2C_Second_Estimation')\n",
    "    labels.append(f'{col} TD2C_Third_Estimation')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "box = plt.boxplot(combined_data, patch_artist=True)\n",
    "\n",
    "# Color customization\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "for patch, i in zip(box['boxes'], range(len(box['boxes']))):\n",
    "    patch.set_facecolor(colors[i % 3])\n",
    "\n",
    "plt.xticks(range(1, len(labels) + 1), labels, rotation=-90)\n",
    "plt.title(f'Boxplot of ROC-AUC scores for TD2C and Iterative TD2C (with {k} top var) with Regression to estimate MI (5 variables processes)')\n",
    "plt.xlabel('Processes')\n",
    "plt.ylabel('ROC-AUC score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for df1 and df2 with the ROC-AUC scores for each process over all graphs\n",
    "df1.mean().mean(), df2.mean().mean() , df3.mean().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITERATIVE TD2C\n",
    "\n",
    "def iterative_td2c(method = 'ts', k = 1, it = 3, top_vars = 3, treshold = True, treshold_value = 0.9, size_causal_df = 5,\n",
    "                   data_folder = 'home/data/', descr_folder = 'home/descr/', results_folder = 'home/results/'):\n",
    "    \"\"\"\n",
    "    # This function requires data already generated and stored in the data folder\n",
    "\n",
    "    # Methods:\n",
    "        # 'ts' = for classic TD2C\n",
    "        # 'original' = for original D2C\n",
    "        # 'ts_rank' = for TD2C with ranking\n",
    "        # 'ts_rank_2' = for TD2C with ranking 2\n",
    "        # 'ts_rank_3' = for TD2C with ranking 3\n",
    "        # 'ts_rank_4' = for TD2C with ranking 4\n",
    "        # 'ts_rank_5' = for TD2C with ranking 5\n",
    "        # 'ts_rank_6' = for TD2C with ranking 6\n",
    "        # 'ts_rank_7' = for TD2C with ranking 7\n",
    "        # 'ts_past' = for TD2C with past and future nodes\n",
    "        # 'ts_rank_no_count' = for TD2C with ranking with no contemporaneous nodes\n",
    "\n",
    "    Parameters:\n",
    "    # k is the number of top variables to keep at each iteration for each DAG (keep = 1 if treshold = False)\n",
    "    # it is the limit for the number of iterations to perform\n",
    "    # top_vars is the number of top variables to keep in case of TD2C Ranking\n",
    "    # treshold is a boolean to determine if we want to keep in the causal df the variables with a pred.proba higher than treshold_value\n",
    "    # treshold_value is the value to keep the variables in the causal df\n",
    "    # size_causal_df is the number of variables to keep in the causal_df in case of treshold = False\n",
    "    # data_folder is the folder where the data is stored\n",
    "    # descr_folder is the folder where the descriptors are stored\n",
    "    # results_folder is the folder where the results are stored\n",
    "\n",
    "    Stopping Criteria:\n",
    "     1. if average ROC-AUC score does not improve or is the same as the previous iteration for 3 consecutive iterations\n",
    "     2. if the first iteration has an average ROC-AUC score lower than 0.5\n",
    "     3. if the average ROC-AUC score is more than 0.2 points lower than the first iteration\n",
    "     4. if causal df is the same as the previous one for 3 consecutive iterations\n",
    "\n",
    "    Output:\n",
    "     1. Plot of average ROC-AUC scores (saved in results folder as pdf file)\n",
    "     2. Average ROC-AUC scores for each iteration (saved in results folder as csv file)\n",
    "    \"\"\"\n",
    "    \n",
    "    iter_df = pd.DataFrame\n",
    "    stop_1 = 0\n",
    "    stop_2 = 0\n",
    "    roc_scores = []\n",
    "\n",
    "    print()\n",
    "    print(f'Iterative TD2C - Method: {method} - Max iterations: {it} - Variables to keep per DAG: {k} - Top Variables: {top_vars} - Treshold: {treshold} - Size of Causal DF: {size_causal_df}')\n",
    "    print()\n",
    "    if COUPLES_TO_CONSIDER_PER_DAG == -1 and size_causal_df == 5 and treshold == False:\n",
    "        print('Using all couples for each DAG')\n",
    "        print(f'This iteration will take approximately {8.5*it} minutes')\n",
    "        print()\n",
    "    elif COUPLES_TO_CONSIDER_PER_DAG == -1 and treshold == True:\n",
    "        print(f'Using all couples for each DAG and a pred.proba higher than {treshold_value}')\n",
    "        print(f'This iteration will take approximately {10.5*it} minutes')\n",
    "        print()\n",
    "    elif COUPLES_TO_CONSIDER_PER_DAG != -1 and size_causal_df == 5:\n",
    "        print(f'Using the top {COUPLES_TO_CONSIDER_PER_DAG} couples for each DAG')\n",
    "        print(f'This iteration will take approximately {4*it} minutes')\n",
    "        print()\n",
    "    elif COUPLES_TO_CONSIDER_PER_DAG != -1 and size_causal_df == 1:\n",
    "        print(f'Using the top {COUPLES_TO_CONSIDER_PER_DAG} couples for each DAG')\n",
    "        print(f'This iteration will take approximately {3.5*it} minutes')\n",
    "        print()\n",
    "\n",
    "    print(\"Do you want to continue with the rest of the function? (y/n): \")\n",
    "\n",
    "    response = input(\"Do you want to continue with the rest of the function? (y/n): \").strip().lower()\n",
    "\n",
    "    if response in ['yes', 'y', 'Yes', 'Y']:\n",
    "        print()\n",
    "        print(\"Ok! Let's start the iteration.\")\n",
    "        print()\n",
    "\n",
    "        # Estimation For Cycle\n",
    "        for i in range(1,it+1):\n",
    "\n",
    "            print()\n",
    "            print(f'----------------------------  Estimation {i}  ----------------------------')\n",
    "            print()\n",
    "\n",
    "            warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "            input_folder = data_folder\n",
    "            \n",
    "            output_folder = descr_folder + f'estimate_{i}/'\n",
    "            \n",
    "            # Descriptors Generation #############################################################################\n",
    "            # List of files to process\n",
    "            to_process = []\n",
    "\n",
    "            # Filtering the files to process\n",
    "            for file in sorted(os.listdir(input_folder)):\n",
    "                gen_process_number = int(file.split('_')[0][1:])\n",
    "                n_variables = int(file.split('_')[1][1:])\n",
    "                max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "                noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "                if noise_std != noise_std_filter or max_neighborhood_size != max_neighborhood_size_filter:\n",
    "                    continue\n",
    "\n",
    "                to_process.append(file)\n",
    "\n",
    "            # Create output folder if it does not exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            print('Making Descriptors...')\n",
    "\n",
    "            # Process each file and create new DAGs based on causal paths\n",
    "            for file in tqdm(to_process):\n",
    "                gen_process_number = int(file.split('_')[0][1:])\n",
    "                n_variables = int(file.split('_')[1][1:])\n",
    "                max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "                noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "                dataloader = DataLoader(n_variables=n_variables, maxlags=maxlags)\n",
    "                dataloader.from_pickle(input_folder + file)\n",
    "\n",
    "                if i  == 1:\n",
    "                    d2c = D2C(\n",
    "                        observations=dataloader.get_observations(),\n",
    "                        dags=dataloader.get_dags(),\n",
    "                        couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "                        MB_size=MB_SIZE,\n",
    "                        n_variables=n_variables,\n",
    "                        maxlags=maxlags,\n",
    "                        seed=SEED,\n",
    "                        n_jobs=N_JOBS,\n",
    "                        full=True,\n",
    "                        quantiles=True,\n",
    "                        normalize=True,\n",
    "                        cmi='original',\n",
    "                        mb_estimator=method,\n",
    "                        top_vars=top_vars\n",
    "                    )\n",
    "                \n",
    "                else:\n",
    "                    d2c = D2C(\n",
    "                        observations=dataloader.get_observations(),\n",
    "                        dags=dataloader.get_dags(),\n",
    "                        couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG,\n",
    "                        MB_size=MB_SIZE,\n",
    "                        n_variables=n_variables,\n",
    "                        maxlags=maxlags,\n",
    "                        seed=SEED,\n",
    "                        n_jobs=N_JOBS,\n",
    "                        full=True,\n",
    "                        quantiles=True,\n",
    "                        normalize=True,\n",
    "                        cmi='original',\n",
    "                        mb_estimator= 'iterative',\n",
    "                        top_vars=top_vars,\n",
    "                        causal_df=iter_df\n",
    "                    )\n",
    "\n",
    "                d2c.initialize()  # Initializes the D2C object\n",
    "                descriptors_df = d2c.get_descriptors_df()  # Computes the descriptors\n",
    "\n",
    "                # Save the descriptors along with new DAGs if needed\n",
    "                descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "                descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "                descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "                descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "                descriptors_df.to_pickle(output_folder + f'Estimate_{i}_P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')\n",
    "\n",
    "            # Set Classifier #################################################################################\n",
    "            data_root = data_folder\n",
    "\n",
    "            to_dos = []\n",
    "\n",
    "            # This loop gets a list of all the files to be processed\n",
    "            for testing_file in sorted(os.listdir(data_root)):\n",
    "                if testing_file.endswith('.pkl'):\n",
    "                    gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "                    n_variables = int(testing_file.split('_')[1][1:])\n",
    "                    max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "                    noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "                    \n",
    "                    if noise_std != 0.01: # if the noise is different we skip the file\n",
    "                        continue\n",
    "\n",
    "                    if max_neighborhood_size != 2: # if the max_neighborhood_size is different we skip the file\n",
    "                        continue\n",
    "\n",
    "                    to_dos.append(testing_file) # we add the file to the list (to_dos) to be processed\n",
    "\n",
    "            # sort to_dos by number of variables\n",
    "            to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "            # to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "            # to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "\n",
    "            # we create a dictionary with the lists of files to be processed\n",
    "            todos = {'5': to_dos_5_variables} # , '10': to_dos_10_variables, '25': to_dos_25_variables\n",
    "\n",
    "            # we create a dictionary to store the results\n",
    "            dfs = []\n",
    "            descriptors_root = descr_folder + f'estimate_{i}/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(descriptors_root):\n",
    "                os.makedirs(descriptors_root)\n",
    "\n",
    "            # Re-save pickle files with protocol 4\n",
    "            for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "                if testing_file.endswith('.pkl'):\n",
    "                    file_path = os.path.join(descriptors_root, testing_file)\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        data = pickle.load(f)\n",
    "                    \n",
    "                    # Re-save with protocol 4\n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        pickle.dump(data, f, protocol=4)\n",
    "\n",
    "            # This loop gets the descriptors for the files to be processed\n",
    "            for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "                if testing_file.endswith('.pkl'):\n",
    "                    df = pd.read_pickle(os.path.join(descriptors_root, testing_file))\n",
    "                    if isinstance(df, pd.DataFrame):\n",
    "                        dfs.append(df)\n",
    "\n",
    "            # we concatenate the descriptors\n",
    "            descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "            # Classifier & Evaluation Metrics #################################################################\n",
    "\n",
    "            print('Classification & Evaluation Metrics')\n",
    "\n",
    "            for n_vars, todo in todos.items():\n",
    "\n",
    "                m1 = f'Estimate_{i}_rocs_process'\n",
    "                # m2 = f'Estimate_{i}_precision_process'\n",
    "                # m3 = f'Estimate_{i}_recall_process'\n",
    "                # m4 = f'Estimate_{i}_f1_process'\n",
    "\n",
    "                globals()[m1] = {}\n",
    "                # globals()[m2] = {}\n",
    "                # globals()[m3] = {}\n",
    "                # globals()[m4] = {}\n",
    "                causal_df_1 = {}\n",
    "\n",
    "                for testing_file in tqdm(todo):\n",
    "                    gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "                    n_variables = int(testing_file.split('_')[1][1:])\n",
    "                    max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "                    noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "                    # split training and testing data\n",
    "                    training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "                    X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "                    y_train = training_data['is_causal']\n",
    "\n",
    "                    testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "                    model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "                    # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "                    model = model.fit(X_train, y_train)\n",
    "\n",
    "                    rocs = {}\n",
    "                    # precisions = {}\n",
    "                    # recalls = {}\n",
    "                    # f1s = {}\n",
    "                    causal_dfs = {}\n",
    "                    for graph_id in range(40):\n",
    "                        #load testing descriptors\n",
    "                        test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "                        test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "                        X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "                        y_test = test_df['is_causal']\n",
    "\n",
    "                        y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "                        y_pred = model.predict(X_test)\n",
    "\n",
    "                        roc = roc_auc_score(y_test, y_pred_proba)\n",
    "                        # precision = precision_score(y_test, y_pred)\n",
    "                        # recall = recall_score(y_test, y_pred)\n",
    "                        # f1 = f1_score(y_test, y_pred)\n",
    "                        \n",
    "                        rocs[graph_id] = roc\n",
    "                        # precisions[graph_id] = precision\n",
    "                        # recalls[graph_id] = recall\n",
    "                        # f1s[graph_id] = f1\n",
    "                        \n",
    "                        # add to causal_df test_df, y_pred_proba and y_pred\n",
    "                        causal_dfs[graph_id] = test_df\n",
    "                        causal_dfs[graph_id]['y_pred_proba'] = y_pred_proba\n",
    "                        causal_dfs[graph_id]['y_pred'] = y_pred\n",
    "\n",
    "                    causal_df_1[gen_process_number] = causal_dfs\n",
    "                    globals()[m1][gen_process_number] = rocs\n",
    "                    # globals()[m2][gen_process_number] = precisions\n",
    "                    # globals()[m3][gen_process_number] = recalls\n",
    "                    # globals()[m4][gen_process_number] = f1s\n",
    "\n",
    "            # pickle everything\n",
    "            output_folder = results_folder + f'journals/estimate_{i}/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            with open(os.path.join(output_folder, f'journal_results_td2c_R_N5.pkl'), 'wb') as f:\n",
    "                everything = (globals()[m1], causal_df_1) #, globals()[m2], globals()[m3], globals()[m4]\n",
    "                pickle.dump(everything, f)\n",
    "\n",
    "            # Load results #####################################################################################\n",
    "            input_folder = results_folder + f'journals/estimate_{i}/'\n",
    "\n",
    "            with open(os.path.join(input_folder, f'journal_results_td2c_R_N5.pkl'), 'rb') as f:\n",
    "                TD2C_1_rocs_process, causal_df_1 = pickle.load(f) # , TD2C_1_precision_process, TD2C_1_recall_process, TD2C_1_f1_process\n",
    "\n",
    "\n",
    "            # STOPPING CRITERIA 2: Using ROC-AUC score\n",
    "            roc = pd.DataFrame(TD2C_1_rocs_process).mean().mean()\n",
    "\n",
    "            if i == 1:\n",
    "                if roc > 0.5:\n",
    "                    roc_first = roc\n",
    "                else:\n",
    "                    print('ROC-AUC is too low, let\\'s stop here.')\n",
    "                    break\n",
    "            elif i > 1:\n",
    "                if roc <= roc_0:\n",
    "                    stop_2 = stop_2 + 1\n",
    "                    if stop_2 == 3:\n",
    "                        print()\n",
    "                        print('Estimation are not improving, let\\'s stop here.')\n",
    "                        print()\n",
    "                        break\n",
    "                else:\n",
    "                    stop_2 = 0\n",
    "                \n",
    "                if roc <= roc_first-0.2:\n",
    "                    print()\n",
    "                    print('Estimation are not improving, let\\'s stop here.')\n",
    "                    print()\n",
    "                    break\n",
    "            \n",
    "            print()\n",
    "            print(f'ROC-AUC score: {roc}')\n",
    "            print()\n",
    "            roc_scores.append(roc)\n",
    "            roc_0 = roc\n",
    "\n",
    "            # Reshape causal_df #################################################################################\n",
    "            # keep only rows for top k y_pred_proba\n",
    "            for process_id, process_data in causal_df_1.items():\n",
    "                for graph_id, graph_data in process_data.items():\n",
    "                    causal_df_1[process_id][graph_id] = graph_data.nlargest(k, 'y_pred_proba')\n",
    "\n",
    "            # for each causal_df keep only process_id, graph_id, edge_source, edge_dest and y_pred_proba\n",
    "            for process_id, process_data in causal_df_1.items():\n",
    "                for graph_id, graph_data in process_data.items():\n",
    "                    causal_df_1[process_id][graph_id] = graph_data[['process_id', 'graph_id', 'edge_source', 'edge_dest', 'y_pred_proba']]\n",
    "                    causal_df_1[process_id][graph_id].reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # save the causal_df as a pkl file alone\n",
    "            output_folder = results_folder + f'metrics/estimate_{i}/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'wb') as f:\n",
    "                pickle.dump(causal_df_1, f)\n",
    "\n",
    "            # Unify causal_df #################################################################################\n",
    "            input_folder = results_folder + f'metrics/estimate_{i}/'\n",
    "\n",
    "            with open(os.path.join(input_folder, f'causal_df_top_{k}_td2c_R_N5.pkl'), 'rb') as f:\n",
    "                causal_df_1 = pickle.load(f)\n",
    "\n",
    "            # create a dataframe with all the causal_df\n",
    "            dfs = []\n",
    "            for process_id, process_data in causal_df_1.items():\n",
    "                for graph_id, graph_data in process_data.items():\n",
    "                    dfs.append(graph_data)\n",
    "\n",
    "            causal_df_unif_1 = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "            # sort in ascending order by process_id, graph_id, edge_source and edge_dest\n",
    "            causal_df_unif_1.sort_values(by=['process_id', 'graph_id', 'edge_source', 'edge_dest'], inplace=True)\n",
    "\n",
    "            # # keep couples edge-dest,edge_source present for more than one dag (TO TRY)\n",
    "            # causal_df_unif_1 = causal_df_unif_1[causal_df_unif_1.duplicated(subset=['edge_source', 'edge_dest'], keep=False)]\n",
    "\n",
    "            # unique of causal_df_unif for couples of edge_source and edge_dest\n",
    "            causal_df_unif_1 = causal_df_unif_1.drop_duplicates(subset=['edge_source', 'edge_dest'])\n",
    "\n",
    "            # KEEP VARIABLES IN CAUSAL_DF FOR A TRESHOLD OR TOP N \n",
    "            if treshold == True:\n",
    "                # drop rows with y_pred_proba < 0.7 (not necessary given the next step)\n",
    "                causal_df_unif_1 = causal_df_unif_1[causal_df_unif_1['y_pred_proba'] >= treshold_value]\n",
    "                if causal_df_unif_1.shape[0] > 1:\n",
    "                    causal_df_unif_1 = causal_df_unif_1.nlargest(10, 'y_pred_proba')\n",
    "\n",
    "            else:\n",
    "                # if n row > 5, keep only the top 5 rows with highest y_pred_proba\n",
    "                if causal_df_unif_1.shape[0] > 1:\n",
    "                    causal_df_unif_1 = causal_df_unif_1.nlargest(size_causal_df, 'y_pred_proba')\n",
    "\n",
    "            # index reset\n",
    "            causal_df_unif_1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # STOPPING CRITERIA 1: if causal df is the same as the previous one for 2 consecutive iterations\n",
    "            if causal_df_unif_1.equals(iter_df):\n",
    "                stop_1 = stop_1 + 1\n",
    "                if stop_1 == 2:\n",
    "                    print()\n",
    "                    print(f'No new edges to add in the next iteration')\n",
    "                    print()\n",
    "                    break\n",
    "            else:\n",
    "                stop_1 = 0\n",
    "\n",
    "            # save the causal_df as a pkl file alone\n",
    "            output_folder = results_folder + f'metrics/estimate_{i}/'\n",
    "\n",
    "            # Create the folder if it doesn't exist\n",
    "            if not os.path.exists(output_folder):\n",
    "                os.makedirs(output_folder)\n",
    "\n",
    "            with open(os.path.join(output_folder, f'causal_df_top_{k}_td2c_R_N5_unified.pkl'), 'wb') as f:\n",
    "                pickle.dump(causal_df_unif_1, f)\n",
    "\n",
    "            iter_df = causal_df_unif_1\n",
    "\n",
    "            print()\n",
    "            print(f'Most relevant Edges that will be added in the next iteration:')\n",
    "            print(causal_df_unif_1)\n",
    "            print()\n",
    "\n",
    "        # PLOT RESULTS #################################################################################\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(range(1, it+1), roc_scores, marker='o')\n",
    "        if treshold == False:\n",
    "            size = size_causal_df\n",
    "            plt.title(f'ROC-AUC scores for Iterative {method} ({it} iterations and {size} top vars) with Regression MI (5 vars processes) ({COUPLES_TO_CONSIDER_PER_DAG} couples per dag)')\n",
    "        else:\n",
    "            size = k\n",
    "            plt.title(f'ROC-AUC scores for Iterative {method} ({it} iterations and {size} top vars) with Regression MI (5 vars processes) ({COUPLES_TO_CONSIDER_PER_DAG} couples per dag)')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('ROC-AUC score')\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # save the plot in folder\n",
    "        output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/try_complete_function/plots/'\n",
    "        plt.savefig(output_folder + f'ROC_AUC_scores_TD2C_{method}_{it}_iterations_{size}_top_vars_{COUPLES_TO_CONSIDER_PER_DAG}_couples_per_dag.pdf')\n",
    "\n",
    "        roc_scores_df = pd.DataFrame(roc_scores, columns=['roc_score'])\n",
    "        roc_scores_df['iteration'] = range(1, it+1)\n",
    "\n",
    "        # save the df in a csv file\n",
    "        output_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/try_complete_function/metrics/'\n",
    "        roc_scores_df.to_csv(output_folder + f'roc_scores_TD2C_{method}_{it}_iterations_{size}_top_vars_{COUPLES_TO_CONSIDER_PER_DAG}_couples_per_dag.csv', index=False)\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        print()\n",
    "        print(\"Wise choice! Change the parameters and try again.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_td2c(method = 'ts', k = 5, it = 6, top_vars=3, treshold = True, treshold_value = 0.8, size_causal_df = 5,\n",
    "               data_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/', \n",
    "               descr_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/try_complete_function/', \n",
    "               results_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/try_complete_function/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative TD2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We suggest you to keep k higher than 10 for a better performance\n",
      "\n",
      "Iterative TD2C - Method: ts - Max iterations: 3 - Variables to keep per DAG: 3 - Top Variables: 4\n",
      "\n",
      "\n",
      "########################################################################################\n",
      "  WARNING: The number of iterations is less than 6. The results might not be accurate.  \n",
      "########################################################################################\n",
      "\n",
      "\n",
      "\n",
      "Ok! Let's start the iteration.\n",
      "\n",
      "\n",
      "----------------------------  Estimation 0  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:20<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification & Evaluation Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [01:21<00:00,  4.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROC-AUC score: 0.8569\n",
      "\n",
      "2\n",
      "0.8\n",
      "\n",
      "Example of most relevant Edges that will be added in the next iteration:\n",
      "   process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "0           1        19            7          2          0.94\n",
      "1           1        19           19          1          0.88\n",
      "\n",
      "Oerall most relevant Edges found in the last iteration:\n",
      "   process_id  graph_id  edge_source  edge_dest  y_pred_proba\n",
      "0           6         7            5          0           1.0\n",
      "1           6        11            5          0           1.0\n",
      "2           6        22            5          0           1.0\n",
      "3           6        30            8          3           1.0\n",
      "4           8         4            7          2           1.0\n",
      "\n",
      "\n",
      "----------------------------  Estimation 1  ----------------------------\n",
      "\n",
      "Making Descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 6/18 [00:17<00:35,  2.95s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m Iter \u001b[38;5;241m=\u001b[39m IterativeTD2C(\n\u001b[1;32m      2\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     noise_std_filter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[43mIter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# primary:\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# AGGIUNGI OUTPUT FINALE CON GRAFICO DI UN DAG CHE USA I MIGLIORI EDGES\u001b[39;00m\n",
      "File \u001b[0;32m~/td2c/src/d2c/benchmark/iterative_td2c.py:58\u001b[0m, in \u001b[0;36mIterativeTD2C.main\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOk! Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms start the iteration.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 58\u001b[0m roc_scores, causal_dfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_scores:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(roc_scores)\n",
      "File \u001b[0;32m~/td2c/src/d2c/benchmark/iterative_td2c.py:182\u001b[0m, in \u001b[0;36mIterativeTD2C.iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m input_folder, output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialization(i\u001b[38;5;241m=\u001b[39mi)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Descriptors Generation _____________________________________________________________________________\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescriptors_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Set Classifier _____________________________________________________________________________________\u001b[39;00m\n\u001b[1;32m    186\u001b[0m descriptors_training, todos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_classifier(i)\n",
      "File \u001b[0;32m~/td2c/src/d2c/benchmark/iterative_td2c.py:303\u001b[0m, in \u001b[0;36mIterativeTD2C.descriptors_generation\u001b[0;34m(self, i, input_folder, output_folder)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# i > 0 iterations: uses the causal_df from the previous iteration and the strategy to adjust the causal_df for the next iteration\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     d2c \u001b[38;5;241m=\u001b[39m D2C(\n\u001b[1;32m    286\u001b[0m         observations \u001b[38;5;241m=\u001b[39m dataloader\u001b[38;5;241m.\u001b[39mget_observations(),\n\u001b[1;32m    287\u001b[0m         dags \u001b[38;5;241m=\u001b[39m dataloader\u001b[38;5;241m.\u001b[39mget_dags(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    300\u001b[0m         causal_df_list \u001b[38;5;241m=\u001b[39m  dataloader\u001b[38;5;241m.\u001b[39mfrom_pickle_causal_df(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcausal_dfs/single/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mit_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_causal_df_top_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_td2c_R_N5.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    301\u001b[0m     )\n\u001b[0;32m--> 303\u001b[0m \u001b[43md2c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Initializes the D2C object\u001b[39;00m\n\u001b[1;32m    304\u001b[0m descriptors_df \u001b[38;5;241m=\u001b[39m d2c\u001b[38;5;241m.\u001b[39mget_descriptors_df()  \u001b[38;5;66;03m# Computes the descriptors\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Save the descriptors along with new DAGs if needed\u001b[39;00m\n",
      "File \u001b[0;32m~/td2c/src/d2c/descriptors_generation/d2c.py:103\u001b[0m, in \u001b[0;36mD2C.initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m     args \u001b[38;5;241m=\u001b[39m [(dag_idx, dag, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_variables, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxlags, num_samples) \u001b[38;5;28;01mfor\u001b[39;00m dag_idx, dag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDAGs)]\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m--> 103\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_descriptors_with_dag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m#merge lists into a single list\u001b[39;00m\n\u001b[1;32m    106\u001b[0m results \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n",
      "File \u001b[0;32m~/python3.8/lib/python3.8/multiprocessing/pool.py:372\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python3.8/lib/python3.8/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/python3.8/lib/python3.8/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python3.8/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/python3.8/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Iter = IterativeTD2C(\n",
    "    method = 'ts', \n",
    "    k = 3, \n",
    "    it = 3, \n",
    "    top_vars = 4, \n",
    "    strategy = \"Classic\",\n",
    "    data_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/data/to_use/', \n",
    "    descr_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/descriptors/Regression/TD2C/try_complete_function/', \n",
    "    results_folder = '/home/jpalombarini/td2c/notebooks/contributions/td2c_extesions/results/Regression/try_complete_function/',\n",
    "    COUPLES_TO_CONSIDER_PER_DAG = 40,\n",
    "    MB_SIZE = 2,\n",
    "    SEED = 42,\n",
    "    maxlags = 5,\n",
    "    max_neighborhood_size_filter = 2,\n",
    "    N_JOBS = 40,\n",
    "    noise_std_filter = 0.01\n",
    ")\n",
    "\n",
    "Iter.main()\n",
    "\n",
    "# primary:\n",
    "# AGGIUNGI OUTPUT FINALE CON GRAFICO DI UN DAG CHE USA I MIGLIORI EDGES\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
