{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook is designed for conducting experiments or analyses related to a machine learning pipeline involving `TD2C` method It focuses on data processing, model training, and evaluation within the context of `multivariate TS` data. The notebook includes steps for selecting and loading datasets, computing descriptors, and training a `Random Forest` classifiers.\n",
    "\n",
    "### Breakdown of the Notebook:\n",
    "\n",
    "1. **Scope and Purpose**:\n",
    "    - Data Selection: The notebook filters and selects relevant files containing time-series data, ensuring consistency with precomputed descriptors.\n",
    "    - Descriptor Loading: It loads and concatenates data descriptors, which are crucial for the subsequent machine learning tasks.\n",
    "    - Model Training and Evaluation: It employs machine learning models (particularly Random Forest) to train on the selected data and then evaluates the models using various metrics.\n",
    "\n",
    "2. **Key Modules and Libraries**:\n",
    "    - `Pandas`: For data manipulation and handling.\n",
    "    - `NumPy`: For numerical computations.\n",
    "    - `Scikit-learn`: For implementing machine learning models and evaluation metrics.\n",
    "    - `Imbalanced-learn`: Specifically for handling imbalanced datasets using a BalancedRandomForestClassifier.\n",
    "    - Custom Modules (`d2c.benchmark`, `d2c.descriptors.loader`): These seem to be part of a custom package or framework, likely related to the `D2C` (Data to Concepts) methodology.\n",
    "\n",
    "3. **Main Functions and Classes**:\n",
    "    - `DataLoader`: Likely used to load data descriptors from files.\n",
    "    - `D2CWrapper`: Presumably a wrapper class that facilitates benchmarking of models using the D2C approach.\n",
    "    - `RandomForestClassifier`: A machine learning model used to train on the data.\n",
    "    - `BalancedRandomForestClassifier`: An alternative to the standard RandomForest, designed to handle imbalanced datasets.\n",
    "\n",
    "4. **Outputs**: The notebook produces several key outputs:\n",
    "    - Trained Machine Learning Models: After processing and training, models like the Random Forest classifier are produced.\n",
    "    - Evaluation Metrics: The notebook computes and outputs various metrics (e.g., `accuracy`, `F1 score`, `ROC-AUC`) to evaluate the performance of the trained models.\n",
    "    - Filtered Data Files: It also generates or modifies lists of files to be processed based on specific criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "\n",
    "from d2c.benchmark import D2CWrapper\n",
    "\n",
    "from d2c.descriptors_generation.loader import DataLoader\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET LAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlags = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select only the files that we are interested in working with. This should be coherent with the files for which we have computed the descriptors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/jpalombarini/td2c/notebooks/paper_td2c/.data/'\n",
    "\n",
    "to_dos = []\n",
    "\n",
    "# This loop gest a list of all the files to be processed\n",
    "for testing_file in sorted(os.listdir(data_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "          \n",
    "        if noise_std != 0.01: # if the noise is different we skip the file\n",
    "            continue\n",
    "\n",
    "        if max_neighborhood_size != 2: # if the max_neighborhood_size is different we skip the file\n",
    "            continue\n",
    "\n",
    "        to_dos.append(testing_file) # we add the file to the list (to_dos) to be processed\n",
    "\n",
    "# sort to_dos by number of variables\n",
    "to_dos_5_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 5]\n",
    "to_dos_10_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 10]\n",
    "to_dos_25_variables = [file for file in to_dos if int(file.split('_')[1][1:]) == 25]\n",
    "\n",
    "# we create a dictionary with the lists of files to be processed\n",
    "todos = {'5': to_dos_5_variables, '10': to_dos_10_variables, '25': to_dos_25_variables}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a dictionary to store the results\n",
    "\n",
    "dfs = []\n",
    "descriptors_root = '/home/jpalombarini/td2c/notebooks/paper_td2c/.descriptors_ts_original_entropy/'\n",
    "\n",
    "# This loop gets the descriptors for the files to be processed\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(descriptors_root + testing_file)\n",
    "        dfs.append(df)\n",
    "\n",
    "# we concatenate the descriptors\n",
    "descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the precision recall curve against the threshold\n",
    "# from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, precision_recall_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# maxlags = 5\n",
    "\n",
    "# td2c_rocs_process = {}\n",
    "# td2c_precision_process = {}\n",
    "# td2c_recall_process = {}\n",
    "# td2c_f1_process = {}\n",
    "\n",
    "# for testing_file in to_dos_5_variables:\n",
    "#     print(\"Working on\", testing_file)\n",
    "#     gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "#     n_variables = int(testing_file.split('_')[1][1:])\n",
    "#     max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "#     noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "\n",
    "#     # split training and testing data\n",
    "#     training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "#     X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "#     y_train = training_data['is_causal']\n",
    "\n",
    "#     testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "#     model = BalancedRandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "#     # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     rocs = {}\n",
    "#     precisions = {}\n",
    "#     recalls = {}\n",
    "#     f1s = {}\n",
    "#     #load testing descriptors\n",
    "#     test_df = testing_data\n",
    "#     test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort to match the order of the model!\n",
    "\n",
    "#     X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "#     y_test = test_df['is_causal']\n",
    "\n",
    "#     y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "#     y_pred = model.predict(X_test)\n",
    "\n",
    "#     # Step 2: Calculate precision, recall, and thresholds\n",
    "#     precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "#     # Step 3: Plot the precision-recall curve\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(thresholds, precision[:-1], label='Precision')\n",
    "#     plt.plot(thresholds, recall[:-1], label='Recall')\n",
    "#     plt.xlabel('Threshold')\n",
    "#     plt.ylabel('Precision/Recall')\n",
    "#     plt.title('Precision-Recall vs Threshold')\n",
    "#     plt.legend()\n",
    "#     plt.grid()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "# maxlags = 5\n",
    "# descriptors_root = './descriptors_ts_original_entropy/'\n",
    "# data_root = '../data/'\n",
    "\n",
    "# training_set_full = []\n",
    "# for testing_file in to_dos_5_variables:\n",
    "#     gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "#     n_variables = int(testing_file.split('_')[1][1:])\n",
    "#     max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "#     noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "#     dataloader = DataLoader(n_variables = n_variables,\n",
    "#                 maxlags = maxlags)\n",
    "#     dataloader.from_pickle(data_root+testing_file)\n",
    "#     true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "#     for i in range(40):\n",
    "\n",
    "#         test_df = pd.read_csv(descriptors_root+testing_file+'_'+str(i)+'.csv', index_col=0)\n",
    "#         test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort to match the order of the model!\n",
    "#         test_df['is_causal'] = true_causal_dfs[i]['is_causal']\n",
    "#         test_df['process_id'] = gen_process_number\n",
    "#         test_df['graph_id'] = i\n",
    "#         test_df['n_variables'] = n_variables\n",
    "#         test_df['max_neighborhood_size'] = max_neighborhood_size\n",
    "#         test_df['noise_std'] = noise_std\n",
    "\n",
    "#         training_set_full.append(test_df)\n",
    "\n",
    "# training_set_full = pd.concat(training_set_full, axis=0).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop does the following:\n",
    "# 1. Creates some dictionaries to store the results\n",
    "# 2. Loads the training data\n",
    "# 3. Trains the model\n",
    "# 4. Evaluates the model\n",
    "# 5. Stores the results in the dictionaries\n",
    "# 6. Saves the dictionaries in a pickle file\n",
    "\n",
    "for n_vars, todo in todos.items():\n",
    "    td2c_rocs_process = {}\n",
    "    td2c_precision_process = {}\n",
    "    td2c_recall_process = {}\n",
    "    td2c_f1_process = {}\n",
    "    for testing_file in tqdm(todo):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "        # split training and testing data\n",
    "        training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "        model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "        # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        rocs = {}\n",
    "        precisions = {}\n",
    "        recalls = {}\n",
    "        f1s = {}\n",
    "        for graph_id in range(40):\n",
    "            #load testing descriptors\n",
    "            test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "            test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "            X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_test = test_df['is_causal']\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            roc = roc_auc_score(y_test, y_pred_proba)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            rocs[graph_id] = roc\n",
    "            precisions[graph_id] = precision\n",
    "            recalls[graph_id] = recall\n",
    "            f1s[graph_id] = f1\n",
    "\n",
    "        td2c_rocs_process[gen_process_number] = rocs\n",
    "        td2c_precision_process[gen_process_number] = precisions\n",
    "        td2c_recall_process[gen_process_number] = recalls\n",
    "        td2c_f1_process[gen_process_number] = f1s\n",
    "\n",
    "    # pickle everything\n",
    "    with open(f'journal_results_t2dc_N{n_vars}.pkl', 'wb') as f:\n",
    "        everything = (td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process)\n",
    "        pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD2C CMIKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "descriptors_root = '/home/jpalombarini/td2c/notebooks/paper_td2c/.descriptors_ts_cmiknn_entropy/'\n",
    "for testing_file in sorted(os.listdir(descriptors_root)):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(descriptors_root + testing_file)\n",
    "        dfs.append(df)\n",
    "\n",
    "descriptors_training = pd.concat(dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for n_vars, todo in todos.items():\n",
    "    td2c_rocs_process = {}\n",
    "    td2c_precision_process = {}\n",
    "    td2c_recall_process = {}\n",
    "    td2c_f1_process = {}\n",
    "    for testing_file in tqdm(todo):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "        # split training and testing data\n",
    "        training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        testing_data = descriptors_training.loc[(descriptors_training['process_id'] == gen_process_number) & (descriptors_training['n_variables'] == n_variables) & (descriptors_training['max_neighborhood_size'] == max_neighborhood_size) & (descriptors_training['noise_std'] == noise_std)]\n",
    "\n",
    "        model = BalancedRandomForestClassifier(n_estimators=100, random_state=0, n_jobs=50, max_depth=None, sampling_strategy='auto', replacement=True, bootstrap=False)\n",
    "        # model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50, max_depth=10)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        rocs = {}\n",
    "        precisions = {}\n",
    "        recalls = {}\n",
    "        f1s = {}\n",
    "        for graph_id in range(40):\n",
    "            #load testing descriptors\n",
    "            test_df = testing_data.loc[testing_data['graph_id'] == graph_id]\n",
    "            test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True) # sort for coherence\n",
    "\n",
    "            X_test = test_df.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_test = test_df['is_causal']\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            roc = roc_auc_score(y_test, y_pred_proba)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            rocs[graph_id] = roc\n",
    "            precisions[graph_id] = precision\n",
    "            recalls[graph_id] = recall\n",
    "            f1s[graph_id] = f1\n",
    "\n",
    "        td2c_rocs_process[gen_process_number] = rocs\n",
    "        td2c_precision_process[gen_process_number] = precisions\n",
    "        td2c_recall_process[gen_process_number] = recalls\n",
    "        td2c_f1_process[gen_process_number] = f1s\n",
    "\n",
    "    # pickle everything\n",
    "    with open(f'journal_results_t2dc_cmiknn_N{n_vars}.pkl', 'wb') as f:\n",
    "        everything = (td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process)\n",
    "        pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td2c_rocs_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load \n",
    "# with open('journal_results_t2dc_N5.pkl', 'rb') as f:\n",
    "#     td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix_td2c = pd.concat([pd.DataFrame(td2c_rocs_process).mean(),pd.DataFrame(td2c_precision_process).mean(),pd.DataFrame(td2c_recall_process).mean(),pd.DataFrame(td2c_f1_process).mean()],axis=1)\n",
    "# mix_td2c.columns = ['roc','precision','recall','f1']\n",
    "# #index name 'generative process'\n",
    "# mix_td2c.index.name = 'generative process'\n",
    "# mix_td2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix_td2c = pd.concat([pd.DataFrame(td2c_rocs_process).mean(),pd.DataFrame(td2c_precision_process).mean(),pd.DataFrame(td2c_recall_process).mean(),pd.DataFrame(td2c_f1_process).mean()],axis=1)\n",
    "# mix_td2c.columns = ['roc','precision','recall','f1']\n",
    "# #index name 'generative process'\n",
    "# mix_td2c.index.name = 'generative process'\n",
    "# mix_td2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix_td2c = pd.concat([pd.DataFrame(td2c_rocs_process).mean(),pd.DataFrame(td2c_precision_process).mean(),pd.DataFrame(td2c_recall_process).mean(),pd.DataFrame(td2c_f1_process).mean()],axis=1)\n",
    "# mix_td2c.columns = ['roc','precision','recall','f1']\n",
    "# #index name 'generative process'\n",
    "# mix_td2c.index.name = 'generative process'\n",
    "# mix_td2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix_pcmci = pd.concat([pd.DataFrame(pcmci_rocs_process).mean(),pd.DataFrame(pcmci_precision_process).mean(),pd.DataFrame(pcmci_recall_process).mean(),pd.DataFrame(pcmci_f1_process).mean()],axis=1)\n",
    "# mix_pcmci.columns = ['roc','precision','recall','f1']\n",
    "# #index name 'generative process'\n",
    "# mix_pcmci.index.name = 'generative process'\n",
    "# mix_pcmci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETA 8 min\n",
    "\n",
    "from d2c.benchmark.granger import Granger\n",
    "# suppress Future Warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "for n_vars, todo in todos.items():\n",
    "\n",
    "        granger_rocs_process = {}\n",
    "        granger_precision_process = {}\n",
    "        granger_recall_process = {}\n",
    "        granger_f1_process = {}\n",
    "\n",
    "        for testing_file in tqdm(todo):\n",
    "                gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "                n_variables = int(testing_file.split('_')[1][1:])\n",
    "                max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "                noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "                # load original data for truth values\n",
    "                dataloader = DataLoader(n_variables = n_variables,\n",
    "                                maxlags = maxlags)\n",
    "                dataloader.from_pickle(data_root+testing_file)\n",
    "                observations = dataloader.get_original_observations()\n",
    "                true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "\n",
    "                granger = Granger(ts_list=observations, maxlags=maxlags, n_jobs=40)\n",
    "                granger.run()\n",
    "                causal_dfs_granger = granger.get_causal_dfs()\n",
    "                rocs = {}\n",
    "                precisions = {}\n",
    "                recalls = {}\n",
    "                f1s = {}\n",
    "                for i in range(40):\n",
    "\n",
    "                        y_pred = causal_dfs_granger[i]['is_causal'].astype(int)\n",
    "                        y_pred_proba = 1 - causal_dfs_granger[i]['p_value']\n",
    "                        y_test = true_causal_dfs[i]['is_causal'].astype(int)\n",
    "\n",
    "                        roc = roc_auc_score(y_test, y_pred_proba)\n",
    "                        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "                        recall = recall_score(y_test, y_pred)\n",
    "                        f1 = f1_score(y_test, y_pred)\n",
    "                        \n",
    "                        rocs[i] = roc\n",
    "                        precisions[i] = precision\n",
    "                        recalls[i] = recall\n",
    "                        f1s[i] = f1\n",
    "\n",
    "                granger_rocs_process[gen_process_number] = rocs\n",
    "                granger_precision_process[gen_process_number] = precisions\n",
    "                granger_recall_process[gen_process_number] = recalls\n",
    "                granger_f1_process[gen_process_number] = f1s\n",
    "\n",
    "        # pickle everything\n",
    "        with open(f'journal_results_granger_N{n_vars}.pkl', 'wb') as f:\n",
    "                everything = (granger_rocs_process, granger_precision_process, granger_recall_process, granger_f1_process)\n",
    "                pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle everything\n",
    "with open('journal_results_granger_N25.pkl', 'wb') as f:\n",
    "    everything = (granger_rocs_process, granger_precision_process, granger_recall_process, granger_f1_process)\n",
    "    pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETA 50 min \n",
    "\n",
    "from d2c.benchmark.pcmci import PCMCI\n",
    "\n",
    "for n_vars, todo in todos.items():\n",
    "\n",
    "        pcmci_rocs_process = {}\n",
    "        pcmci_precision_process = {}\n",
    "        pcmci_recall_process = {}\n",
    "        pcmci_f1_process = {}\n",
    "\n",
    "        for testing_file in tqdm(todo):\n",
    "                gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "                n_variables = int(testing_file.split('_')[1][1:])\n",
    "                max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "                noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "                # load original data for truth values\n",
    "                dataloader = DataLoader(n_variables = n_variables,\n",
    "                                maxlags = maxlags)\n",
    "                dataloader.from_pickle(data_root+testing_file)\n",
    "                observations = dataloader.get_original_observations()\n",
    "                true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "\n",
    "                pcmci = PCMCI(ts_list=observations, maxlags=maxlags, n_jobs=40, ci=\"ParCorr\")\n",
    "                pcmci.run()\n",
    "                causal_dfs_pcmci = pcmci.get_causal_dfs()\n",
    "                rocs = {}\n",
    "                precisions = {}\n",
    "                recalls = {}\n",
    "                f1s = {}\n",
    "                for i in range(40):\n",
    "\n",
    "                        y_pred = causal_dfs_pcmci[i]['is_causal'].astype(int)\n",
    "                        y_pred_proba = 1 - causal_dfs_pcmci[i]['p_value']\n",
    "                        y_test = true_causal_dfs[i]['is_causal'].astype(int)\n",
    "\n",
    "                        roc = roc_auc_score(y_test, y_pred_proba)\n",
    "                        precision = precision_score(y_test, y_pred)\n",
    "                        recall = recall_score(y_test, y_pred)\n",
    "                        f1 = f1_score(y_test, y_pred)\n",
    "                        \n",
    "                        rocs[i] = roc\n",
    "                        precisions[i] = precision\n",
    "                        recalls[i] = recall\n",
    "                        f1s[i] = f1\n",
    "\n",
    "                pcmci_rocs_process[gen_process_number] = rocs\n",
    "                pcmci_precision_process[gen_process_number] = precisions\n",
    "                pcmci_recall_process[gen_process_number] = recalls\n",
    "                pcmci_f1_process[gen_process_number] = f1s\n",
    "\n",
    "        # pickle everything\n",
    "        with open(f'journal_results_pcmci_N{n_vars}.pkl', 'wb') as f:\n",
    "                everything = (pcmci_rocs_process, pcmci_precision_process, pcmci_recall_process, pcmci_f1_process)\n",
    "                pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix_pcmci = pd.concat([pd.DataFrame(pcmci_rocs_process).mean(),pd.DataFrame(pcmci_precision_process).mean(),pd.DataFrame(pcmci_recall_process).mean(),pd.DataFrame(pcmci_f1_process).mean()],axis=1)\n",
    "# mix_pcmci.columns = ['roc','precision','recall','f1']\n",
    "# #index name 'generative process'\n",
    "# mix_pcmci.index.name = 'generative process'\n",
    "# mix_pcmci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install -y seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # boxplot mix_pcmci and mix_td2c\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# fig, ax = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "# sns.boxplot(data=mix_pcmci, ax=ax[0, 0])\n",
    "# ax[0, 0].set_title('PCMCI')\n",
    "# ax[0, 0].set_ylabel('Score')\n",
    "# ax[0, 0].set_xlabel('Metric')\n",
    "\n",
    "# sns.boxplot(data=mix_td2c, ax=ax[0, 1])\n",
    "# ax[0, 1].set_title('TD2C')\n",
    "# ax[0, 1].set_ylabel('Score')\n",
    "# ax[0, 1].set_xlabel('Metric')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynotears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2c.benchmark.dynotears import DYNOTEARS\n",
    "N_JOBS = 40\n",
    "\n",
    "for n_vars, todo in todos.items():\n",
    "\n",
    "        dyno_rocs_process = {}\n",
    "        dyno_precision_process = {}\n",
    "        dyno_recall_process = {}\n",
    "        dyno_f1_process = {}\n",
    "\n",
    "        for testing_file in tqdm(todo):\n",
    "                gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "                n_variables = int(testing_file.split('_')[1][1:])\n",
    "                max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "                noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "                # load original data for truth values\n",
    "                dataloader = DataLoader(n_variables = n_variables,\n",
    "                                maxlags = maxlags)\n",
    "                dataloader.from_pickle(data_root+testing_file)\n",
    "                observations = dataloader.get_original_observations()\n",
    "                true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "                dynotears = DYNOTEARS(ts_list=observations, maxlags=maxlags, n_jobs=N_JOBS)\n",
    "                dynotears.run()\n",
    "                causal_dfs_dynotears = dynotears.get_causal_dfs()\n",
    "\n",
    "                rocs = {}\n",
    "                precisions = {}\n",
    "                recalls = {}\n",
    "                f1s = {}\n",
    "                for i in range(40):\n",
    "\n",
    "                        y_pred = causal_dfs_dynotears[i]['is_causal'].astype(int)\n",
    "                        y_pred_proba = 1 - causal_dfs_dynotears[i]['p_value']\n",
    "                        y_test = true_causal_dfs[i]['is_causal'].astype(int)\n",
    "\n",
    "                        roc = None\n",
    "                        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "                        recall = recall_score(y_test, y_pred)\n",
    "                        f1 = f1_score(y_test, y_pred)\n",
    "                        \n",
    "                        rocs[i] = roc\n",
    "                        precisions[i] = precision\n",
    "                        recalls[i] = recall\n",
    "                        f1s[i] = f1\n",
    "\n",
    "                dyno_rocs_process[gen_process_number] = rocs\n",
    "                dyno_precision_process[gen_process_number] = precisions\n",
    "                dyno_recall_process[gen_process_number] = recalls\n",
    "                dyno_f1_process[gen_process_number] = f1s\n",
    "\n",
    "        # pickle everything\n",
    "        with open(f'journal_results_dyno_N{n_vars}.pkl', 'wb') as f:\n",
    "                everything = (dyno_rocs_process, dyno_precision_process, dyno_recall_process, dyno_f1_process)\n",
    "                pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix_dyno = pd.concat([pd.DataFrame(dyno_rocs_process).mean(),pd.DataFrame(dyno_precision_process).mean(),pd.DataFrame(dyno_recall_process).mean(),pd.DataFrame(dyno_f1_process).mean()],axis=1)\n",
    "# mix_dyno.columns = ['roc','precision','recall','f1']\n",
    "# #index name 'generative process'\n",
    "# mix_dyno.index.name = 'generative process'\n",
    "# mix_dyno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "N_JOBS = 40\n",
    "from d2c.benchmark.varlingam import VARLiNGAM\n",
    "\n",
    "os.environ['MKL_NUM_THREADS'] = '1'  # Limit to 4 threads\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'  # Limit to 4 threads\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# def complete_causal_df(causal_df, n_variables, maxlags):\n",
    "#     causal_df = causal_df.copy()\n",
    "#     all_pairs = [(from_, to) for from_ in range(n_variables,n_variables * (maxlags + 1)) for to in range(n_variables)]\n",
    "    \n",
    "#     existing_pairs = set(zip(causal_df['from'], causal_df['to']))\n",
    "#     missing_pairs = [(from_, to) for from_, to in all_pairs if (from_, to) not in existing_pairs]\n",
    "    \n",
    "#     # Create all missing rows at once if there are any missing pairs\n",
    "#     if missing_pairs:\n",
    "#         missing_rows = pd.DataFrame(missing_pairs, columns=['from', 'to'])\n",
    "#         missing_rows['effect'] = 0.0\n",
    "#         missing_rows['p-value'] = None\n",
    "#         missing_rows['probability'] = 0.0\n",
    "#         missing_rows['is_causal'] = False\n",
    "#         causal_df = pd.concat([causal_df, missing_rows], ignore_index=True)\n",
    "    \n",
    "#     return causal_df.sort_values(by=['from', 'to']).reset_index(drop=True)\n",
    "\n",
    "for n_vars, todo in todos.items():\n",
    "        \n",
    "        if n_vars != '25':\n",
    "                continue\n",
    "\n",
    "\n",
    "        varlingam_rocs_process = {}\n",
    "        varlingam_precision_process = {}\n",
    "        varlingam_recall_process = {}\n",
    "        varlingam_f1_process = {}\n",
    "\n",
    "        for testing_file in tqdm(todo):\n",
    "                gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "                n_variables = int(testing_file.split('_')[1][1:])\n",
    "                max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "                noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "\n",
    "                # load original data for truth values\n",
    "                dataloader = DataLoader(n_variables = n_variables,\n",
    "                                maxlags = maxlags)\n",
    "                dataloader.from_pickle(data_root+testing_file)\n",
    "                observations = dataloader.get_original_observations()\n",
    "                true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "                varlingam = VARLiNGAM(ts_list=observations, maxlags=maxlags, n_jobs=N_JOBS)\n",
    "                varlingam.run()\n",
    "                causal_dfs_varlingam = varlingam.get_causal_dfs()\n",
    "\n",
    "                # causal_dfs_varlingam = [complete_causal_df(causal_df, n_variables,maxlags) for causal_df in causal_dfs_varlingam.values()]\n",
    "\n",
    "                rocs = {}\n",
    "                precisions = {}\n",
    "                recalls = {}\n",
    "                f1s = {}\n",
    "                for i in range(40):\n",
    "\n",
    "                        y_pred = causal_dfs_varlingam[i]['is_causal'].astype(int)\n",
    "                        y_pred_proba = causal_dfs_varlingam[i]['probability']\n",
    "                        y_test = true_causal_dfs[i]['is_causal'].astype(int)\n",
    "\n",
    "                        roc = roc_auc_score(y_test, y_pred_proba)\n",
    "                        precision = precision_score(y_test, y_pred)\n",
    "                        recall = recall_score(y_test, y_pred)\n",
    "                        f1 = f1_score(y_test, y_pred)\n",
    "                        \n",
    "                        rocs[i] = roc\n",
    "                        precisions[i] = precision\n",
    "                        recalls[i] = recall\n",
    "                        f1s[i] = f1\n",
    "\n",
    "                varlingam_rocs_process[gen_process_number] = rocs\n",
    "                varlingam_precision_process[gen_process_number] = precisions\n",
    "                varlingam_recall_process[gen_process_number] = recalls\n",
    "                varlingam_f1_process[gen_process_number] = f1s\n",
    "\n",
    "        # pickle everything\n",
    "        with open(f'journal_results_varlingam_N{n_vars}.pkl', 'wb') as f:\n",
    "                everything = (varlingam_rocs_process, varlingam_precision_process, varlingam_recall_process, varlingam_f1_process)\n",
    "                pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_varlingam = pd.concat([pd.DataFrame(varlingam_rocs_process).mean(),pd.DataFrame(varlingam_precision_process).mean(),pd.DataFrame(varlingam_recall_process).mean(),pd.DataFrame(varlingam_f1_process).mean()],axis=1)\n",
    "mix_varlingam.columns = ['roc','precision','recall','f1']\n",
    "#index name 'generative process'\n",
    "mix_varlingam.index.name = 'generative process'\n",
    "mix_varlingam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle everything\n",
    "with open('journal_results_N5.pkl', 'wb') as f:\n",
    "    everything = (td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process, pcmci_rocs_process, pcmci_precision_process, pcmci_recall_process, pcmci_f1_process, dyno_rocs_process, dyno_precision_process, dyno_recall_process, dyno_f1_process, varlingam_rocs_process, varlingam_precision_process, varlingam_recall_process, varlingam_f1_process)\n",
    "    pickle.dump(everything, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load \n",
    "import pickle\n",
    "with open('journal_results_N5.pkl', 'rb') as f:\n",
    "    td2c_rocs_process, td2c_precision_process, td2c_recall_process, td2c_f1_process, pcmci_rocs_process, pcmci_precision_process, pcmci_recall_process, pcmci_f1_process, dyno_rocs_process, dyno_precision_process, dyno_recall_process, dyno_f1_process, varlingam_rocs_process, varlingam_precision_process, varlingam_recall_process, varlingam_f1_process = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df1 = pd.DataFrame(td2c_rocs_process)\n",
    "df2 = pd.DataFrame(pcmci_rocs_process)\n",
    "df3 = pd.DataFrame(varlingam_rocs_process)\n",
    "\n",
    "# Combine data for boxplot\n",
    "combined_data = []\n",
    "\n",
    "for col in df1.columns:\n",
    "    combined_data.append(df1[col])\n",
    "    combined_data.append(df2[col])\n",
    "    combined_data.append(df3[col])\n",
    "\n",
    "# Create labels for x-axis\n",
    "labels = []\n",
    "for col in df1.columns:\n",
    "    labels.append(f'{col} TD2C')\n",
    "    labels.append(f'{col} PCMCI')\n",
    "    labels.append(f'{col} VARLiNGAM')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "box = plt.boxplot(combined_data, patch_artist=True)\n",
    "\n",
    "# Color customization\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "for patch, i in zip(box['boxes'], range(len(box['boxes']))):\n",
    "    patch.set_facecolor(colors[i % 3])\n",
    "\n",
    "\n",
    "plt.xticks(range(1, len(labels) + 1), labels, rotation=-90)\n",
    "plt.title('Side-by-Side Boxplots for Two DataFrames')\n",
    "plt.xlabel('Processes')\n",
    "plt.ylabel('Values')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df1 = pd.DataFrame(td2c_f1_process)\n",
    "df2 = pd.DataFrame(pcmci_f1_process)\n",
    "df3 = pd.DataFrame(varlingam_f1_process)\n",
    "df4 = pd.DataFrame(dyno_f1_process)\n",
    "\n",
    "# Combine data for boxplot\n",
    "combined_data = []\n",
    "\n",
    "for col in df1.columns:\n",
    "    combined_data.append(df1[col])\n",
    "    combined_data.append(df2[col])\n",
    "    combined_data.append(df3[col])\n",
    "    combined_data.append(df4[col])\n",
    "\n",
    "# Create labels for x-axis\n",
    "labels = []\n",
    "for col in df1.columns:\n",
    "    labels.append(f'{col} td2c')\n",
    "    labels.append(f'{col} pcmci')\n",
    "    labels.append(f'{col} varlingam')\n",
    "    labels.append(f'{col} dyno')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "box = plt.boxplot(combined_data, patch_artist=True)\n",
    "\n",
    "# Color customization\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "for patch, i in zip(box['boxes'], range(len(box['boxes']))):\n",
    "    patch.set_facecolor(colors[i % 4])\n",
    "\n",
    "\n",
    "plt.xticks(range(1, len(labels) + 1), labels, rotation=-90)\n",
    "plt.title('Side-by-Side Boxplots for Two DataFrames')\n",
    "plt.xlabel('Processes')\n",
    "plt.ylabel('Values')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming td2c_f1_process, pcmci_f1_process, varlingam_f1_process, and dyno_f1_process are defined\n",
    "\n",
    "df1 = pd.DataFrame(td2c_f1_process)\n",
    "df2 = pd.DataFrame(pcmci_f1_process)\n",
    "df3 = pd.DataFrame(varlingam_f1_process)\n",
    "df4 = pd.DataFrame(dyno_f1_process)\n",
    "\n",
    "# Number of processes\n",
    "num_processes = df1.shape[1]\n",
    "fontsize = 7\n",
    "# Plotting each process separately in a grid of 6 columns and 3 rows\n",
    "fig, axes = plt.subplots(3, 6, figsize=(10, 7), sharex=True, sharey=True)\n",
    "\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "\n",
    "for i, col in enumerate([1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20]):\n",
    "    combined_data = [df1[col], df2[col], df3[col], df4[col]]\n",
    "    labels = [f'TD2C', f'PCMCI', f'VARLINGAM', f'DYNOTEARS']\n",
    "    \n",
    "    row, col_idx = divmod(i, 6)\n",
    "    box = axes[row, col_idx].boxplot(combined_data, patch_artist=True)\n",
    "    \n",
    "    for patch, j in zip(box['boxes'], range(len(box['boxes']))):\n",
    "        patch.set_facecolor(colors[j % 4])\n",
    "    \n",
    "    axes[row, col_idx].set_title(f'Process {col}')\n",
    "    axes[row, col_idx].title.set_fontsize(fontsize)\n",
    "    axes[row, col_idx].set_xticks(range(1, len(labels) + 1))\n",
    "    axes[row, col_idx].set_xticklabels(labels, rotation=-90)\n",
    "    axes[row, col_idx].tick_params(axis='x', labelsize=fontsize)\n",
    "    if col_idx == 0:\n",
    "        axes[row, col_idx].set_ylabel('F1 Score')\n",
    "        axes[row, col_idx].yaxis.label.set_size(fontsize)\n",
    "    # Add this line:\n",
    "    axes[row, col_idx].grid(True)\n",
    "\n",
    "\n",
    "# Remove any empty subplots if the number of processes is less than 18\n",
    "if num_processes < 18:\n",
    "    for i in range(num_processes, 18):\n",
    "        row, col_idx = divmod(i, 6)\n",
    "        fig.delaxes(axes[row, col_idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('f1_scores_N5.pdf', format='pdf')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#make vector image pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming td2c_f1_process, pcmci_f1_process, varlingam_f1_process, and dyno_f1_process are defined\n",
    "\n",
    "df1 = pd.DataFrame(td2c_rocs_process)\n",
    "df2 = pd.DataFrame(pcmci_rocs_process)\n",
    "df3 = pd.DataFrame(varlingam_rocs_process)\n",
    "\n",
    "# Number of processes\n",
    "num_processes = df1.shape[1]\n",
    "fontsize = 7\n",
    "# Plotting each process separately in a grid of 6 columns and 3 rows\n",
    "fig, axes = plt.subplots(3, 6, figsize=(10, 7), sharex=True, sharey=True)\n",
    "\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "\n",
    "for i, col in enumerate([1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20]):\n",
    "    combined_data = [df1[col], df2[col], df3[col]]\n",
    "    labels = [f'TD2C', f'PCMCI', f'VARLINGAM']\n",
    "    \n",
    "    row, col_idx = divmod(i, 6)\n",
    "    box = axes[row, col_idx].boxplot(combined_data, patch_artist=True)\n",
    "    \n",
    "    for patch, j in zip(box['boxes'], range(len(box['boxes']))):\n",
    "        patch.set_facecolor(colors[j % 3])\n",
    "    \n",
    "    axes[row, col_idx].set_title(f'Process {col}')\n",
    "    axes[row, col_idx].title.set_fontsize(fontsize)\n",
    "    axes[row, col_idx].set_xticks(range(1, len(labels) + 1))\n",
    "    axes[row, col_idx].set_xticklabels(labels, rotation=-90)\n",
    "    axes[row, col_idx].tick_params(axis='x', labelsize=fontsize)\n",
    "    if col_idx == 0:\n",
    "        axes[row, col_idx].set_ylabel('ROC AUC')\n",
    "        axes[row, col_idx].yaxis.label.set_size(fontsize)\n",
    "    # Add this line:\n",
    "    axes[row, col_idx].grid(True)\n",
    "\n",
    "\n",
    "# Remove any empty subplots if the number of processes is less than 18\n",
    "if num_processes < 18:\n",
    "    for i in range(num_processes, 18):\n",
    "        row, col_idx = divmod(i, 6)\n",
    "        fig.delaxes(axes[row, col_idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_scores_N5.pdf', format='pdf')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#make vector image pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming td2c_f1_process, pcmci_f1_process, varlingam_f1_process, and dyno_f1_process are defined\n",
    "\n",
    "df1 = pd.DataFrame(td2c_f1_process)\n",
    "df2 = pd.DataFrame(pcmci_f1_process)\n",
    "df3 = pd.DataFrame(varlingam_f1_process)\n",
    "df4 = pd.DataFrame(dyno_f1_process)\n",
    "\n",
    "# Concatenate the data for each method across all processes\n",
    "combined_td2c = pd.concat([df1[col] for col in df1.columns], ignore_index=True)\n",
    "combined_pcmci = pd.concat([df2[col] for col in df2.columns], ignore_index=True)\n",
    "combined_varlingam = pd.concat([df3[col] for col in df3.columns], ignore_index=True)\n",
    "combined_dyno = pd.concat([df4[col] for col in df4.columns], ignore_index=True)\n",
    "\n",
    "# Combine all methods into one DataFrame for plotting\n",
    "combined_data = [combined_td2c, combined_pcmci, combined_varlingam, combined_dyno]\n",
    "labels = ['TD2C', 'PCMCI', 'VARLINGAM', 'DYNOTEARS']\n",
    "\n",
    "# Create a single boxplot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "box = ax.boxplot(combined_data, patch_artist=True)\n",
    "\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "for patch, color in zip(box['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_xticks(range(1, len(labels) + 1))\n",
    "ax.set_xticklabels(labels, rotation=-90, fontsize=10)\n",
    "ax.set_ylabel('F1 Score', fontsize=10)\n",
    "ax.set_title('Combined F1 Scores for All Processes - N=5', fontsize=12)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('combined_f1_scores_N5.pdf', format='pdf')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming td2c_rocs_process, pcmci_rocs_process, and varlingam_rocs_process are defined\n",
    "\n",
    "df1 = pd.DataFrame(td2c_rocs_process)\n",
    "df2 = pd.DataFrame(pcmci_rocs_process)\n",
    "df3 = pd.DataFrame(varlingam_rocs_process)\n",
    "\n",
    "# Concatenate the data for each method across all processes\n",
    "combined_td2c = pd.concat([df1[col] for col in df1.columns], ignore_index=True)\n",
    "combined_pcmci = pd.concat([df2[col] for col in df2.columns], ignore_index=True)\n",
    "combined_varlingam = pd.concat([df3[col] for col in df3.columns], ignore_index=True)\n",
    "\n",
    "# Combine all methods into one DataFrame for plotting\n",
    "combined_data = [combined_td2c, combined_pcmci, combined_varlingam]\n",
    "labels = ['TD2C', 'PCMCI', 'VARLINGAM']\n",
    "\n",
    "# Create a single boxplot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "box = ax.boxplot(combined_data, patch_artist=True)\n",
    "\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "for patch, color in zip(box['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_xticks(range(1, len(labels) + 1))\n",
    "ax.set_xticklabels(labels, rotation=-90, fontsize=10)\n",
    "ax.set_ylabel('ROC AUC', fontsize=10)\n",
    "ax.set_title('Combined ROC AUC Scores for All Processes - N=5', fontsize=12)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('combined_roc_scores_N5.pdf', format='pdf')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rocs_process = {}\n",
    "for process in descriptors_training['process_id'].unique():\n",
    "\n",
    "    training_data = descriptors_training.loc[descriptors_training['process_id'] != process]\n",
    "    X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "    y_train = training_data['is_causal']\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=50)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    rocs = {}\n",
    "    for i in range(40):\n",
    "        test_df = pd.read_csv(f'./d2c_benchmark/P{process}_N5_Nj2_n0.01.pkl'+'_'+str(i)+'.csv', index_col=0)\n",
    "        test_df = test_df.sort_values(by=['edge_source','edge_dest']).reset_index(drop=True)\n",
    "\n",
    "        X_test = test_df.drop(columns=['graph_id', 'edge_source', 'edge_dest', 'is_causal'])\n",
    "        y_test = true_causal_dfs[i]['is_causal']\n",
    "\n",
    "\n",
    "        y_pred = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "        roc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "        rocs[i] = roc\n",
    "\n",
    "    rocs_process[process] = rocs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '../../data/new_data/'\n",
    "destination_root = './'\n",
    "destination = 'd2c_all_couples_MB5_full'\n",
    "if not os.path.exists(destination_root+'/'+destination):\n",
    "    os.makedirs(destination_root+'/'+destination)\n",
    "maxlags = 5\n",
    "# empty folder ../../data/new_benchmark/\n",
    "for todo in [to_dos_10_variables]:\n",
    "    for testing_file in tqdm(todo):\n",
    "        if testing_file.endswith('.pkl'):\n",
    "            gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "            n_variables = int(testing_file.split('_')[1][1:])\n",
    "            max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "            noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "                \n",
    "            filename = f'{destination_root}/{destination}/P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "\n",
    "            training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "            X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_train = training_data['is_causal']\n",
    "\n",
    "            model = BalancedRandomForestClassifier(n_estimators=50, random_state=0, n_jobs=1, replacement=True, sampling_strategy='all')\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            dataloader = DataLoader(n_variables = n_variables,\n",
    "                            maxlags = maxlags)\n",
    "            dataloader.from_pickle(data_root+testing_file)\n",
    "            observations = dataloader.get_original_observations()\n",
    "            true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "            d2cwrapper = D2CWrapper(ts_list=observations, n_variables=n_variables, model=model, maxlags=maxlags, n_jobs = 55, full=True)\n",
    "\n",
    "            d2cwrapper.run()\n",
    "\n",
    "            causal_df = d2cwrapper.get_causal_dfs()\n",
    "\n",
    "            with open(filename, 'wb') as f:\n",
    "                    pickle.dump((\n",
    "                                causal_df, \n",
    "                                true_causal_dfs), f)     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '../../data/new_data/'\n",
    "destination_root = '../../data/new_benchmark'\n",
    "destination = 'd2c_all_couples_MB5_full'\n",
    "\n",
    "if not os.path.exists(destination_root+'/'+destination):\n",
    "    os.makedirs(destination_root+'/'+destination)\n",
    "maxlags = 5\n",
    "# empty folder ../../data/new_benchmark/\n",
    "\n",
    "for todo in [to_dos_25_variables]:\n",
    "    for testing_file in tqdm(todo):\n",
    "        if testing_file.endswith('.pkl'):\n",
    "            gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "            n_variables = int(testing_file.split('_')[1][1:])\n",
    "            max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "            noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "                \n",
    "            filename = f'{destination_root}/{destination}/P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "\n",
    "            training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "            X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "            y_train = training_data['is_causal']\n",
    "\n",
    "            model = BalancedRandomForestClassifier(n_estimators=50, random_state=0, n_jobs=1, replacement=True, sampling_strategy='all')\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            dataloader = DataLoader(n_variables = n_variables,\n",
    "                            maxlags = maxlags)\n",
    "            dataloader.from_pickle(data_root+testing_file)\n",
    "            observations = dataloader.get_original_observations()\n",
    "            true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "\n",
    "            d2cwrapper = D2CWrapper(ts_list=observations, n_variables=n_variables, model=model, maxlags=maxlags, n_jobs = 55, full=True)\n",
    "\n",
    "            d2cwrapper.run()\n",
    "\n",
    "            causal_df = d2cwrapper.get_causal_dfs()\n",
    "\n",
    "            with open(filename, 'wb') as f:\n",
    "                    pickle.dump((\n",
    "                                causal_df, \n",
    "                                true_causal_dfs), f)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the two missing files separately\n",
    "missing_files = ['P8_N25_Nj8_n0.005.pkl','P9_N25_Nj8_n0.005.pkl']\n",
    "\n",
    "data_root = '../../data/new_data/'\n",
    "destination_root = '../../data/new_benchmark'\n",
    "destination = 'd2c_all_couples_MB5_full'\n",
    "\n",
    "maxlags = 5\n",
    "# empty folder ../../data/new_benchmark/\n",
    "\n",
    "\n",
    "for testing_file in tqdm(missing_files):\n",
    "    if testing_file.endswith('.pkl'):\n",
    "        gen_process_number = int(testing_file.split('_')[0][1:])\n",
    "        n_variables = int(testing_file.split('_')[1][1:])\n",
    "        max_neighborhood_size = int(testing_file.split('_')[2][2:])\n",
    "        noise_std = float(testing_file.split('_')[3][1:-4])\n",
    "            \n",
    "        filename = f'{destination_root}/{destination}/P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "\n",
    "        training_data = descriptors_training.loc[descriptors_training['process_id'] != gen_process_number]\n",
    "        X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "        y_train = training_data['is_causal']\n",
    "\n",
    "        model = BalancedRandomForestClassifier(n_estimators=50, random_state=0, n_jobs=1, replacement=True, sampling_strategy='all')\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        dataloader = DataLoader(n_variables = n_variables,\n",
    "                        maxlags = maxlags)\n",
    "        dataloader.from_pickle(data_root+testing_file)\n",
    "        observations = dataloader.get_original_observations()\n",
    "        true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "\n",
    "        d2cwrapper = D2CWrapper(ts_list=observations, n_variables=n_variables, model=model, maxlags=maxlags, n_jobs = 55, full=True)\n",
    "\n",
    "        d2cwrapper.run()\n",
    "\n",
    "        causal_df = d2cwrapper.get_causal_dfs()\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "                pickle.dump((\n",
    "                            causal_df, \n",
    "                            true_causal_dfs), f)     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 variables is just too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = '../../data/new_data/'\n",
    "# destination_root = '../../data/new_benchmark'\n",
    "# destination = 'd2c_all_couples_MB5_full'\n",
    "\n",
    "# if not os.path.exists(destination_root+'/'+destination):\n",
    "#     os.makedirs(destination_root+'/'+destination)\n",
    "# maxlags = 5\n",
    "# # empty folder ../../data/new_benchmark/\n",
    "\n",
    "# for todo in [to_dos_50_variables]:\n",
    "#     for file in tqdm(todo):\n",
    "#         if file.endswith('.pkl'):\n",
    "#             gen_process_number = int(file.split('_')[0][1:])\n",
    "#             n_variables = int(file.split('_')[1][1:])\n",
    "#             max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "#             noise_std = float(file.split('_')[3][1:-4])\n",
    "                \n",
    "#             filename = f'{destination_root}/{destination}/P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "\n",
    "#             training_data = data.loc[data['process_id'] != gen_process_number]\n",
    "#             X_train = training_data.drop(columns=['process_id', 'graph_id', 'n_variables', 'max_neighborhood_size','noise_std', 'edge_source', 'edge_dest', 'is_causal',])\n",
    "#             y_train = training_data['is_causal']\n",
    "\n",
    "#             model = BalancedRandomForestClassifier(n_estimators=50, random_state=0, n_jobs=1, replacement=True, sampling_strategy='all')\n",
    "\n",
    "#             model.fit(X_train, y_train)\n",
    "\n",
    "#             dataloader = DataLoader(n_variables = n_variables,\n",
    "#                             maxlags = maxlags)\n",
    "#             dataloader.from_pickle(root+file)\n",
    "#             observations = dataloader.get_original_observations()\n",
    "#             true_causal_dfs = dataloader.get_true_causal_dfs()\n",
    "\n",
    "\n",
    "#             d2cwrapper = D2CWrapper(ts_list=observations, n_variables=n_variables, model=model, maxlags=maxlags, n_jobs = 55, full=True)\n",
    "\n",
    "#             d2cwrapper.run()\n",
    "\n",
    "#             causal_df = d2cwrapper.get_causal_dfs()\n",
    "\n",
    "#             with open(filename, 'wb') as f:\n",
    "#                     pickle.dump((\n",
    "#                                 causal_df, \n",
    "#                                 true_causal_dfs), f)     \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2cpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
