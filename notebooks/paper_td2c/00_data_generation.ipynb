{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook is dedicated to generating time series data for multiple configurations using the `TSBuilder` class from the `d2c.data_generation.builder` module. The notebook is structured to perform data generation in several phases, leveraging Pythonâ€™s multiprocessing capabilities to handle multiple data generation tasks simultaneously.\n",
    "\n",
    "### Breakdown of the Notebook:\n",
    "\n",
    "1. **Setup and Imports:**\n",
    "   - The notebook starts by importing necessary libraries such as `Pool` from `multiprocessing` and `TSBuilder` for building time series.\n",
    "   - Necessary constants like `N_JOBS` (number of parallel jobs) are defined.\n",
    "\n",
    "2. **Initial Data Generation:**\n",
    "   - A `run_process` function is defined to handle the generation of time series data. This function attempts to build time series with specific parameters (like the number of variables, neighborhood size, noise standard deviation, etc.) and save them as pickle files.\n",
    "   - A multiprocessing pool is set up to run these data generation tasks in parallel across various configurations (different processes, variable counts, neighborhood sizes, and noise levels).\n",
    "\n",
    "3. **Verification of Generated Data:**\n",
    "   - The notebook includes a check for any missing data files based on expected combinations of parameters. If any files are missing, their names are collected into a list.\n",
    "\n",
    "4. **Re-Generation of Missing Data:**\n",
    "   - The missing data files are then targeted for regeneration. Adjustments are made (like changing the seed and increasing the number of max attempts) to ensure successful generation.\n",
    "   - Another round of multiprocessing is initiated specifically for the missing files.\n",
    "\n",
    "5. **Final Verification:**\n",
    "   - A final check is performed to ensure all expected data files are generated. If any files are still missing, they are targeted for another attempt with even more attempts and a different seed.\n",
    "\n",
    "6. **Summary:**\n",
    "   - At the end of the notebook, a check is performed to confirm the total number of files in the data directory, indicating the completion of the data generation process.\n",
    "\n",
    "### Key Features:\n",
    "- **Use of Multiprocessing:** To expedite data generation, the notebook utilizes the multiprocessing library to run multiple instances of data generation tasks simultaneously.\n",
    "- **Error Handling:** Each data generation task includes try-except blocks to handle potential errors that might occur during the generation process.\n",
    "- **Progressive Problem Solving:** The notebook incrementally addresses issues by adjusting parameters and retrying data generation for failed tasks.\n",
    "\n",
    "This notebook serves as a comprehensive script for generating a large dataset of time series with varying characteristics, aiming to robustly handle errors and ensure complete generation across specified configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install ../../../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from d2c.data_generation.builder import TSBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = 55\n",
    "def run_process(params):\n",
    "    \"\"\"\n",
    "    Run a single process of the data generation.\n",
    "    \"\"\"\n",
    "    process, n_variables, max_neighborhood_size, noise_std = params\n",
    "    try:\n",
    "        tsbuilder = TSBuilder(observations_per_time_series=250, \n",
    "                              maxlags=5, \n",
    "                              n_variables=n_variables, \n",
    "                              time_series_per_process=40, \n",
    "                              processes_to_use=[process], \n",
    "                              noise_std=noise_std, \n",
    "                              max_neighborhood_size=max_neighborhood_size, \n",
    "                              seed=42, \n",
    "                              max_attempts=200,\n",
    "                              verbose=True)\n",
    "\n",
    "        tsbuilder.build()\n",
    "        tsbuilder.to_pickle(f'/home/jpalombarini/td2c/notebooks/paper_td2c/.data/P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl')\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} done')\n",
    "    except ValueError as e:\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} failed: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3_N5_Nj2_n0.01 done\n",
      "P4_N5_Nj2_n0.01 done\n",
      "P2_N5_Nj2_n0.01 done\n",
      "P6_N5_Nj2_n0.01 done\n",
      "P3_N5_Nj4_n0.01 done\n",
      "P1_N5_Nj2_n0.01 done\n",
      "P2_N5_Nj4_n0.01 done\n",
      "P6_N5_Nj4_n0.01 done\n",
      "P4_N5_Nj4_n0.01 done\n",
      "P1_N5_Nj4_n0.01 done\n",
      "P3_N10_Nj2_n0.01 done\n",
      "P6_N10_Nj2_n0.01 done\n",
      "P3_N10_Nj4_n0.01 done\n",
      "P4_N10_Nj2_n0.01 done\n",
      "P4_N10_Nj4_n0.01 done\n",
      "P2_N10_Nj2_n0.01 done\n",
      "P1_N10_Nj2_n0.01 done\n",
      "P6_N10_Nj4_n0.01 done\n",
      "P1_N10_Nj4_n0.01 done\n",
      "P2_N10_Nj4_n0.01 done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    This script generates the data for different parameters: processes, number of variables, neighborhood sizes and noise levels.\n",
    "    The data is saved in the .data folder.\n",
    "    The if __name__ == '__main__': is used to avoid multiprocessing issues in Jupyter notebooks, i.e. the script is run as a script and not\n",
    "    as a module as it would have been if the script was imported, with the __name__ being the name of the module.\n",
    "    If the script is imported, the __name__ is the name of the module, if it is run as a script, the __name__ is __main__.\n",
    "    So, to run this script in a Jupyter notebook, we write the code inside the if __name__ == '__main__': block, while, if we want to import\n",
    "    the functions from this script, we write \"from script import run_process\".\n",
    "    \"\"\"\n",
    "    parameters = [(process, n_variables, max_neighborhood_size, noise_std)\n",
    "                  for process in [1, 2, 3, 4, 6] # , 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20\n",
    "                  for n_variables in [5, 10] # , 25\n",
    "                  for max_neighborhood_size in [2, 4] # , 8\n",
    "                  for noise_std in [0.01]] # , 0.005, 0.001\n",
    "\n",
    "    with Pool(processes=N_JOBS) as pool:\n",
    "        pool.map(run_process, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check any missing combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "missing = []\n",
    "for process in [1, 2, 3, 4, 6]: # , 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20\n",
    "    for n_variables in [5,10]: # ,25\n",
    "        for max_neighborhood_size in [2,4]: # ,8\n",
    "            for noise_std in [0.01]: # , 0.005, 0.001\n",
    "                filename = f'/home/jpalombarini/td2c/notebooks/paper_td2c/.data/P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "                if not os.path.exists(filename):\n",
    "                    missing.append(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we focus on recreating the missing ones, by changing the seeds and increasing the number of max_attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from d2c.data_generation.builder import TSBuilder\n",
    "N_JOBS = 55\n",
    "def run_process(params):\n",
    "    process, n_variables, max_neighborhood_size, noise_std = params\n",
    "    try:# we change the seed and increase the max_attempts\n",
    "        tsbuilder = TSBuilder(observations_per_time_series=250, \n",
    "                              maxlags=5, \n",
    "                              n_variables=n_variables, \n",
    "                              time_series_per_process=40, \n",
    "                              processes_to_use=[process], \n",
    "                              noise_std=noise_std, \n",
    "                              max_neighborhood_size=max_neighborhood_size, \n",
    "                              seed=24, \n",
    "                              max_attempts=400,\n",
    "                              verbose=True)\n",
    "\n",
    "        tsbuilder.build()\n",
    "        tsbuilder.to_pickle(f'./data/P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl')\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} done')\n",
    "    except ValueError as e:\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} failed: {e}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parameters = []\n",
    "    for missing_file in missing:\n",
    "        process = int(missing_file.split('/')[-1].split('_')[0][1:])\n",
    "        n_variables = int(missing_file.split('/')[-1].split('_')[1][1:])\n",
    "        max_neighborhood_size = int(missing_file.split('/')[-1].split('_')[2][2:])\n",
    "        noise_std = float(missing_file.split('/')[-1].split('_')[3][1:-4])\n",
    "        parameters.append((process, n_variables, max_neighborhood_size, noise_std))\n",
    "\n",
    "\n",
    "    with Pool(processes=N_JOBS) as pool:\n",
    "        pool.map(run_process, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what is still missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "missing = []\n",
    "for process in [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20]:\n",
    "    for n_variables in [5,10,25]:\n",
    "        for max_neighborhood_size in [2,4,8]:\n",
    "            for noise_std in [0.01, 0.005, 0.001]:\n",
    "                filename = f'./data/P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl'\n",
    "                if not os.path.exists(filename):\n",
    "                    missing.append(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try one last time with even more max_attemps and a different seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from d2c.data_generation.builder import TSBuilder\n",
    "N_JOBS = len(missing)\n",
    "def run_process(params):\n",
    "    process, n_variables, max_neighborhood_size, noise_std = params\n",
    "    try:# we change the seed and increase the max_attempts\n",
    "        tsbuilder = TSBuilder(observations_per_time_series=250, \n",
    "                              maxlags=5, \n",
    "                              n_variables=n_variables, \n",
    "                              time_series_per_process=40, \n",
    "                              processes_to_use=[process], \n",
    "                              noise_std=noise_std, \n",
    "                              max_neighborhood_size=max_neighborhood_size, \n",
    "                              seed=0, \n",
    "                              max_attempts=1000,\n",
    "                              verbose=True)\n",
    "\n",
    "        tsbuilder.build()\n",
    "        tsbuilder.to_pickle(f'./data/P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}.pkl')\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} done')\n",
    "    except ValueError as e:\n",
    "        print(f'P{process}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std} failed: {e}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parameters = []\n",
    "    for missing_file in missing:\n",
    "        process = int(missing_file.split('/')[-1].split('_')[0][1:])\n",
    "        n_variables = int(missing_file.split('/')[-1].split('_')[1][1:])\n",
    "        max_neighborhood_size = int(missing_file.split('/')[-1].split('_')[2][2:])\n",
    "        noise_std = float(missing_file.split('/')[-1].split('_')[3][1:-4])\n",
    "        parameters.append((process, n_variables, max_neighborhood_size, noise_std))\n",
    "# what this if statement does is to run the function run_process in parallel for each parameter in the list parameters\n",
    "# if __name__ == '__main__': is a python thing that allows you to run the code in the if statement only if you run the script directly\n",
    "\n",
    "    with Pool(processes=N_JOBS) as pool:\n",
    "        pool.map(run_process, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All time series have been generated correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43824/1325791777.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jpalombarini/td2c/notebooks/paper_td2c/.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "len(os.listdir('/home/jpalombarini/td2c/notebooks/paper_td2c/.data'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2cpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
