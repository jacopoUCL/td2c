{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook is centered on calculating and storing descriptors for time series data using the `D2C` class from the `d2c.descriptors` module. It works with previously generated data stored in a directory and processes each file to extract time series descriptors. The notebook employs the Python libraries pandas and tqdm to manage and monitor data operations.\n",
    "\n",
    "### Breakdown of the Notebook:\n",
    "\n",
    "1. **Setup and Imports:**\n",
    "   - Essential libraries like `pickle`, `os`, `pandas`, and `tqdm` are imported. Additionally, `D2C` and `DataLoader` from the `d2c.descriptors` module are used for processing the data.\n",
    "   - Constants such as `N_JOBS` (number of parallel jobs), `SEED`, `MB_SIZE` (Markov blanket size), `COUPLES_TO_CONSIDER_PER_DAG`, and `maxlags` are defined for the descriptor calculations.\n",
    "\n",
    "2. **Processing Each Data File:**\n",
    "   - The notebook iterates over each file in the sorted order from the data directory. For each file, it extracts metadata like process number, number of variables, neighborhood size, and noise standard deviation from the filename.\n",
    "   - A `DataLoader` instance is created and used to load the data from the respective file. The loaded data includes observations and DAGs (directed acyclic graphs) which represent the time series data and its underlying relationships.\n",
    "\n",
    "3. **Descriptor Calculation:**\n",
    "   - The `D2C` (Data to Causality) object is initialized with the loaded data and the predefined settings (like seed, number of jobs, etc.). The initialization prepares the framework for descriptor calculations.\n",
    "   - Descriptors are computed for the data, which involves statistical and causal analysis to derive meaningful metrics that describe the interactions and dependencies within the data.\n",
    "\n",
    "4. **Saving the Descriptors:**\n",
    "   - The resulting dataframe containing descriptors is augmented with metadata columns indicating the process ID, number of variables, neighborhood size, and noise standard deviation.\n",
    "   - The dataframe is then saved as a pickle file in a designated directory for descriptors. Each file is named systematically to reflect the configuration of the data it describes.\n",
    "\n",
    "### Key Features:\n",
    "- **Efficient Data Handling:** The notebook effectively handles large datasets by iterating through files and processing them individually, making efficient use of computational resources.\n",
    "- **Progress Monitoring:** Integration of `tqdm` provides a progress bar for tracking the completion of processing across multiple files.\n",
    "- **Detailed Metadata Management:** Metadata from filenames is used to annotate the descriptor results, ensuring that each output file is traceable back to its input conditions.\n",
    "\n",
    "This notebook automates the computation of detailed descriptors for time series datasets, providing a scalable solution for analyzing large volumes of data in a structured and reproducible manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook generates the training descriptors. \n",
    "For each file; only a subset of 300 couples of variables are chosen, according to the following logic\n",
    "\n",
    "- 100 causal couples: A selection of 20 pairs of variables that are causally related according to the DAG. These serve as positive examples.\n",
    "- 100 opposite couples: For each of the causal couples selected, the corresponding opposite pair (effect as cause and cause as effect) is also chosen. These pairs, despite being theoretically informative, do not respect the temporal ordering and thus they won't appear in the results returned by competitor methods. For this reason, they will be excluded from the evaluation phase later on, although they will remain in the training set of our classifier.\n",
    "- 100 additional noncausal couples: To compensate for the exclusion of the opposite couples in validation and ensure a balanced dataset. Unlike the opposite ones, these pairs are chosen based on their lack of causal connection in the DAG and are more likely to resemble noncausal relationships encountered in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os\n",
    "import pandas as pd\n",
    "from d2c.descriptors_generation import D2C, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = 40 # number of jobs to run in parallel. For D2C, parallelism is implemented at the observation level: each observation from a single file is processed in parallel\n",
    "SEED = 42 # random seed for reproducibility\n",
    "MB_SIZE = 2 # size to consider when estimating the markov blanket. This is only useful if the MB is actually estimated\n",
    "COUPLES_TO_CONSIDER_PER_DAG = -1 # edges that are considered in total to compute descriptors, for each TS. This can speed up the process. If set to -1, all possible edges are considered\n",
    "maxlags = 5 # maximum lags to consider when considering variable couples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For which files do we want to compute our descriptors? We choose the ones with fixed `noise_std == 0.01`and fixed `max neighbordhood size==2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script takes the files in the input folder and filters them according to the parameters of the process.\n",
    "\"\"\"\n",
    "\n",
    "input_folder = '/home/jpalombarini/td2c/notebooks/paper_td2c/.data/'\n",
    "\n",
    "to_process = [] # list of files to process\n",
    "\n",
    "# This loop is used to filter the files to process and obtain the parameters of the process\n",
    "# The resulting list will be used to parallelize the process and will be passed to the DataLoader.\n",
    "# The result is of the form (file, gen_process_number, n_variables, max_neighborhood_size, noise_std)\n",
    "# asnd is saved in the to_process list.\n",
    "for file in sorted(os.listdir(input_folder)): \n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "    if noise_std != 0.01:\n",
    "        continue\n",
    "    \n",
    "    if max_neighborhood_size != 2:\n",
    "        continue\n",
    "\n",
    "    # if n_variables != 5:\n",
    "    #     continue\n",
    "\n",
    "    to_process.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the D2C method to compute the descriptors. <br>\n",
    "We use `mb_estimator='ts'` to select the past and future values of the considered variable as its Markov Blanket <br>\n",
    "We use `cmi='original'` to estimate conditional mutual information as in (Bontempi et al, 2015) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETA 2h with 40 jobs!!\n",
    "\n",
    "\"\"\"\n",
    "This sccript processes the files in the input folder and saves the descriptors in the output folder.\n",
    "\"\"\"\n",
    "\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/paper_td2c/.descriptors_ts_cmiknn_entropy/'  \n",
    "\n",
    "# create output folder if it does not exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# This loop processes the files in the input folder (to_process) and saves the descriptors in the output folder.\n",
    "\n",
    "# At first, we collect the parameters of the process from the file name.\n",
    "for file in tqdm(to_process):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "# The DataLoader is initialized with the parameters of the process. \n",
    "    dataloader = DataLoader(n_variables = n_variables,\n",
    "                    maxlags = maxlags)\n",
    "    dataloader.from_pickle(input_folder+file)\n",
    "\n",
    "# The D2C object is initialized with the DataLoader and the parameters of the process.\n",
    "    d2c = D2C(observations=dataloader.get_observations(), \n",
    "            dags=dataloader.get_dags(), \n",
    "            couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG, \n",
    "            MB_size=MB_SIZE, \n",
    "            n_variables=n_variables, \n",
    "            maxlags=maxlags,\n",
    "            seed=SEED,\n",
    "            n_jobs=N_JOBS,\n",
    "            full=True,\n",
    "            quantiles=True,\n",
    "            normalize=True,\n",
    "            cmi='original',\n",
    "            mb_estimator='ts')\n",
    "\n",
    "    d2c.initialize() # initializes the D2C object\n",
    "\n",
    "    descriptors_df = d2c.get_descriptors_df()  # computes the descriptors\n",
    "\n",
    "    descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "    descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "    descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "    descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "    # The descriptors are saved in the output folder as a pickle file.\n",
    "    descriptors_df.to_pickle(output_folder+f'P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use `mb_estimator='ts'` to select the past and future values of the considered variable as its Markov Blanket <br>\n",
    "We use `cmi='cmiknn'` to estimate conditional mutual information with knncmi <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this step it's required to clone the knncmi repository and install it\n",
    "#\n",
    "# git clone https://github.com/omesner/knncmi.git\n",
    "# cd knncmi\n",
    "# pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script removes the files that were already processed from the to_process list and creates the output folder for knnCMI descriptors.\n",
    "\"\"\"\n",
    "\n",
    "output_folder = '/home/jpalombarini/td2c/notebooks/paper_td2c/.descriptors_ts_cmiknn_entropy/'  \n",
    "\n",
    "#create output folder if it does not exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# remove already processed files\n",
    "to_process_copy = to_process.copy()\n",
    "for file in os.listdir(output_folder):\n",
    "    name = file[:-8]+file[-4:]\n",
    "    if name in to_process_copy:\n",
    "        to_process.remove(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop processes the files in the input folder (to_process) and saves the descriptors in the output folder.\n",
    "\n",
    "# At first, we collect the parameters of the process from the file name.\n",
    "for file in tqdm(to_process):\n",
    "    gen_process_number = int(file.split('_')[0][1:])\n",
    "    n_variables = int(file.split('_')[1][1:])\n",
    "    max_neighborhood_size = int(file.split('_')[2][2:])\n",
    "    noise_std = float(file.split('_')[3][1:-4])\n",
    "\n",
    "# The DataLoader is initialized with the parameters of the process.\n",
    "    dataloader = DataLoader(n_variables = n_variables,\n",
    "                    maxlags = maxlags)\n",
    "    dataloader.from_pickle(input_folder+file)\n",
    "\n",
    "# The D2C object is initialized with the DataLoader and the parameters of the process.\n",
    "    d2c = D2C(observations=dataloader.get_observations(), \n",
    "            dags=dataloader.get_dags(), \n",
    "            couples_to_consider_per_dag=COUPLES_TO_CONSIDER_PER_DAG, \n",
    "            MB_size=MB_SIZE, \n",
    "            n_variables=n_variables, \n",
    "            maxlags=maxlags,\n",
    "            seed=SEED,\n",
    "            n_jobs=N_JOBS,\n",
    "            full=True,\n",
    "            quantiles=True,\n",
    "            normalize=True,\n",
    "            cmi='cmiknn_3',\n",
    "            mb_estimator='ts')\n",
    "\n",
    "    d2c.initialize() # initializes the D2C object\n",
    "\n",
    "    descriptors_df = d2c.get_descriptors_df() # computes the descriptors\n",
    "\n",
    "    descriptors_df.insert(0, 'process_id', gen_process_number)\n",
    "    descriptors_df.insert(2, 'n_variables', n_variables)\n",
    "    descriptors_df.insert(3, 'max_neighborhood_size', max_neighborhood_size)\n",
    "    descriptors_df.insert(4, 'noise_std', noise_std)\n",
    "\n",
    "    # The descriptors are saved in the output folder as a pickle file.\n",
    "    descriptors_df.to_pickle(output_folder+f'P{gen_process_number}_N{n_variables}_Nj{max_neighborhood_size}_n{noise_std}_MB{MB_SIZE}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = {}\n",
    "# for output_folder in ['/home/jpalombarini/td2c/notebooks/paper_td2c/descriptors_original_full/',\n",
    "#                      '/home/jpalombarini/td2c/notebooks/paper_td2c/descriptors_cmiknn_full/',\n",
    "#                      '/home/jpalombarini/td2c/notebooks/paper_td2c/descriptors_cmiknn_full_k5/',\n",
    "#                      '/home/jpalombarini/td2c/notebooks/paper_td2c/descriptors_cmiknn_full_k15/']:\n",
    "for output_folder in ['/home/jpalombarini/td2c/notebooks/paper_td2c/.descriptors_ts_cmiknn_entropy/',\n",
    "                    '/home/jpalombarini/td2c/notebooks/paper_td2c/.descriptors_ts_original_entropy/']:\n",
    "    for single_output_folder in [output_folder]:\n",
    "        dfs = []\n",
    "        for file in os.listdir(single_output_folder):\n",
    "            df = pd.read_pickle(single_output_folder+file)\n",
    "            dfs.append(df)\n",
    "        descriptors[single_output_folder] = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It returns the columns' names of the descriptors\n",
    "descriptors[single_output_folder].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script performs the Leave-One-Process-Out Cross-Validation (LOPO-CV) for the descriptors using a Random Forest classifier.\n",
    "The scope is to evaluate the performance of the descriptors in predicting causal relationships.\n",
    "The performance is evaluated using the ROC-AUC metric.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# This function performs the Leave-One-Process-Out Cross-Validation (LOPO-CV) for the descriptors using a Random Forest classifier.\n",
    "def perform_lopo_cv(descriptors_df):\n",
    "    roc_aucs = {} # dictionary to store the ROC-AUC values for each process\n",
    "\n",
    "    # This loop performs the LOPO-CV\n",
    "    for process_id in descriptors_df['process_id'].unique(): \n",
    "        df_test = descriptors_df[descriptors_df['process_id'] == process_id]\n",
    "        df_train = descriptors_df[descriptors_df['process_id'] != process_id]\n",
    "\n",
    "        X_train = df_train.drop(columns=['process_id','graph_id','n_variables','max_neighborhood_size','noise_std','edge_source','edge_dest','is_causal'])\n",
    "        y_train = df_train['is_causal']\n",
    "\n",
    "        X_test = df_test.drop(columns=['process_id','graph_id','n_variables','max_neighborhood_size','noise_std','edge_source','edge_dest','is_causal'])\n",
    "        y_test = df_test['is_causal']\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=50, max_depth=None)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        roc_aucs[process_id] = roc_auc\n",
    "    return roc_aucs\n",
    "\n",
    "\n",
    "# rocs = {}\n",
    "# for key in descriptors.keys():\n",
    "#     rocs[key] = perform_lopo_cv(descriptors[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original mi, ts mb, less nj, a lot of couples \n",
    "perform_lopo_cv(descriptors[single_output_folder])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original mi, original mb, less nj\n",
    "perform_lopo_cv(descriptors[single_output_folder])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original mi, original mb\n",
    "perform_lopo_cv(descriptors[single_output_folder])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmiknn, original mb\n",
    "perform_lopo_cv(descriptors[single_output_folder])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmiknn, tsmb\n",
    "perform_lopo_cv(descriptors[single_output_folder])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_lopo_cv(descriptors['/home/jpalombarini/td2c/notebooks/paper_td2c/.descriptors_ts_original_entropy/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_lopo_cv(descriptors['/home/jpalombarini/td2c/notebooks/paper_td2c/.descriptors_ts_cmiknn_entropy/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{13: 0.9317180555555555,\n",
    " 3: 0.8845465277777778,\n",
    " 16: 0.9710965277777778,\n",
    " 11: 0.9660762152777778,\n",
    " 19: 0.954526388888889,\n",
    " 14: 0.9804326388888889,\n",
    " 7: 0.9450670138888889,\n",
    " 4: 0.9019128472222222,\n",
    " 9: 1.0,\n",
    " 20: 0.9452003472222222,\n",
    " 10: 0.9563795138888889,\n",
    " 15: 1.0,\n",
    " 2: 0.9165769097222222,\n",
    " 8: 0.9330982638888888,\n",
    " 12: 1.0,\n",
    " 18: 1.0,\n",
    " 6: 0.928396875,\n",
    " 1: 0.9210937499999999}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "rocs = {}\n",
    "for key in descriptors.keys():\n",
    "    rocs[key] = perform_lopo_cv(descriptors[key])\n",
    "\n",
    "rocs_df = pd.DataFrame(rocs)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(data=rocs_df)\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.xticks([0,1,2],['Original','Original-MBTS', 'CMIKNN-3-MBTS'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot pairwise\n",
    "\n",
    "rocs_df = pd.DataFrame(rocs)\n",
    "rocs_df = rocs_df.T\n",
    "rocs_df = rocs_df.reset_index()\n",
    "rocs_df = rocs_df.melt(id_vars='index')\n",
    "rocs_df.columns = ['process_id','method','roc_auc']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=rocs_df, x='process_id', y='roc_auc', hue='method')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.show()\n",
    "\n",
    "# barplot same process_id side by side\n",
    "rocs_df = pd.DataFrame(rocs)\n",
    "rocs_df = rocs_df.T\n",
    "rocs_df = rocs_df.reset_index()\n",
    "rocs_df = rocs_df.melt(id_vars='index')\n",
    "rocs_df.columns = ['process_id','method','roc_auc']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(data=rocs_df, x='method', y='roc_auc', hue='process_id')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some minor feature importance\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "df = descriptors['/home/jpalombarini/td2c/notebooks/paper_td2c/.descriptors_ts_cmiknn_entropy/']\n",
    "\n",
    "df_train = df[df['process_id'] >= 10]\n",
    "df_test = df[df['process_id'] < 10]\n",
    "\n",
    "X_train = df_train.drop(columns=['process_id', 'graph_id', 'n_variables','max_neighborhood_size','noise_std','edge_source','edge_dest','is_causal'])\n",
    "y_train = df_train['is_causal']\n",
    "\n",
    "X_test = df_test.drop(columns=['process_id', 'graph_id' ,'n_variables','max_neighborhood_size','noise_std','edge_source','edge_dest','is_causal'])\n",
    "y_test = df_test['is_causal']\n",
    "\n",
    "clf = BalancedRandomForestClassifier(n_estimators=50, random_state=42, n_jobs=50, sampling_strategy='all', replacement=True)\n",
    "# clf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=50)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Some minor feature importance\n",
    "df = descriptors['/home/jpalombarini/td2c/notebooks/paper_td2c/.descriptors_ts_cmiknn_entropy/']\n",
    "\n",
    "X = df.drop(columns=['process_id', 'graph_id', 'n_variables','max_neighborhood_size','noise_std','edge_source','edge_dest','is_causal'])\n",
    "y = df['is_causal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=50, max_depth=None)\n",
    "clf.fit(X, y)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    print(f\"{f+1}. feature {X.columns[indices[f]]} ({importances[indices[f]]})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['process_id'] >= 10]\n",
    "df_test = df[df['process_id'] < 10]\n",
    "\n",
    "X_train = df_train.drop(columns=['process_id', 'graph_id', 'n_variables','max_neighborhood_size','noise_std','edge_source','edge_dest','is_causal'])\n",
    "y_train = df_train['is_causal']\n",
    "\n",
    "\n",
    "X_test = df_test.drop(columns=['process_id', 'graph_id' ,'n_variables','max_neighborhood_size','noise_std','edge_source','edge_dest','is_causal'])\n",
    "y_test = df_test['is_causal']\n",
    "\n",
    "\n",
    "# clf = BalancedRandomForestClassifier(n_estimators=50, random_state=42, n_jobs=50, sampling_strategy='all', replacement=True)\n",
    "# # clf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=50)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(random_state=42, hidden_layer_sizes=(100, 100), max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy and other metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1: {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topick"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2cpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
